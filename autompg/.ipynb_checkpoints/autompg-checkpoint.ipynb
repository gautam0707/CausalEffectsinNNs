{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed832b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os,csv,math,sys, joblib\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import collections\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.model_selection, sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "#import pydot\n",
    "from similaritymeasures import frechet_dist\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib\n",
    "seed = 99 # To reproduce the results\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f996d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 6)\n",
      "   cylinders  displacement  horsepower    weight  acceleration       mpg\n",
      "0        1.0      0.617571    0.456522  0.536150      0.238095  0.239362\n",
      "1        1.0      0.728682    0.646739  0.589736      0.208333  0.159574\n",
      "2        1.0      0.645995    0.565217  0.516870      0.178571  0.239362\n",
      "3        1.0      0.609819    0.565217  0.516019      0.238095  0.186170\n",
      "4        1.0      0.604651    0.510870  0.520556      0.148810  0.212766\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original',\n",
    "                   delim_whitespace=True, header=None,\n",
    "                   names = ['mpg', 'cylinders', 'displacement',\n",
    "                            'horsepower', 'weight', 'acceleration',\n",
    "                            'model year', 'origin', 'car name'])\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.drop(['model year', 'origin', 'car name'], axis=1, inplace=True)\n",
    "dataset = dataset[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'mpg']]\n",
    "print(dataset.shape)\n",
    "\n",
    "x = dataset.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "dataset = pd.DataFrame(x_scaled, columns=['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'mpg'])\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7be73f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'ACE of $cylinders$ on $mpg$')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = np.linspace(0,1,392)\n",
    "y_est = np.polyfit(dataset['cylinders'], dataset['mpg'], 2)\n",
    "gtc = np.polyval(y_est, x_axis)\n",
    "plt.plot(gtc-np.mean(gtc))\n",
    "plt.xlabel('intervention on $cylinders$')\n",
    "plt.ylabel('ACE of $cylinders$ on $mpg$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a65f03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'ACE of $weight$ on $mpg$')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gtw = []\n",
    "inp = dataset[['weight', 'cylinders']]\n",
    "out = dataset['mpg']\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "inp = poly.fit_transform(inp)\n",
    "clf = linear_model.Ridge()\n",
    "\n",
    "model = clf.fit(np.array(inp), np.array(out))\n",
    "\n",
    "for alpha in np.linspace(0,1,392):\n",
    "    df1 = pd.DataFrame.copy(dataset[['weight', 'cylinders']])\n",
    "    df1['weight'] = alpha\n",
    "    df1 = poly.transform(df1)\n",
    "    gtw.append(np.mean(model.predict(df1)))\n",
    "\n",
    "plt.plot(gtw-np.mean(gtw))\n",
    "plt.xlabel('intervention on $weight$')\n",
    "plt.ylabel('ACE of $weight$ on $mpg$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37bba4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'ACE of $displacement$ on $mpg$')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gtd = []\n",
    "inp = dataset[['displacement', 'cylinders']]\n",
    "out = dataset['mpg']\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "inp = poly.fit_transform(inp)\n",
    "clf = linear_model.Ridge()\n",
    "\n",
    "model = clf.fit(np.array(inp), np.array(out))\n",
    "\n",
    "for alpha in np.linspace(0,1,392):\n",
    "    df1 = pd.DataFrame.copy(dataset[['displacement','cylinders']])\n",
    "    df1['displacement'] = alpha\n",
    "    df1 = poly.transform(df1)\n",
    "    gtd.append(np.mean(model.predict(df1)))\n",
    "\n",
    "plt.plot(gtd-np.mean(gtd))\n",
    "plt.xlabel('intervention on $displacement$')\n",
    "plt.ylabel('ACE of $displacement$ on $mpg$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7e16c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'ACE of $acceleration$ on $mpg$')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gta = []\n",
    "inp = dataset[['acceleration', 'weight', 'horsepower']]\n",
    "out = dataset['mpg']\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "inp = poly.fit_transform(inp)\n",
    "clf = linear_model.Ridge()\n",
    "\n",
    "model = clf.fit(np.array(inp), np.array(out))\n",
    "\n",
    "for alpha in np.linspace(0,1,392):\n",
    "    df1 = pd.DataFrame.copy(dataset[['acceleration', 'weight', 'horsepower']])\n",
    "    df1['acceleration'] = alpha\n",
    "    df1 = poly.transform(df1)\n",
    "    gta.append(np.mean(model.predict(df1)))\n",
    "\n",
    "plt.plot(gta-np.mean(gta))\n",
    "plt.xlabel('intervention on $acceleration$')\n",
    "plt.ylabel('ACE of $acceleration$ on $mpg$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1305fccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'ACE of $horsepower$ on $mpg$')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gth = []\n",
    "inp = dataset[['horsepower','cylinders']]\n",
    "out = dataset['mpg']\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "inp = poly.fit_transform(inp)\n",
    "clf = linear_model.Ridge()\n",
    "\n",
    "model = clf.fit(np.array(inp), np.array(out))\n",
    "\n",
    "for alpha in np.linspace(0,1,392):\n",
    "    df1 = pd.DataFrame.copy(dataset[['horsepower','cylinders']])\n",
    "    df1['horsepower'] = alpha\n",
    "    df1 = poly.transform(df1)\n",
    "    gth.append(np.mean(model.predict(df1)))\n",
    "\n",
    "plt.plot(gth-np.mean(gth))\n",
    "plt.xlabel('intervention on $horsepower$')\n",
    "plt.ylabel('ACE of $horsepower$ on $mpg$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1519a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aces_gt=[]\n",
    "aces_gt.append(gtc-np.mean(gtc))\n",
    "aces_gt.append(gtw-np.mean(gtw))\n",
    "aces_gt.append(gtd-np.mean(gtd))\n",
    "aces_gt.append(gta-np.mean(gth))\n",
    "aces_gt.append(gta-np.mean(gth))\n",
    "np.save('./aces/autompg_gt.npy',aces_gt,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3707cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to load data from csv file\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe1, dataframe2):\n",
    "        self.data_points = dataframe1\n",
    "        self.targets = dataframe2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_point = torch.tensor(np.array(self.data_points[idx]), dtype=torch.float)\n",
    "        target_point = torch.tensor(np.array(self.targets[idx]), dtype=torch.float)\n",
    "        return input_point, target_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3d0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, feature_dim, batch_size=1, device='cuda',sample_size=1000):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.causal_link_c_w = nn.Linear(1,1)\n",
    "        \n",
    "        self.causal_link_c_d = nn.Linear(1,1)\n",
    "        \n",
    "        self.causal_link_c_h = nn.Linear(1,1)\n",
    "        \n",
    "        self.causal_link_w_a = nn.Linear(1,1)\n",
    "\n",
    "        self.causal_link_d_a = nn.Linear(1,1)\n",
    "\n",
    "        self.causal_link_d_h = nn.Linear(1,1)\n",
    "        \n",
    "        self.causal_link_h_a = nn.Linear(1,1)\n",
    "        \n",
    "        self.first_layer = nn.Linear(5,16)\n",
    "        self.second_layer = nn.Linear(16,8)\n",
    "        self.third_layer = nn.Linear(8,8)\n",
    "        self.fourth_layer = nn.Linear(8,4)\n",
    "        self.regression_layer = nn.Linear(4, 1)\n",
    "        self.sample_size = sample_size\n",
    "        self.batch_norm1 = nn.BatchNorm1d(16)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(8)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(8)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(4)\n",
    "        \n",
    "    def forward(self, inp, phase='freeze', inde=0, alpha=0):\n",
    "        if phase=='freeze':\n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            x = F.relu(self.fourth_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "            return prediction\n",
    "        elif phase=='train_dag':\n",
    "            c_sample = inp[:,0].reshape(self.batch_size,-1)\n",
    "            w_sample = self.causal_link_c_w(c_sample)\n",
    "            d_sample = self.causal_link_c_d(c_sample)\n",
    "            h_sample = self.causal_link_c_h(c_sample) + self.causal_link_d_h(d_sample) \n",
    "            a_sample = self.causal_link_w_a(w_sample)+self.causal_link_h_a(h_sample)\n",
    "            \n",
    "            inp = torch.cat((c_sample,d_sample,h_sample,w_sample,a_sample),dim=1)\n",
    "            \n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            x = F.relu(self.fourth_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "            return prediction, d_sample, h_sample, w_sample, a_sample \n",
    "        elif phase=='sample':\n",
    "            # c-0,d-1,h-2,w-3,a-4\n",
    "            if inde == 0:\n",
    "                c_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                w_sample = self.causal_link_c_w(torch.tensor(c_sample, dtype=torch.float))\n",
    "                d_sample = self.causal_link_c_d(torch.tensor(c_sample, dtype=torch.float))\n",
    "                h_sample = self.causal_link_c_h(c_sample)+self.causal_link_d_h(d_sample)\n",
    "                a_sample = self.causal_link_w_a(w_sample)+self.causal_link_h_a(h_sample)\n",
    "                inp = torch.cat((c_sample,d_sample,h_sample,w_sample,a_sample),dim=1)\n",
    "                return inp\n",
    "            \n",
    "            elif inde == 1:\n",
    "                c_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                w_sample = torch.tensor(inp[:,3].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "\n",
    "                d_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                h_sample = self.causal_link_c_h(torch.tensor(c_sample,dtype=torch.float))+self.causal_link_d_h(d_sample)\n",
    "                a_sample = self.causal_link_w_a(w_sample)+self.causal_link_h_a(h_sample)\n",
    "                inp = torch.cat((c_sample,d_sample,h_sample,w_sample,a_sample),dim=1)\n",
    "                return inp\n",
    "            elif inde == 2:\n",
    "                c_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                w_sample = torch.tensor(inp[:,3].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                d_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                h_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                a_sample = self.causal_link_w_a(torch.tensor(w_sample,dtype=torch.float))+self.causal_link_h_a(torch.tensor(h_sample,dtype=torch.float))\n",
    "\n",
    "                inp = torch.cat((c_sample,d_sample,h_sample,w_sample,a_sample),dim=1)\n",
    "                return inp\n",
    "            elif inde == 3:\n",
    "                c_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                d_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                h_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                \n",
    "                w_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                a_sample = self.causal_link_w_a(torch.tensor(w_sample,dtype=torch.float))+self.causal_link_h_a(torch.tensor(h_sample,dtype=torch.float))\n",
    "                inp = torch.cat((c_sample,d_sample,h_sample,w_sample,a_sample),dim=1)\n",
    "                return inp\n",
    "            elif inde == 4:\n",
    "                c_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                w_sample = torch.tensor(inp[:,3].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                d_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                h_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                a_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                inp = torch.cat((c_sample,d_sample,h_sample,w_sample,a_sample),dim=1)\n",
    "                return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df86b96",
   "metadata": {},
   "source": [
    "# Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124435c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size=5 # to get mean and std values\n",
    "batch_size = 1\n",
    "\n",
    "values = list(dataset.columns.values)\n",
    "y = dataset[values[-1:]]\n",
    "y = np.array(y, dtype='float32')\n",
    "X = dataset[values[:-1]]\n",
    "X = np.array(X, dtype='float32')\n",
    "\n",
    "indices = np.random.choice(len(X), len(X), replace=False)\n",
    "X_values = X[indices]\n",
    "y_values = y[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 50\n",
    "val_size = 20\n",
    "\n",
    "interval = 5\n",
    "epoch = 20\n",
    "\n",
    "X_test = X_values[-test_size:]\n",
    "X_trainval = X_values[:-test_size]\n",
    "X_val = X_trainval[-val_size:]\n",
    "X_train = X_trainval[:-val_size]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "y_test = y_values[-test_size:]\n",
    "y_trainval = y_values[:-test_size]\n",
    "y_val = y_trainval[-val_size:]\n",
    "y_train = y_trainval[:-val_size]\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8a421",
   "metadata": {},
   "source": [
    "# ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf0b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "\n",
    "    # Interval / Epochs\n",
    "    trainval = DataSet(X_train,y_train)\n",
    "    train_loader = DataLoader(trainval, batch_size=batch_size)\n",
    "    \n",
    "    testval = DataSet(X_test,y_test)\n",
    "    test_loader = DataLoader(testval, batch_size=batch_size)\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "    erm_model = Model(5,sample_size=len(dataset))\n",
    "    optimizer = optim.Adam([{'params': erm_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "    losses = []\n",
    "\n",
    "    for ep in range(0,epoch): \n",
    "        loss_val = 0\n",
    "        for input_data, target in train_loader:\n",
    "            erm_model.zero_grad()\n",
    "            input_data.requires_grad=True\n",
    "            output = erm_model(input_data)\n",
    "            loss = torch.sqrt(loss_func(output,target))\n",
    "            loss_val += loss\n",
    "\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        if epoch%interval == 0:\n",
    "            print ('train_loss:', loss_val.item()/len(train_loader))\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=16)\n",
    "            val_loss = 0\n",
    "            for input_data, target in val_loader:\n",
    "                output = erm_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                val_loss += loss\n",
    "            print ('validation_loss:', val_loss/len(val_loader))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            loss_val = 0\n",
    "            for input_data, target in test_loader:\n",
    "                output = erm_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                loss_val += loss\n",
    "            print('test loss:', loss_val/len(test_loader))\n",
    "            print()\n",
    "    print(\"************\")\n",
    "    torch.save(erm_model, \"models/erm_autompg_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48347654",
   "metadata": {},
   "source": [
    "# Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bde629b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    model = torch.load(\"models/erm_autompg_\"+str(ensemble+1))\n",
    "    ig = IntegratedGradients(model)\n",
    "    \n",
    "    data_c = dataset['cylinders'].values\n",
    "    data_d = dataset['displacement'].values\n",
    "    data_h = dataset['horsepower'].values\n",
    "    data_w = dataset['weight'].values\n",
    "    data_a = dataset['acceleration'].values\n",
    "    do_c = np.linspace(min(dataset['cylinders']), max(dataset['cylinders']), 392)\n",
    "    do_d = np.linspace(min(dataset['displacement']), max(dataset['displacement']), 392)\n",
    "    do_h = np.linspace(min(dataset['horsepower']), max(dataset['horsepower']), 392)\n",
    "    do_w = np.linspace(min(dataset['weight']), max(dataset['weight']), 392)\n",
    "    do_a = np.linspace(min(dataset['acceleration']), max(dataset['acceleration']), 392)\n",
    "    test_array_c = np.stack((do_c, data_d, data_h, data_w, data_a), axis=1)\n",
    "    test_array_c = torch.from_numpy(test_array_c).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_d = np.stack((data_c, do_d, data_h, data_w, data_a), axis=1)\n",
    "    test_array_d = torch.from_numpy(test_array_d).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_h = np.stack((data_c, data_d, do_h, data_w, data_a), axis=1)\n",
    "    test_array_h = torch.from_numpy(test_array_h).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_w = np.stack((data_c, data_d, data_h, do_w, data_a), axis=1)\n",
    "    test_array_w = torch.from_numpy(test_array_w).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_a = np.stack((data_c, data_d, data_h, data_w, do_a), axis=1)\n",
    "    test_array_a = torch.from_numpy(test_array_a).type(torch.FloatTensor)\n",
    "    \n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    ig_attr_test_c, delta = ig.attribute(test_array_c, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_d, delta = ig.attribute(test_array_d, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_h, delta = ig.attribute(test_array_h, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_w, delta = ig.attribute(test_array_w, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_a, delta = ig.attribute(test_array_a, n_steps=50, return_convergence_delta=True)\n",
    "    print(ensemble)\n",
    "    rmse_results.append([rmse(aces_gt[0], np.array(ig_attr_test_c[:,0])),\n",
    "                         rmse(aces_gt[1], np.array(ig_attr_test_d[:,1])),\n",
    "                         rmse(aces_gt[2], np.array(ig_attr_test_h[:,2])),\n",
    "                         rmse(aces_gt[3], np.array(ig_attr_test_w[:,3])),\n",
    "                         rmse(aces_gt[4], np.array(ig_attr_test_a[:,4]))])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(392,1), ig_attr_test_c[:,0].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(392,1), ig_attr_test_d[:,1].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(392,1), ig_attr_test_h[:,2].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(392,1), ig_attr_test_w[:,3].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(392,1), ig_attr_test_a[:,4].reshape(392,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "33859973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.1285353  0.11690189 0.2168338  0.27248046 0.07468748]\n",
      "rmse std:  [0.00871158 0.00544642 0.02347835 0.04847185 0.01910516]\n",
      "rmse all features mean:  0.16188778504611037\n",
      "rmse all features std:  0.021042672343352546\n",
      "frechet mean:  [0.27585241 0.25436556 0.25755731 0.4519391  0.12018562]\n",
      "frechet std:  [0.         0.         0.02428228 0.08871915 0.01501826]\n",
      "frechet all features mean:  0.27198000107628245\n",
      "frechet all features std:  0.02560393915491973\n"
     ]
    }
   ],
   "source": [
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d55fc5",
   "metadata": {},
   "source": [
    "# Causal Attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3c88505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=1\n",
    "num_feat=5\n",
    "num_alpha=392\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_ca_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_ca_total = []\n",
    "    model = torch.load(\"models/erm_autompg_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_ca_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "    aces_ca_total.append(ace_ca_total)\n",
    "np.save('./aces/autompg_ca_total.npy',aces_ca_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2fcc7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_ca_total[ensemble][0]),rmse(aces_gt[1], aces_ca_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_ca_total[ensemble][2]),rmse(aces_gt[3], aces_ca_total[ensemble][3]),\n",
    "                         rmse(aces_gt[4], aces_ca_total[ensemble][4])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(392,1), aces_ca_total[ensemble][0].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(392,1), aces_ca_total[ensemble][1].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(392,1), aces_ca_total[ensemble][2].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(392,1), aces_ca_total[ensemble][3].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(392,1), aces_ca_total[ensemble][4].reshape(392,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d46dc24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.13558165 0.11723435 0.04366154 0.09085574 0.07594673]\n",
      "rmse std:  [0.0077085  0.00896583 0.01012463 0.00735457 0.00080118]\n",
      "rmse all features mean:  0.09265600325398186\n",
      "rmse all features std:  0.006990944764798526\n",
      "frechet mean:  [0.25642532 0.21343595 0.0793606  0.15774682 0.09690216]\n",
      "frechet std:  [0.00596437 0.01305097 0.02004061 0.02014844 0.01371071]\n",
      "frechet all features mean:  0.16077416909599246\n",
      "frechet all features std:  0.0145830202623532\n"
     ]
    }
   ],
   "source": [
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5bbb2",
   "metadata": {},
   "source": [
    "# Causal Shapley Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3aa2e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability is taken over indices of baseline only\n",
    "def get_probabiity(unique_count, x_hat, indices_baseline, n):\n",
    "    if len(indices_baseline) > 0:\n",
    "        count = 0\n",
    "        for i in unique_count:\n",
    "            check = True\n",
    "            key = np.asarray(i)\n",
    "            for j in indices_baseline:\n",
    "                check = check and key[j] == x_hat[j]\n",
    "            if check:\n",
    "                count += unique_count[i]\n",
    "        return count / n\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def conditional_prob(unique_count, x_hat, indices, indices_baseline, n):\n",
    "    numerator_indices = indices + indices_baseline\n",
    "    numerator = get_probabiity(unique_count, x_hat, numerator_indices, n)\n",
    "    denominator = get_probabiity(unique_count, x_hat, indices, n)\n",
    "    try:\n",
    "        kk = numerator / denominator\n",
    "    except ZeroDivisionError:\n",
    "        denominator = 1e-7\n",
    "        # pass\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def causal_prob(unique_count, x_hat, indices, indices_baseline, causal_struc, n):\n",
    "    p = 1\n",
    "    for i in indices_baseline:\n",
    "        intersect_s, intersect_s_hat = [], []\n",
    "        intersect_s_hat.append(i)\n",
    "        if len(causal_struc[str(i)]) > 0:\n",
    "            for index in causal_struc[str(i)]:\n",
    "                if index in indices or index in indices_baseline:\n",
    "                    intersect_s.append(index)\n",
    "            p *= conditional_prob(unique_count, x_hat, intersect_s, intersect_s_hat, n)\n",
    "        else:\n",
    "            p *= get_probabiity(unique_count, x_hat, intersect_s_hat, n)\n",
    "    return p\n",
    "\n",
    "def get_baseline(X, model):\n",
    "    fx = 0\n",
    "    n_features = X.shape[1]\n",
    "    X = np.reshape(X, (len(X), 1, n_features))\n",
    "    for i in X:\n",
    "        fx += model(torch.tensor(i, dtype=torch.float))\n",
    "    return fx / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "08ed7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns value from using function for different versions\n",
    "def get_value(version, permutation, X, x, unique_count, causal_struct, model, N, xi):\n",
    "    # intializing returns\n",
    "    absolute_diff, f1, f2 = 0, 0, 0\n",
    "    xi_index = permutation.index(xi)\n",
    "    indices = permutation[:xi_index + 1]\n",
    "    indices_baseline = permutation[xi_index + 1:]\n",
    "    x_hat = np.zeros(N)\n",
    "    x_hat_2 = np.zeros(N)\n",
    "    len_X = len(X)\n",
    "    for j in indices:\n",
    "        x_hat[j] = x[j]\n",
    "        x_hat_2[j] = x[j]\n",
    "    if version == '2' or version == '3' or version == '4':\n",
    "        proba1, proba2 = 0, 0\n",
    "        baseline_check_1, baseline_check_2 = [], []\n",
    "        f1, f2 = 0, 0\n",
    "        indices_baseline_2 = indices_baseline[:]\n",
    "        for i in unique_count:\n",
    "            X = np.asarray(i)\n",
    "            for j in indices_baseline:\n",
    "                x_hat[j] = X[j]\n",
    "                x_hat_2[j] = X[j]\n",
    "\n",
    "            # No repetition\n",
    "            # Eg if baseline_indices is null, it'll only run once as x_hat will stay the same over each iteration\n",
    "            if x_hat.tolist() not in baseline_check_1:\n",
    "                baseline_check_1.append(x_hat.tolist())\n",
    "                if version == '2':\n",
    "                    prob_x_hat = get_probabiity(unique_count, x_hat, indices_baseline, len_X)\n",
    "                elif version == '3':\n",
    "                    prob_x_hat = conditional_prob(unique_count, x_hat, indices, indices_baseline, len_X)\n",
    "                else:\n",
    "                    prob_x_hat = causal_prob(unique_count, x_hat, indices, indices_baseline, causal_struct, len_X)\n",
    "                proba1 += prob_x_hat\n",
    "                x_hat = np.reshape(x_hat, (1, N))\n",
    "                # print(x_hat.shape)\n",
    "                f1 = f1 + (model(torch.tensor(x_hat, dtype=torch.float)) * prob_x_hat)\n",
    "\n",
    "            # xi index will be given to baseline for f2\n",
    "            x_hat_2[xi] = X[xi]\n",
    "            if xi not in indices_baseline_2:\n",
    "                indices_baseline_2.append(xi)\n",
    "\n",
    "            # No repetition\n",
    "            indices_2 = indices[:]\n",
    "            indices_2.remove(xi)\n",
    "            if x_hat_2.tolist() not in baseline_check_2:\n",
    "                baseline_check_2.append(x_hat_2.tolist())\n",
    "                if version == '2':\n",
    "                    prob_x_hat_2 = get_probabiity(unique_count, x_hat_2, indices_baseline_2, len_X)\n",
    "                elif version == '3':\n",
    "                    prob_x_hat_2 = conditional_prob(unique_count, x_hat_2, indices_2, indices_baseline_2, len_X)\n",
    "                else:\n",
    "                    prob_x_hat_2 = causal_prob(unique_count, x_hat_2, indices_2, indices_baseline_2, causal_struct,\n",
    "                                               len_X)\n",
    "                proba2 += prob_x_hat_2\n",
    "                x_hat_2 = np.reshape(x_hat_2, (1, N))\n",
    "                f2 = f2 + model(torch.tensor(x_hat_2, dtype=torch.float)) * prob_x_hat_2\n",
    "            x_hat = np.squeeze(x_hat)\n",
    "            x_hat_2 = np.squeeze(x_hat_2)\n",
    "        absolute_diff = abs(f1 - f2)\n",
    "    elif version == '1':\n",
    "        f1, f2 = 0, 0\n",
    "        for i in range(len(X)):\n",
    "            for j in indices_baseline:\n",
    "                x_hat[j] = X[i][j]\n",
    "                x_hat_2[j] = X[i][j]\n",
    "            x_hat = np.reshape(x_hat, (1, N))\n",
    "            f1 += model(torch.tensor(x_hat, dtype=torch.float))\n",
    "            x_hat_2[xi] = X[i][xi]\n",
    "            x_hat_2 = np.reshape(x_hat_2, (1, N))\n",
    "            f2 += model(torch.tensor(x_hat_2, dtype=torch.float))\n",
    "            x_hat = np.squeeze(x_hat)\n",
    "            x_hat_2 = np.squeeze(x_hat_2)\n",
    "        absolute_diff = abs(f1 - f2) / len_X\n",
    "        f1 = f1 / len_X\n",
    "        f2 = f2 / len_X\n",
    "    return absolute_diff, f1, f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e7bcb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_shapley(version, xi, N, X, x, m, model, unique_count, causal_struct,\n",
    "                        global_shap=False):\n",
    "    R = list(itertools.permutations(range(N)))\n",
    "    random.shuffle(R)\n",
    "    score = 0\n",
    "    count_negative = 0\n",
    "    vf1, vf2 = 0, 0\n",
    "    for i in range(m):\n",
    "        abs_diff, f1, f2 = get_value(version, list(R[i]), X, x, unique_count, causal_struct, model, N, xi)\n",
    "        vf1 += f1\n",
    "        vf2 += f2\n",
    "        score += abs_diff\n",
    "        if not global_shap:\n",
    "            if vf2 > vf1:\n",
    "                count_negative -= 1\n",
    "            else:\n",
    "                count_negative += 1\n",
    "    if count_negative < 0 and not global_shap:\n",
    "        score = -1 * score\n",
    "    return score / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e075a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapley(model, version, local_shap=0):\n",
    "    sigma_phi = 0\n",
    "    global_shap=True\n",
    "    causal_struct = None\n",
    "    try:\n",
    "        causal_struct = json.load(open('autompg.json', 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    n_features = 5\n",
    "    unique_count = collections.Counter(map(tuple, X_train))\n",
    "    X = torch.tensor(X_values, dtype=torch.float)\n",
    "    ##### f(x) with baseline\n",
    "    f_o = get_baseline(X, model)[0]\n",
    "    rmse_shapley_values = []\n",
    "    frechet_shapley_values = []\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        global_shap_score = Parallel(n_jobs=-1)(\n",
    "            delayed(approximate_shapley)(version, feature, n_features, X, x, math.factorial(n_features), model,\n",
    "                                         unique_count, causal_struct, global_shap) for i, x in\n",
    "            enumerate(X))\n",
    "        \n",
    "        rmse_shapley_values.append(rmse(aces_gt[feature], np.array([i.detach().numpy()[0][0] for i in global_shap_score])))\n",
    "        frechet_shapley_values.append(frechet_dist(aces_gt[feature].reshape(-1,1), np.array([i.detach().numpy()[0][0] for i in global_shap_score]).reshape(-1,1)))\n",
    "    return rmse_shapley_values, frechet_shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "for ensemble in range(ensemble_size):\n",
    "    print(ensemble)\n",
    "    model = torch.load(\"models/erm_autompg_\"+str(ensemble+1))\n",
    "    r, f = shapley(model, version='4', local_shap=12)\n",
    "    rmse_results.append(r)\n",
    "    frechet_results.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "04ab64fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.20954783 0.20049425 0.17444317 0.05626573 0.02644929]\n",
      "rmse std:  [0.00129166 0.00104488 0.00065756 0.00080104 0.00116919]\n",
      "rmse all features mean:  0.13344005476287013\n",
      "rmse all features std:  0.0009928636492865875\n",
      "frechet mean:  [0.37280938 0.38196421 0.30778307 0.06942045 0.06425604]\n",
      "frechet std:  [0.00670653 0.00608737 0.00495012 0.0022481  0.01037824]\n",
      "frechet all features mean:  0.23924662873472197\n",
      "frechet all features std:  0.006074068545254802\n"
     ]
    }
   ],
   "source": [
    "np.save('./models/cshapresults.npy', np.array([rmse_results, frechet_results]))\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceee87a",
   "metadata": {},
   "source": [
    "# CREDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "396b6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREDO prior\n",
    "prior = {0:(lambda ii:-ii), 1:(lambda ii:-ii), 2:(lambda ii:-ii), 3:(lambda ii:-ii), 4:(lambda ii:-ii)}\n",
    "def get_grad(x, prior):\n",
    "    a = x.clone().detach().requires_grad_(True)\n",
    "    for f in prior.keys():\n",
    "        z = prior[f]\n",
    "        z = torch.sum(z(a[0][f]), dim=0)\n",
    "        z.backward()\n",
    "    return a.grad\n",
    "\n",
    "def get_grads_to_match(ip, prior):\n",
    "    return get_grad(ip, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f54937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.002078382124811966\n",
      "validation_loss: tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0014828767465508502\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0109, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002098568663093614\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0020934724659653183\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0109, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0020800429101316084\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0019733987239577014\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002156500490555852\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0020414191003171552\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0019141404154878225\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0018101034697538577\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0017128449789485577\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0016256533794521545\n",
      "validation_loss: tensor(0.0107, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0015748513411290898\n",
      "validation_loss: tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0162, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0014874357614457977\n",
      "validation_loss: tensor(0.0158, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0171, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0014156129049218218\n",
      "validation_loss: tensor(0.0211, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0176, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0013889889724506355\n",
      "validation_loss: tensor(0.0280, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0182, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0013217319798025284\n",
      "validation_loss: tensor(0.0390, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0203, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0012929674631320172\n",
      "validation_loss: tensor(0.0507, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0217, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0012557494529285786\n",
      "validation_loss: tensor(0.0484, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0203, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0010779276027442506\n",
      "validation_loss: tensor(0.0471, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0197, grad_fn=<DivBackward0>)\n",
      "***********\n",
      "train_loss: 0.0024164465273389165\n",
      "validation_loss: tensor(0.0194, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0011428124415948525\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0019245384642796487\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001675060817173549\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0014805162545316708\n",
      "validation_loss: tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0112, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0015470503094773855\n",
      "validation_loss: tensor(0.0254, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0186, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0020633310264682177\n",
      "validation_loss: tensor(0.0217, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0019792310939812513\n",
      "validation_loss: tensor(0.0253, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0018762187187716087\n",
      "validation_loss: tensor(0.0381, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0022050801271237203\n",
      "validation_loss: tensor(0.0292, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002193914066930735\n",
      "validation_loss: tensor(0.0356, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0021326688135632817\n",
      "validation_loss: tensor(0.0278, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0019852420928315345\n",
      "validation_loss: tensor(0.0332, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001648031961843834\n",
      "validation_loss: tensor(0.0358, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0112, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001566408213621341\n",
      "validation_loss: tensor(0.0433, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0021441482609103183\n",
      "validation_loss: tensor(0.0394, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002175426631240371\n",
      "validation_loss: tensor(0.0363, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0021974481410861756\n",
      "validation_loss: tensor(0.0321, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0113, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002222107063909495\n",
      "validation_loss: tensor(0.0309, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0107, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002253238458811126\n",
      "validation_loss: tensor(0.0301, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "***********\n",
      "train_loss: 0.0023187669167607466\n",
      "validation_loss: tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0119, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002101659589672681\n",
      "validation_loss: tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0020157537475135757\n",
      "validation_loss: tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001786408217056938\n",
      "validation_loss: tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001743728514783871\n",
      "validation_loss: tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0114, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0015014571062526348\n",
      "validation_loss: tensor(0.0286, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0015129434025805929\n",
      "validation_loss: tensor(0.0290, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001569551340541484\n",
      "validation_loss: tensor(0.0274, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0015698304827909291\n",
      "validation_loss: tensor(0.0274, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0013332762148069298\n",
      "validation_loss: tensor(0.0378, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0011029118521613363\n",
      "validation_loss: tensor(0.0368, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0167, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0018876054272148179\n",
      "validation_loss: tensor(0.0335, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0165, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0019219804624593035\n",
      "validation_loss: tensor(0.0315, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001977620657926761\n",
      "validation_loss: tensor(0.0348, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001997864579562075\n",
      "validation_loss: tensor(0.0321, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002015029051289055\n",
      "validation_loss: tensor(0.0342, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002027476611344711\n",
      "validation_loss: tensor(0.0354, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0020323767795325806\n",
      "validation_loss: tensor(0.0346, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002039991365456433\n",
      "validation_loss: tensor(0.0306, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.002033829689025879\n",
      "validation_loss: tensor(0.0307, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "***********\n",
      "train_loss: 0.00208287042860659\n",
      "validation_loss: tensor(0.0169, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0108, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001846046181198973\n",
      "validation_loss: tensor(0.0045, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0017501367545275956\n",
      "validation_loss: tensor(0.0039, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0108, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0018526671095664457\n",
      "validation_loss: tensor(0.0045, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001427949215314403\n",
      "validation_loss: tensor(0.0052, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001330696277736877\n",
      "validation_loss: tensor(0.0155, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0166, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0013126303320345671\n",
      "validation_loss: tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001159295069504969\n",
      "validation_loss: tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0011288958676853535\n",
      "validation_loss: tensor(0.0155, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0012465379438044862\n",
      "validation_loss: tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0012609543082136546\n",
      "validation_loss: tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0182, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0010985093457358225\n",
      "validation_loss: tensor(0.0176, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0202, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0011294954311773643\n",
      "validation_loss: tensor(0.0176, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0201, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0012096708050425748\n",
      "validation_loss: tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0196, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0013594756955685823\n",
      "validation_loss: tensor(0.0192, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0232, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0012136321075214363\n",
      "validation_loss: tensor(0.0180, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0221, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0009867460090921532\n",
      "validation_loss: tensor(0.0212, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0244, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.000966720792077343\n",
      "validation_loss: tensor(0.0220, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0258, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0010418982609458592\n",
      "validation_loss: tensor(0.0205, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0245, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0010079550817146065\n",
      "validation_loss: tensor(0.0208, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0251, grad_fn=<DivBackward0>)\n",
      "***********\n",
      "train_loss: 0.0016397278501380304\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0016533987862723215\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0017086137895998747\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0116, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0018552693521013911\n",
      "validation_loss: tensor(0.0108, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0021081466852507975\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0016292129984553556\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0015466114193756389\n",
      "validation_loss: tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0013216532165219324\n",
      "validation_loss: tensor(0.0093, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002628936160425222\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0025338797095399465\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0024052199369631938\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002319212655843415\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0021974603581872785\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0021358935729317045\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.002098313029508413\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.00203977682575676\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.001882605671142199\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0017891907543869491\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0017180057786266255\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0020368386129414814\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "***********\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    \n",
    "    trainval = DataSet(X_train,y_train)\n",
    "    train_loader = DataLoader(trainval, batch_size=1)\n",
    "    \n",
    "    testval = DataSet(X_test,y_test)\n",
    "    test_loader = DataLoader(testval, batch_size=1)\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    credo_model = Model(5,sample_size=len(dataset))\n",
    "\n",
    "    optimizer = optim.Adam([{'params': credo_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "    losses = []\n",
    "    for ep in range(0,epoch): \n",
    "        loss_val = 0\n",
    "        for input_data, target in train_loader:\n",
    "            credo_model.zero_grad()\n",
    "            input_data.requires_grad=True\n",
    "            output = credo_model(input_data)\n",
    "\n",
    "            calc_grads = (autograd.grad(torch.sum(output, dim=0), input_data, retain_graph=True, create_graph=True)[0])\n",
    "            grads_to_match = get_grads_to_match(input_data, prior) \n",
    "            hinge_input = torch.abs(grads_to_match - calc_grads)\n",
    "            \n",
    "            loss = torch.sqrt(loss_func(output,target)) + 0.1 * torch.norm(torch.clamp(hinge_input, min=0), p=1)\n",
    "            loss_val = loss\n",
    "            \n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%interval == 0:\n",
    "            print ('train_loss:', loss_val.item()/len(train_loader))\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=16)\n",
    "            val_loss = 0\n",
    "            for input_data, target in val_loader:\n",
    "                output = credo_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                val_loss += loss\n",
    "            print ('validation_loss:', val_loss/len(val_loader))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            loss_val = 0\n",
    "            for input_data, target in test_loader:\n",
    "                output = credo_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                loss_val += loss\n",
    "            print('test loss_'+str(ensemble+1), loss_val/len(test_loader))\n",
    "    print(\"***********\")\n",
    "    torch.save(credo_model, \"./models/credo_autompg_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb32596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=1\n",
    "num_feat=5\n",
    "num_alpha=392\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_credo_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_credo_total = []\n",
    "    model = torch.load(\"models/credo_autompg_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_credo_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "    aces_credo_total.append(ace_credo_total)\n",
    "np.save('./aces/autompg_credo_total.npy',aces_credo_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f89357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_credo_total[ensemble][0]),rmse(aces_gt[1], aces_credo_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_credo_total[ensemble][2]),rmse(aces_gt[3], aces_credo_total[ensemble][3]),\n",
    "                         rmse(aces_gt[4], aces_credo_total[ensemble][4])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(392,1), aces_credo_total[ensemble][0].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(392,1), aces_credo_total[ensemble][1].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(392,1), aces_credo_total[ensemble][2].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(392,1), aces_credo_total[ensemble][3].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(392,1), aces_credo_total[ensemble][4].reshape(392,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de719c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.11067724 0.09911243 0.07775197 0.09264661 0.15092632]\n",
      "rmse std:  [0.02305305 0.02396792 0.02067706 0.02103478 0.05827141]\n",
      "rmse all features mean:  0.10622291302961084\n",
      "rmse all features std:  0.029400844157543904\n",
      "frechet mean:  [0.22518257 0.19144516 0.1593951  0.17079422 0.33543197]\n",
      "frechet std:  [0.04330802 0.03868591 0.03527255 0.06285842 0.16111691]\n",
      "frechet all features mean:  0.21644980568098618\n",
      "frechet all features std:  0.06824836255924133\n"
     ]
    }
   ],
   "source": [
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fa84e",
   "metadata": {},
   "source": [
    "# AHCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f4c8ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.28678190337945214\n",
      "validation_loss: tensor(0.0850, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.1035, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.21747444579319924\n",
      "validation_loss: tensor(0.0494, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0657, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.17993449572450626\n",
      "validation_loss: tensor(0.0350, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0497, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.16015911694639218\n",
      "validation_loss: tensor(0.0269, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0401, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.14664161871679082\n",
      "validation_loss: tensor(0.0223, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0342, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.13623979224921753\n",
      "validation_loss: tensor(0.0192, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0304, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.12613088003596903\n",
      "validation_loss: tensor(0.0165, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0270, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.11478639685589334\n",
      "validation_loss: tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0235, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10413713632903485\n",
      "validation_loss: tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0209, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09653195090915846\n",
      "validation_loss: tensor(0.0109, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0192, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09128346650496773\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0177, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08791860734453853\n",
      "validation_loss: tensor(0.0097, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08576432222164936\n",
      "validation_loss: tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08443798633836071\n",
      "validation_loss: tensor(0.0093, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0162, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08350098059044121\n",
      "validation_loss: tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0158, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08295983261203174\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08252993873927904\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08213561514149541\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08183919420893888\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08152057221217185\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "train_loss: 6.2660332910762815\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0193, grad_fn=<DivBackward0>)\n",
      "train_loss: 5.7767754785762815\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0172, grad_fn=<DivBackward0>)\n",
      "train_loss: 5.315986206812888\n",
      "validation_loss: tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0159, grad_fn=<DivBackward0>)\n",
      "train_loss: 4.8750754409695265\n",
      "validation_loss: tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "train_loss: 4.4674985897467\n",
      "validation_loss: tensor(0.0099, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0160, grad_fn=<DivBackward0>)\n",
      "train_loss: 4.082696950213509\n",
      "validation_loss: tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.717042152926048\n",
      "validation_loss: tensor(0.0112, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0190, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.38365486097632\n",
      "validation_loss: tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0221, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.1403353673330745\n",
      "validation_loss: tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0244, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.9629328946889557\n",
      "validation_loss: tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0252, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.8076010757351515\n",
      "validation_loss: tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0272, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.664273658894604\n",
      "validation_loss: tensor(0.0178, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0290, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.5299472216493593\n",
      "validation_loss: tensor(0.0194, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0312, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.4022749432865877\n",
      "validation_loss: tensor(0.0211, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0331, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.2800095836568324\n",
      "validation_loss: tensor(0.0224, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0345, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.1604022861267467\n",
      "validation_loss: tensor(0.0237, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0360, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.047389628724282\n",
      "validation_loss: tensor(0.0248, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0371, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.9483432177431095\n",
      "validation_loss: tensor(0.0254, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0377, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.8690060443759704\n",
      "validation_loss: tensor(0.0254, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0374, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.7984059967609667\n",
      "validation_loss: tensor(0.0252, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0371, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09868467224310644\n",
      "validation_loss: tensor(0.0105, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0162, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08684341359582747\n",
      "validation_loss: tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0159, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08605692845694027\n",
      "validation_loss: tensor(0.0097, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0160, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08538803550767603\n",
      "validation_loss: tensor(0.0096, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0159, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08496656477081109\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0159, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08466176808991047\n",
      "validation_loss: tensor(0.0093, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0842206611396363\n",
      "validation_loss: tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08389247870593337\n",
      "validation_loss: tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0155, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08353931711327216\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08324258816168174\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08299712660890188\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08269820005997368\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.082412358396542\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0822518745564526\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08191365188693409\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08164436624657294\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0814153007839037\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08112254646253882\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08088076929127948\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08066067192125025\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0148, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.7491261736946817\n",
      "validation_loss: tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0213, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.6736677655522127\n",
      "validation_loss: tensor(0.0108, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0207, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.6151686010893829\n",
      "validation_loss: tensor(0.0108, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0205, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.5591943397285035\n",
      "validation_loss: tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0198, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.5045694860612384\n",
      "validation_loss: tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0195, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.4510409906043769\n",
      "validation_loss: tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0192, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.3981858721430998\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0184, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.346082509674641\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0181, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.2942283109108113\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0175, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.2429527140552212\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0172, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.1920439915627425\n",
      "validation_loss: tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0172, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.1420218900123738\n",
      "validation_loss: tensor(0.0105, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.092163370262762\n",
      "validation_loss: tensor(0.0107, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0167, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.0426301186129172\n",
      "validation_loss: tensor(0.0109, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0168, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9956162731099573\n",
      "validation_loss: tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0169, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.957774950110394\n",
      "validation_loss: tensor(0.0113, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0168, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9316119081485346\n",
      "validation_loss: tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0165, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9113510795261549\n",
      "validation_loss: tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8936891733489422\n",
      "validation_loss: tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8777565334154211\n",
      "validation_loss: tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0163, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0911877673605214\n",
      "validation_loss: tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0158, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08854526022206181\n",
      "validation_loss: tensor(0.0096, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0160, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08735563147882497\n",
      "validation_loss: tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0161, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.086518056644416\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0158, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08568036334114786\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0155, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08481729400824316\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08385046372502487\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08287967065846698\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08209734377653702\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08138762053495609\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08080451088662473\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08027250278070107\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07977299660629367\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07950049453640576\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07927991736749684\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07912347923894847\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07898193856944209\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07887349217574789\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07869992937360491\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07851869571282998\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8819850187123932\n",
      "validation_loss: tensor(0.0096, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8533365000849185\n",
      "validation_loss: tensor(0.0114, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.839771720933618\n",
      "validation_loss: tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8276735862589771\n",
      "validation_loss: tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8166449884450213\n",
      "validation_loss: tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8059822224682163\n",
      "validation_loss: tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7955521293308424\n",
      "validation_loss: tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7852092292738257\n",
      "validation_loss: tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7749281699613014\n",
      "validation_loss: tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7647220303553232\n",
      "validation_loss: tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.75488887810559\n",
      "validation_loss: tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7458751038734958\n",
      "validation_loss: tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7375730431598165\n",
      "validation_loss: tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7298405333335355\n",
      "validation_loss: tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7222332213976368\n",
      "validation_loss: tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7148138010723991\n",
      "validation_loss: tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7075062627377717\n",
      "validation_loss: tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7004447605298914\n",
      "validation_loss: tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6934717308660472\n",
      "validation_loss: tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.686645649975131\n",
      "validation_loss: tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0150, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.09012033332208669\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08466884068080358\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08221354395706461\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08058043296292702\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07973703538408931\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0792480907084779\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07884210977495087\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07870184264568068\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07845805742725823\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07822456122925563\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07812785509950626\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07794642004167071\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07785143763382242\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07765646750882546\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07756776987395672\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07747865167463788\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07731246355897892\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07719953311896473\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0770971227136458\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07693418064472839\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6965580341978843\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6769399702178766\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6675781344775087\n",
      "validation_loss: tensor(0.0116, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6619938915560705\n",
      "validation_loss: tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6569254620475058\n",
      "validation_loss: tensor(0.0119, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6524485238590596\n",
      "validation_loss: tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6481137009140867\n",
      "validation_loss: tensor(0.0116, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6437884502529357\n",
      "validation_loss: tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6395771192467731\n",
      "validation_loss: tensor(0.0113, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6355439654048185\n",
      "validation_loss: tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6318050882090693\n",
      "validation_loss: tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6280882462211277\n",
      "validation_loss: tensor(0.0109, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6244990188882958\n",
      "validation_loss: tensor(0.0108, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6210205362450262\n",
      "validation_loss: tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6176755916998253\n",
      "validation_loss: tensor(0.0105, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6142390944202494\n",
      "validation_loss: tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6111073938215742\n",
      "validation_loss: tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6079486823230056\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6049891880580357\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.602028793429736\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0835360829134165\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07957078211055779\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07837240444207043\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07786460544752039\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07747797047869759\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07719414278587199\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07700158350215935\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07684482550769119\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07657218719861522\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07641596823745633\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0762775136817316\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0761191859748793\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07609421273936397\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07585523735662425\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07579539873585198\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07561719787787206\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07547109023384425\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07534543623835403\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07525182806927225\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07515412680110577\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6066532490416343\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5978620067146254\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5943804438810171\n",
      "validation_loss: tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5918715577688276\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5894076424355833\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5872448275548331\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5850858036775767\n",
      "validation_loss: tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5828291140728116\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5807596052655523\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5787239785520186\n",
      "validation_loss: tensor(0.0093, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5766755572016935\n",
      "validation_loss: tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5746529620626698\n",
      "validation_loss: tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.572696188221807\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5708274604370875\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5690051724451669\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5671780983113354\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5653673254925272\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5635549414972341\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5617053108926146\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5599416412922166\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_1 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5818474337180949\n",
      "validation_loss: tensor(0.2794, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.3045, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.41348787864542896\n",
      "validation_loss: tensor(0.1392, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.1603, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.2622994132663893\n",
      "validation_loss: tensor(0.0650, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0827, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.19351914210349136\n",
      "validation_loss: tensor(0.0401, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0556, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.16923835707006987\n",
      "validation_loss: tensor(0.0300, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0436, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.1546818750985661\n",
      "validation_loss: tensor(0.0248, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0369, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.13825885701623764\n",
      "validation_loss: tensor(0.0198, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0301, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.11554466863596662\n",
      "validation_loss: tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0239, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09987941291761694\n",
      "validation_loss: tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0204, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09227157379529491\n",
      "validation_loss: tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0181, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08811201960403726\n",
      "validation_loss: tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08569006001727181\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08392047289735782\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08260399362315303\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0817354332586253\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08096402920551182\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08036663221276324\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07994259662509705\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07958297847961046\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0794234423903945\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.5100764872864905\n",
      "validation_loss: tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0175, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.1878938852630045\n",
      "validation_loss: tensor(0.0254, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0240, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.9020273907584433\n",
      "validation_loss: tensor(0.0291, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0267, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.6353092549010095\n",
      "validation_loss: tensor(0.0300, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0271, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.390891697095788\n",
      "validation_loss: tensor(0.0298, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0265, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.1781777328585985\n",
      "validation_loss: tensor(0.0301, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0262, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.9971089807356366\n",
      "validation_loss: tensor(0.0331, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0278, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.8292319730201863\n",
      "validation_loss: tensor(0.0328, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0270, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.6889314829192548\n",
      "validation_loss: tensor(0.0312, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0258, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.5628622303838315\n",
      "validation_loss: tensor(0.0298, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0249, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.442798496033094\n",
      "validation_loss: tensor(0.0276, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0233, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.3275059291294642\n",
      "validation_loss: tensor(0.0257, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0221, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.2168640705369274\n",
      "validation_loss: tensor(0.0236, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0207, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.1129364582322399\n",
      "validation_loss: tensor(0.0224, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0200, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.0206257127086569\n",
      "validation_loss: tensor(0.0215, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0195, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9457564265091226\n",
      "validation_loss: tensor(0.0201, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0186, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8895887291949728\n",
      "validation_loss: tensor(0.0190, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0178, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8474436694790858\n",
      "validation_loss: tensor(0.0180, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0172, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8142885006732822\n",
      "validation_loss: tensor(0.0171, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0166, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7876274630149699\n",
      "validation_loss: tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0161, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.09435494938252136\n",
      "validation_loss: tensor(0.0096, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08385523802004986\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08147887709718313\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0807920775798537\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08040344048731075\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08010100133670783\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07998037042084688\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07982797800383953\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07966357284451123\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07949140797490659\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07933389177973967\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07919820939531978\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07906541765106391\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0788199027872974\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07857946431414681\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07828189423365622\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0781450449309734\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07798351264148025\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0778188409272188\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07761388387739288\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7785361153738839\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7515175386985636\n",
      "validation_loss: tensor(0.0098, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7336684991113888\n",
      "validation_loss: tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7211115108513684\n",
      "validation_loss: tensor(0.0112, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.710757569496676\n",
      "validation_loss: tensor(0.0114, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7023423473287073\n",
      "validation_loss: tensor(0.0113, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.694668455893949\n",
      "validation_loss: tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.687854506214213\n",
      "validation_loss: tensor(0.0108, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6816106286848554\n",
      "validation_loss: tensor(0.0107, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6758566435819827\n",
      "validation_loss: tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6704310067692159\n",
      "validation_loss: tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.665252969872137\n",
      "validation_loss: tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6604720879785763\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6557190225731512\n",
      "validation_loss: tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6511154174804688\n",
      "validation_loss: tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6464201174907803\n",
      "validation_loss: tensor(0.0097, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6419948317249369\n",
      "validation_loss: tensor(0.0097, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6375968411842489\n",
      "validation_loss: tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6332736755750195\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6289783738414694\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08305320384339517\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07971645734324959\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07883215839077967\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.078511747514239\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07834878175154976\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07821458022786963\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07812411148355615\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07805579641590947\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07790016979904649\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07773408830536079\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07755419334269459\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0774736878294382\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07734764288671268\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07720162409432926\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07688718997173427\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07670395418723917\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07660179848996748\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07644824507814016\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07637058281750413\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07630296671612663\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6317123033985588\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6229195446701523\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6169985184758346\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.612501961844308\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6081548655255241\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6037719916112675\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5996305098444779\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5954207900148001\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5913175973832977\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5872477181949971\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5833379141292216\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5795529762410229\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.575692407833123\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5720328573854814\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5685692544309249\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5651366903174738\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5617456850798234\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5583023166064149\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5549406916458414\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5515970265643196\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07942767172866727\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07761676859411394\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07710994104420917\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07684257459936675\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07653810370782888\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07635659164523487\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07624986304999878\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07612016334296753\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07596779461973202\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07588692777645514\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07576984500292666\n",
      "validation_loss: tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07562604157820992\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07552620194713522\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07533043215733877\n",
      "validation_loss: tensor(0.0057, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0754302540180846\n",
      "validation_loss: tensor(0.0057, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07535453464673914\n",
      "validation_loss: tensor(0.0057, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07536170793616254\n",
      "validation_loss: tensor(0.0057, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07525869926310474\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07521632887561869\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07521211138423185\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5514110304554056\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5461885914299058\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.54256060108635\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0116, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5390143542556289\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5358038481718265\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5330105894100592\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.530686088230299\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.528325809455066\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5261067929475204\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5237618203488936\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0119, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.521697855884244\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5196902352090208\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5176185939622961\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5156447132181677\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.513580748753518\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5115899506562985\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5095875426108792\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5075440969526398\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5055529671426146\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5035894168830066\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0122, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.07628057610174144\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0119, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07565597865892493\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07553563206832602\n",
      "validation_loss: tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0754166597164936\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07532267837050538\n",
      "validation_loss: tensor(0.0057, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07527882119883662\n",
      "validation_loss: tensor(0.0057, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07519682890139752\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0751298525318596\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07503931862967354\n",
      "validation_loss: tensor(0.0055, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0119, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07498115634325869\n",
      "validation_loss: tensor(0.0055, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07499332901853953\n",
      "validation_loss: tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07484009547263198\n",
      "validation_loss: tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07487731246474366\n",
      "validation_loss: tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07479002460929918\n",
      "validation_loss: tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07477350412688641\n",
      "validation_loss: tensor(0.0053, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07469673630613718\n",
      "validation_loss: tensor(0.0053, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0746873180318323\n",
      "validation_loss: tensor(0.0053, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07466285421241144\n",
      "validation_loss: tensor(0.0053, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07459387571915337\n",
      "validation_loss: tensor(0.0053, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07452395539846479\n",
      "validation_loss: tensor(0.0053, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5028433473954289\n",
      "validation_loss: tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5007003405079338\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4987883478958414\n",
      "validation_loss: tensor(0.0057, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4969593308727193\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4951366163929057\n",
      "validation_loss: tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4935851008255289\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.492226120848093\n",
      "validation_loss: tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0123, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4910571175332395\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.48994199385554155\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.48884952142371896\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.48781419836956524\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.48678797372379656\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4858060564313616\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4847831015261064\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4838244515176145\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4828840457134365\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.48209997141583366\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4811814586568323\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.48040700403059494\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4795332725003639\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_2 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.3236133267420419\n",
      "validation_loss: tensor(0.1239, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.1442, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.29040067684576376\n",
      "validation_loss: tensor(0.1054, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.1249, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.2647696548367139\n",
      "validation_loss: tensor(0.0903, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.1092, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.24174475817946914\n",
      "validation_loss: tensor(0.0746, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0927, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.21849732961713897\n",
      "validation_loss: tensor(0.0588, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0759, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.19622420079959846\n",
      "validation_loss: tensor(0.0461, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0621, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.1809004671084955\n",
      "validation_loss: tensor(0.0377, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0527, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.16928634880492405\n",
      "validation_loss: tensor(0.0317, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0457, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.1586717522662619\n",
      "validation_loss: tensor(0.0270, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0400, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.14747492867226927\n",
      "validation_loss: tensor(0.0229, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0348, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.13324213916470545\n",
      "validation_loss: tensor(0.0185, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0293, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.12007590110257546\n",
      "validation_loss: tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0247, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.11015030760202349\n",
      "validation_loss: tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0217, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10229392673658289\n",
      "validation_loss: tensor(0.0112, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0198, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09564361809203344\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0181, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09128969204351768\n",
      "validation_loss: tensor(0.0093, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0168, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0890421363877954\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0162, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08711181071974476\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08566637809232155\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08457208124006757\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0151, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 4.692366511184977\n",
      "validation_loss: tensor(0.0220, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0340, grad_fn=<DivBackward0>)\n",
      "train_loss: 4.29642349148389\n",
      "validation_loss: tensor(0.0314, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0448, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.967597155837539\n",
      "validation_loss: tensor(0.0348, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0485, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.658272121263587\n",
      "validation_loss: tensor(0.0356, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0494, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.3608633479716614\n",
      "validation_loss: tensor(0.0352, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0488, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.0827634823248253\n",
      "validation_loss: tensor(0.0334, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0467, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.8412052889047943\n",
      "validation_loss: tensor(0.0319, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0449, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.6581870132351515\n",
      "validation_loss: tensor(0.0298, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0424, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.5323088272758154\n",
      "validation_loss: tensor(0.0283, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0405, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.435373626140334\n",
      "validation_loss: tensor(0.0287, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0407, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.3461497052115683\n",
      "validation_loss: tensor(0.0290, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0407, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.261369788128397\n",
      "validation_loss: tensor(0.0307, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0425, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.1781411496748837\n",
      "validation_loss: tensor(0.0334, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0454, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.0922544491217003\n",
      "validation_loss: tensor(0.0359, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0482, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.011402580308618\n",
      "validation_loss: tensor(0.0338, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0459, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.9340598538795613\n",
      "validation_loss: tensor(0.0310, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0428, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.8580663455939441\n",
      "validation_loss: tensor(0.0293, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0409, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.7823526133661685\n",
      "validation_loss: tensor(0.0277, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0392, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.7072609848117235\n",
      "validation_loss: tensor(0.0257, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0369, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.6319236992308812\n",
      "validation_loss: tensor(0.0240, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0350, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10432501609280984\n",
      "validation_loss: tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0191, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09292029149784065\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0171, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08977121151752354\n",
      "validation_loss: tensor(0.0098, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0167, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08841720130873022\n",
      "validation_loss: tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08742551625885578\n",
      "validation_loss: tensor(0.0093, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0161, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08656216260068905\n",
      "validation_loss: tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08577644158594357\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08514022234804142\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08470436949167193\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08439344797075166\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08418407795592124\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08403237265829715\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08380878045692207\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08364626487589771\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08348074610929311\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08330402611205297\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0831928134704969\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0830627168927874\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08290729759642797\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08277470132578975\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.5729033665627425\n",
      "validation_loss: tensor(0.0184, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0287, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.4808285162315606\n",
      "validation_loss: tensor(0.0196, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0301, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.404778521993886\n",
      "validation_loss: tensor(0.0187, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0290, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.3302546909877233\n",
      "validation_loss: tensor(0.0175, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0273, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.2571257597171002\n",
      "validation_loss: tensor(0.0163, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0257, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.18666834860855\n",
      "validation_loss: tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0244, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.121922937239179\n",
      "validation_loss: tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0232, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.0670003950225642\n",
      "validation_loss: tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0220, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.024230388380726\n",
      "validation_loss: tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0212, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9889191811129173\n",
      "validation_loss: tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0204, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9570853665748739\n",
      "validation_loss: tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0195, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.927973018669934\n",
      "validation_loss: tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0188, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9026297812136064\n",
      "validation_loss: tensor(0.0108, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0184, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8804133634389557\n",
      "validation_loss: tensor(0.0105, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0180, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8602165080005337\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0174, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8421213848990683\n",
      "validation_loss: tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0171, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8258311585609958\n",
      "validation_loss: tensor(0.0099, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0169, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.811328485145332\n",
      "validation_loss: tensor(0.0098, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0167, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7981551271047651\n",
      "validation_loss: tensor(0.0098, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0165, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7858376828780086\n",
      "validation_loss: tensor(0.0097, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0163, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.08457890504635639\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08381144008281068\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08340895398062949\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08320989075654782\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0830178349654867\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08287265730200347\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08275497033729316\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08260472220663699\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08248624742401313\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08233947043093096\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08224410299929033\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0821119272931022\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08207221653150476\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08199806094910048\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08184777727778654\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08176024656118074\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08162885867290615\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08153449230312561\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08143956912970692\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08135164154242285\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7758638370111122\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7650573920018925\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7552966479188907\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7458016057932599\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7370204570130532\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7283990042550224\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7201546141819924\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.712573744495463\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7056314219599185\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6988781283360831\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6922894116514218\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6864509345581813\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6814306981815315\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6767618404412121\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.672033369170953\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6674915574352194\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6629026424810753\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.658310599949049\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6537715366908482\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6491721964770962\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08256580518639606\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08180464276615877\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08135693117698527\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08113830282080987\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08108781139302698\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08096067061335403\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08087458521683023\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08078453703696684\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.080699180223927\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08059184743750909\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08044543769789038\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.080425244680843\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08036346909422312\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08030830999338849\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08019809841369249\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08012052500470085\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08002896042343993\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0799666280331819\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07989071911166173\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07978447031530535\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0140, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6458023260839237\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6411375436723602\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6367889783397225\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6325818648249466\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6285675712253737\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6246712726095448\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6208391367278484\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6170471380956425\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6133559801563713\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6096724989991751\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.606012285125922\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6024381744195215\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5988808862911248\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5954608798767469\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5921371175635676\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5887861074127766\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5855217216918187\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.582364194881842\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5793102572423331\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5762899173712879\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0808726956385263\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08023905309831134\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07989954504167071\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07978756235252997\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07969680928295443\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07957914601201596\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07954958210820737\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07940069044598881\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07934257554711763\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07927500090984084\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07926026338375873\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07917050545260032\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07912628695091105\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07909960184038056\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0790690783388126\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07904181273087212\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07893924239259328\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07890826870936045\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07890637913105651\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07885873687933691\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5742717292738257\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5712151853194148\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5679254472625922\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5648175352108404\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5617658247858841\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5587676622852775\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5557708266358938\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5528101002947884\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.549780093364834\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.546766956400427\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5437549093495244\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5407876435273923\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5377719002480833\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5348575189246895\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5321328299386161\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5293637862116654\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5266834519664694\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5239020400906201\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5213795063658531\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5189278170188761\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_3 tensor(0.0133, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.7077234398504222\n",
      "validation_loss: tensor(0.4841, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.5132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6441924764502863\n",
      "validation_loss: tensor(0.4211, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.4488, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6076564907287219\n",
      "validation_loss: tensor(0.3819, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.4089, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5754547119140625\n",
      "validation_loss: tensor(0.3448, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.3710, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5432530752620341\n",
      "validation_loss: tensor(0.3097, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.3352, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5110520546480736\n",
      "validation_loss: tensor(0.2767, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.3015, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.47885150790954967\n",
      "validation_loss: tensor(0.2458, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.2698, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.4466511033336568\n",
      "validation_loss: tensor(0.2170, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.2403, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.41445159912109375\n",
      "validation_loss: tensor(0.1903, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.2128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.3823551154284744\n",
      "validation_loss: tensor(0.1657, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.1875, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.35068924234520576\n",
      "validation_loss: tensor(0.1434, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.1644, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.319616376983453\n",
      "validation_loss: tensor(0.1233, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.1436, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.29084706750715744\n",
      "validation_loss: tensor(0.1061, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.1257, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.26680414424919935\n",
      "validation_loss: tensor(0.0925, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.1115, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.24840782592015237\n",
      "validation_loss: tensor(0.0816, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.1001, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.2339914452215159\n",
      "validation_loss: tensor(0.0731, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0911, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.2228575048979765\n",
      "validation_loss: tensor(0.0661, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0837, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.21368294473020186\n",
      "validation_loss: tensor(0.0600, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0772, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.2049062266853285\n",
      "validation_loss: tensor(0.0537, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0704, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.19563605030130896\n",
      "validation_loss: tensor(0.0480, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0643, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.688016713776203\n",
      "validation_loss: tensor(0.0440, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0599, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.441574570555124\n",
      "validation_loss: tensor(0.0404, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0560, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.260442508673816\n",
      "validation_loss: tensor(0.0375, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0526, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.1228165715377525\n",
      "validation_loss: tensor(0.0357, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0505, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.0033956018293866\n",
      "validation_loss: tensor(0.0339, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0484, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.8926112962805706\n",
      "validation_loss: tensor(0.0323, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0463, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.7882205181240296\n",
      "validation_loss: tensor(0.0311, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0446, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.6880311195894797\n",
      "validation_loss: tensor(0.0303, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0431, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.5924874062863936\n",
      "validation_loss: tensor(0.0302, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0424, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.5059381330975834\n",
      "validation_loss: tensor(0.0307, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0422, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.4297489142565993\n",
      "validation_loss: tensor(0.0316, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0422, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.3625342327615488\n",
      "validation_loss: tensor(0.0329, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0429, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.3050290694147904\n",
      "validation_loss: tensor(0.0332, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0424, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.2532415923124516\n",
      "validation_loss: tensor(0.0333, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0419, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.2058250474633638\n",
      "validation_loss: tensor(0.0330, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0411, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.1627840788468071\n",
      "validation_loss: tensor(0.0325, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0402, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.1232139634789888\n",
      "validation_loss: tensor(0.0320, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0394, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.0853696076766304\n",
      "validation_loss: tensor(0.0313, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0384, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.0491405036878882\n",
      "validation_loss: tensor(0.0303, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0373, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.0144314973250679\n",
      "validation_loss: tensor(0.0297, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0367, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.15230560302734375\n",
      "validation_loss: tensor(0.0211, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0286, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.13183289285031904\n",
      "validation_loss: tensor(0.0192, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0267, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.12071867759183327\n",
      "validation_loss: tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0243, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.11384462865983477\n",
      "validation_loss: tensor(0.0158, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0233, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.1100257612903666\n",
      "validation_loss: tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0219, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10727279378760675\n",
      "validation_loss: tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0209, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10515652532162874\n",
      "validation_loss: tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0209, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10347514419081788\n",
      "validation_loss: tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0204, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10209799109038359\n",
      "validation_loss: tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0201, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10074365509222753\n",
      "validation_loss: tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0201, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09944554441463874\n",
      "validation_loss: tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0198, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0984061282614003\n",
      "validation_loss: tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0196, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09739018078916561\n",
      "validation_loss: tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0192, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09646193581338254\n",
      "validation_loss: tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0191, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09541054245848093\n",
      "validation_loss: tensor(0.0114, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0187, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09458246882657827\n",
      "validation_loss: tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0182, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09371608828905947\n",
      "validation_loss: tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0177, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09293110178124091\n",
      "validation_loss: tensor(0.0105, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0176, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09210452678040688\n",
      "validation_loss: tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0173, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0913419901214031\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0171, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.984322210276349\n",
      "validation_loss: tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.949026830448127\n",
      "validation_loss: tensor(0.0093, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9174536474002815\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8877936890406638\n",
      "validation_loss: tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8623723569123641\n",
      "validation_loss: tensor(0.0096, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8424383127911491\n",
      "validation_loss: tensor(0.0097, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.826549648498156\n",
      "validation_loss: tensor(0.0098, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8142169454823369\n",
      "validation_loss: tensor(0.0099, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8027105864530765\n",
      "validation_loss: tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0159, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7926381271077979\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0161, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7833510215238014\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0162, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7746680123465401\n",
      "validation_loss: tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0163, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7664136235017954\n",
      "validation_loss: tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0167, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7593012270720109\n",
      "validation_loss: tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0168, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7533205399602096\n",
      "validation_loss: tensor(0.0105, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7475919545807453\n",
      "validation_loss: tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0172, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7421620528890479\n",
      "validation_loss: tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0171, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7365366775797021\n",
      "validation_loss: tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0172, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7310662595381648\n",
      "validation_loss: tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0172, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7255981160987238\n",
      "validation_loss: tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0172, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09338514553093762\n",
      "validation_loss: tensor(0.0107, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0185, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09142158934788674\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0178, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09021349723294655\n",
      "validation_loss: tensor(0.0096, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0171, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08924654848086908\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0168, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08835412256465935\n",
      "validation_loss: tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08756156589673914\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0162, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08680648685242079\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0862203740185092\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0856494192751298\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08507543765239833\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08462701525006976\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08413157848097523\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08364542374699753\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08321080889020648\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08283416084621263\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08252639059694658\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08215981240598311\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08180166919779333\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08148985323698624\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08114019239911381\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7222942091663431\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7169422125964431\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7118272011324486\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.707033856314902\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7021653074655474\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6977621990701427\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6933814575953513\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6893336136148583\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.685227388180561\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6811407338018003\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6771074851847584\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6731355560492285\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6693431783166731\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6654776816042314\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.661590576171875\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0159, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6578128411903145\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0159, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.654055767177795\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0160, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6503452277331618\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0162, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6467238242581764\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0160, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6430282592773438\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0163, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.08299119131905693\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08200881199807114\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08152229593407294\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08118149656686724\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08070368796401883\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08054601775933497\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08029889171908361\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08005338277876006\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07990113844782669\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07971080637866666\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07947787883118813\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07925678039929882\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07911331461083075\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07885887311852496\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07879452675766085\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07861424677120232\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07855686637925806\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0784163919294843\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07823459554162825\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07817168265396024\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6410179848996749\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6373967709748641\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6340053392493207\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6304997509310705\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6271005002608211\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6236537672718119\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6202407742138976\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6168922282153775\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6136989711974719\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6104132136943178\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6071324318832492\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6038714580654357\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6006366895592731\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5974536682508007\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5942115546753688\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5909717157020332\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5877801646356997\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5845658675484036\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5813211476580696\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5781067557956862\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07930573765535533\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07887590301703222\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07856583447189805\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07852916835998155\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07827018951036915\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07819716530557004\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07797084239699086\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07786200505606136\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07773099626813616\n",
      "validation_loss: tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0776122904712369\n",
      "validation_loss: tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07746234443617163\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0774034535662728\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07729835984129343\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07719762577033192\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07709002642898086\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07689316672567996\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07682798515935862\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07694677388445931\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07689654901160957\n",
      "validation_loss: tensor(0.0055, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07683949440902804\n",
      "validation_loss: tensor(0.0055, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0132, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5762291191527562\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5731089337271933\n",
      "validation_loss: tensor(0.0056, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5701651721267227\n",
      "validation_loss: tensor(0.0057, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5671747811832784\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5642404023164548\n",
      "validation_loss: tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5614331168417604\n",
      "validation_loss: tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5587701264375485\n",
      "validation_loss: tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5561159975040033\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5534459937433278\n",
      "validation_loss: tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5508905730632522\n",
      "validation_loss: tensor(0.0062, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5483042557047021\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5457728605092682\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5433534420795322\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5409098086149796\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5384812917768586\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5360595513574825\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5336736833086665\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5312607569724136\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5288797698405959\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5265526149583899\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_4 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5523822855505144\n",
      "validation_loss: tensor(0.3066, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.3321, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.49805725287206426\n",
      "validation_loss: tensor(0.2563, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.2805, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.449981547290494\n",
      "validation_loss: tensor(0.2123, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.2354, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.3979876026603746\n",
      "validation_loss: tensor(0.1679, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.1898, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.3333106189040664\n",
      "validation_loss: tensor(0.1128, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.1326, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.24836674091978844\n",
      "validation_loss: tensor(0.0649, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0826, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.19953835528829825\n",
      "validation_loss: tensor(0.0434, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0596, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.18055395754227727\n",
      "validation_loss: tensor(0.0347, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0498, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.170987241756842\n",
      "validation_loss: tensor(0.0303, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0448, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.16435780732528024\n",
      "validation_loss: tensor(0.0275, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0415, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.15876491617712174\n",
      "validation_loss: tensor(0.0252, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0386, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.15203734214261452\n",
      "validation_loss: tensor(0.0226, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0354, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.14303964413471104\n",
      "validation_loss: tensor(0.0198, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0319, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.1324854548673452\n",
      "validation_loss: tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0284, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.12188244458311093\n",
      "validation_loss: tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0253, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.11278561183384486\n",
      "validation_loss: tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0227, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10585711461416683\n",
      "validation_loss: tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0210, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.10089135021896836\n",
      "validation_loss: tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0192, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09747862963943008\n",
      "validation_loss: tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0180, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.09505700768891329\n",
      "validation_loss: tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0173, grad_fn=<DivBackward0>)\n",
      "train_loss: 3.222070919060559\n",
      "validation_loss: tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0159, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.8828132582006987\n",
      "validation_loss: tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.612689853454969\n",
      "validation_loss: tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.367372690520672\n",
      "validation_loss: tensor(0.0107, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 2.1361055551848795\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.9214638656710985\n",
      "validation_loss: tensor(0.0098, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.7375759337999805\n",
      "validation_loss: tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.5966150508904309\n",
      "validation_loss: tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.488151502905425\n",
      "validation_loss: tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.4006901142760093\n",
      "validation_loss: tensor(0.0096, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0151, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.3263196293611703\n",
      "validation_loss: tensor(0.0098, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0155, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.2591157522260772\n",
      "validation_loss: tensor(0.0099, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.2033403289984472\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0158, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.1567239702118108\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0160, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.1174571351234956\n",
      "validation_loss: tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0162, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.0853901738705842\n",
      "validation_loss: tensor(0.0103, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0163, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.0576602153896546\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0161, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.033342539153484\n",
      "validation_loss: tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0160, grad_fn=<DivBackward0>)\n",
      "train_loss: 1.0116657707261743\n",
      "validation_loss: tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0159, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9910601503360346\n",
      "validation_loss: tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0157, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.08844792620735879\n",
      "validation_loss: tensor(0.0093, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08677977212467548\n",
      "validation_loss: tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08622009561669013\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0148, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08568440905268888\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0146, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08520638720589395\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08478521264117697\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08454390934535436\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0843412268976247\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0841718045821101\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0839688851966621\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08382325142807101\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08373022967984217\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0836312963355402\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.083473241107064\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08330635994857882\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08318388091851465\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08307140066016534\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0829553248719399\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08282816039849512\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08272863470989725\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9745104298088121\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9553539205041731\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9390130842694585\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9237326864870439\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.9090543948345303\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.894933475470691\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8814030049010093\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8682612069645284\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8555081764363354\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8434460562948854\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8317449344611316\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8205845045006793\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.8100755584906347\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7999554746639654\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7903059965335064\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7811157510887762\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7722901054050612\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7636829636852194\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7552273673300417\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7469245010281201\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08438679594430865\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08333951938226357\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08284443801974659\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08254837249376759\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08237386786419412\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08221411260758868\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0143, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08207269040694148\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08193218633995293\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08181977123947617\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08167413154744213\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0142, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08152504441160593\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08141984998809625\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08130409406579059\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08119306505096625\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08103367112438131\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08091149418990805\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08079622398992503\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0806843538462005\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0805555011915124\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08045782361711774\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.7415724570706764\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.732088930118158\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7241057401858502\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7163830395811093\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7089423896363063\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.7019130635705794\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.695223695743158\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6888985959639461\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6826703563240004\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6765752282942304\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6707326758722341\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.665163739127402\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6598215991665858\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6546848842075893\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6497806051503057\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6451108470466567\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6409475670097777\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0139, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6372219583262568\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6336857102672506\n",
      "validation_loss: tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6302936625036394\n",
      "validation_loss: tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08295502277634899\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08178738185337611\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08117059150837963\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08087065797414839\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08069076893492515\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08045115974378882\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08029274910873507\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08012326755879089\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07998783988241824\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07986773615298064\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07970926036005435\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07959987806237262\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07945969386130386\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07934226160464079\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07927090781075614\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07916028159005302\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07907203413684916\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07893868558895514\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07879118593583195\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07867808963941492\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6309167139278435\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6260251821198078\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6227568158451815\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.620089560562039\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6176804726168236\n",
      "validation_loss: tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6157335079975009\n",
      "validation_loss: tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6139794580684685\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6122612805099961\n",
      "validation_loss: tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6105353313943614\n",
      "validation_loss: tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6088492352029552\n",
      "validation_loss: tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6071377866756842\n",
      "validation_loss: tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6054693186505241\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6039128747786054\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.6023627334499951\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.601065688992139\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5998566432028823\n",
      "validation_loss: tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5986836593343604\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5975117653793429\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.596374796044012\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5952367841827203\n",
      "validation_loss: tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0136, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.08156034220819888\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.08015176524286685\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0134, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07942296851495778\n",
      "validation_loss: tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07907315959101138\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07883062421905328\n",
      "validation_loss: tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07871982029506139\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0133, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07858820139251141\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07843153817313057\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07831202678798889\n",
      "validation_loss: tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07822777765878239\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07816860838706449\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07804259898499673\n",
      "validation_loss: tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0779917639975222\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0779111163216348\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07776557732813107\n",
      "validation_loss: tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07769363711339346\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.0775902197227715\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07749224597622889\n",
      "validation_loss: tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07740237549965426\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.07729529149784065\n",
      "validation_loss: tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5975542246184734\n",
      "validation_loss: tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5944201546426146\n",
      "validation_loss: tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5928484045940897\n",
      "validation_loss: tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.591500228976611\n",
      "validation_loss: tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5903295197101853\n",
      "validation_loss: tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5892775162406589\n",
      "validation_loss: tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5881481762998593\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5870400185910811\n",
      "validation_loss: tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5859053238578464\n",
      "validation_loss: tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5847524796953852\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5835587874702786\n",
      "validation_loss: tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5823107891201232\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0129, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5812211125533774\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5800439111934685\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5789027717542945\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5778481619698661\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5767560212508492\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5756424613620924\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5745441602623981\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0131, grad_fn=<DivBackward0>)\n",
      "train_loss: 0.5734850012737772\n",
      "validation_loss: tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "test loss_5 tensor(0.0130, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    # Interval / Epochs\n",
    "    trainval = DataSet(X_train,y_train)\n",
    "    train_loader = DataLoader(trainval, batch_size=1)\n",
    "    \n",
    "    testval = DataSet(X_test,y_test)\n",
    "    test_loader = DataLoader(testval, batch_size=1)\n",
    "    \n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    ahce_model = Model(5,sample_size=len(dataset))\n",
    "\n",
    "    optimizer = optim.Adam([{'params': ahce_model.parameters()}], lr = 0.0001, weight_decay=1e-4)\n",
    "    losses = []\n",
    "    \n",
    "    freeze_losses = []\n",
    "    unfreeze_losses = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        for phase in ['freeze', 'train_dag']:\n",
    "            for ep in range(0,epoch): \n",
    "                loss_val = 0\n",
    "                for input_data, target in train_loader:\n",
    "                    ahce_model.zero_grad()\n",
    "                    input_data.requires_grad=False\n",
    "                    if phase == 'freeze':\n",
    "                        output = ahce_model(input_data)\n",
    "                        loss = torch.sqrt(loss_func(output,target))\n",
    "                        loss_val += loss\n",
    "                        losses.append(loss)\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    else:\n",
    "                        output, d_sample, h_sample, w_sample, a_sample  = ahce_model(input_data, phase='train_dag')\n",
    "                           \n",
    "                        lam_bda = 1\n",
    "                        loss = lam_bda*torch.sqrt(loss_func(d_sample, input_data[:,1]))\n",
    "                        loss_val += loss\n",
    "                        loss.backward(retain_graph=True) \n",
    "                    \n",
    "                        loss = lam_bda*torch.sqrt(loss_func(h_sample, input_data[:,2]))\n",
    "                        loss_val += loss\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        \n",
    "                        loss = lam_bda*torch.sqrt(loss_func(w_sample, input_data[:,3]))\n",
    "                        loss_val += loss\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        \n",
    "                        loss = lam_bda*torch.sqrt(loss_func(a_sample, input_data[:,4]))\n",
    "                        loss_val += loss\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        \n",
    "                        loss = torch.sqrt(loss_func(output,target))\n",
    "                        loss_val += loss\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "\n",
    "                if epoch%interval == 0:\n",
    "                    print ('train_loss:', loss_val.item()/len(train_loader))\n",
    "                    val = DataSet(X_val,y_val)\n",
    "                    val_loader = DataLoader(val, batch_size=16)\n",
    "                    val_loss = 0\n",
    "                    for input_data, target in val_loader:\n",
    "                        output = ahce_model(input_data)\n",
    "                        loss = loss_func(output,target)\n",
    "                        val_loss += loss\n",
    "                    print ('validation_loss:', val_loss/len(val_loader))\n",
    "                    testval = DataSet(X_test,y_test)\n",
    "                    test_loader = DataLoader(testval, batch_size=1)\n",
    "                    loss_val = 0\n",
    "                    for input_data, target in test_loader:\n",
    "                        output = ahce_model(input_data)\n",
    "                        loss = loss_func(output,target)\n",
    "                        loss_val += loss\n",
    "                    print('test loss_'+str(ensemble+1), loss_val/len(test_loader))\n",
    "\n",
    "    torch.save(ahce_model, \"./models/ahce_autompg_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "59eecb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42429/3127445249.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_sample = self.causal_link_c_w(torch.tensor(c_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_42429/3127445249.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  d_sample = self.causal_link_c_d(torch.tensor(c_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_42429/3127445249.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  h_sample = self.causal_link_c_h(torch.tensor(c_sample,dtype=torch.float))+self.causal_link_d_h(d_sample)\n",
      "/tmp/ipykernel_42429/3127445249.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a_sample = self.causal_link_w_a(torch.tensor(w_sample,dtype=torch.float))+self.causal_link_h_a(torch.tensor(h_sample,dtype=torch.float))\n",
      "/tmp/ipykernel_42429/3127445249.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a_sample = self.causal_link_w_a(torch.tensor(w_sample,dtype=torch.float))+self.causal_link_h_a(torch.tensor(h_sample,dtype=torch.float))\n"
     ]
    }
   ],
   "source": [
    "n_classes=1\n",
    "num_c=5#no. of features\n",
    "num_alpha=392\n",
    "\n",
    "aces_ahce_total = []\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_ahce_total = []\n",
    "    model =  torch.load(\"./models/ahce_autompg_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):#For every class\n",
    "        #plt.figure()\n",
    "        for t in range(0,num_c):#For every feature\n",
    "            expectation_do_x = []\n",
    "            for x in np.linspace(0, 1, num_alpha):\n",
    "                X_values[:,t] = x\n",
    "                sample_data = model(X_values, phase='sample', inde=t, alpha=x).detach().numpy()\n",
    "                cov = np.cov(sample_data, rowvar=False)\n",
    "                means = np.mean(sample_data, axis=0)\n",
    "                cov=np.array(cov)\n",
    "                mean_vector = np.array(means)\n",
    "                inp=copy.deepcopy(mean_vector)\n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "\n",
    "                output=model(input_torchvar)\n",
    "\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_c):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_c)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "\n",
    "            ace_ahce_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "\n",
    "    aces_ahce_total.append(ace_ahce_total)\n",
    "np.save('./aces/autompg_ahce_total.npy',aces_ahce_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "081132cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_ahce_total[ensemble][0]),rmse(aces_gt[1], aces_ahce_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_ahce_total[ensemble][2]),rmse(aces_gt[3], aces_ahce_total[ensemble][3]),\n",
    "                         rmse(aces_gt[4], aces_ahce_total[ensemble][4])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(392,1), aces_ahce_total[ensemble][0].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(392,1), aces_ahce_total[ensemble][1].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(392,1), aces_ahce_total[ensemble][2].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(392,1), aces_ahce_total[ensemble][3].reshape(392,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(392,1), aces_ahce_total[ensemble][4].reshape(392,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "316126a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.01767579 0.11991732 0.09898293 0.07583601 0.07603691]\n",
      "rmse std:  [0.00593484 0.0156509  0.01673025 0.00059727 0.00052376]\n",
      "rmse all features mean:  0.07768979261907312\n",
      "rmse all features std:  0.00788740654262564\n",
      "frechet mean:  [0.03626515 0.21708779 0.18670736 0.09906441 0.10437627]\n",
      "frechet std:  [0.03482438 0.02076277 0.03514982 0.01109054 0.00823558]\n",
      "frechet all features mean:  0.12870019371289465\n",
      "frechet all features std:  0.022012617191888035\n"
     ]
    }
   ],
   "source": [
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1160b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
