{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccee14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from similaritymeasures import frechet_dist\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os,csv,math,sys, joblib\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation\n",
    "import collections\n",
    "from joblib import Parallel, delayed\n",
    "seed = 99\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({ 'from':['W', 'W', 'X','Z', 'Z'], 'to':['X', 'Z', 'Y','Y','X']})\n",
    "G=nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph() )\n",
    "nx.draw(G, with_labels=True, node_size=2500, alpha=1, arrows=True, arrowsize=20,width=2,font_size=18)\n",
    "plt.title(\"Causal DAG of Synthetic Data 3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d919eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n=1000):\n",
    "    w = np.linspace(-1,1,n)+ np.random.normal(0,0.1,size=n)\n",
    "    z = w/2 + np.random.normal(0,0.1,size=n)\n",
    "    x = -z - w + np.random.normal(0,0.1,size=n)\n",
    "    y = x**3+np.log(z**2+1e-5)+ np.random.normal(0,0.1,size=n)\n",
    "    return pd.DataFrame({'w': w, 'z': z, 'x': x, 'y': y})\n",
    "data = generate_data()\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(scaled, columns=['w', 'z', 'x', 'y'])\n",
    "\n",
    "#plotting the correlation for the features\n",
    "fig, ((ax1, ax2, ax3),(ax4, ax5, ax6)) = plt.subplots(2, 3,figsize=(17,7))\n",
    "ax1.scatter(data['w'],data['x'],s=1)\n",
    "ax1.set_xlabel('w')\n",
    "ax1.set_ylabel('x')\n",
    "ax2.scatter(data['w'],data['z'],s=1)\n",
    "ax2.set_xlabel('w')\n",
    "ax2.set_ylabel('z')\n",
    "ax3.scatter(data['z'], data['x'],s=1)\n",
    "ax3.set_xlabel('z')\n",
    "ax3.set_ylabel('x')\n",
    "ax4.scatter(data['w'],data['y'],s=1)\n",
    "ax4.set_xlabel('w')\n",
    "ax4.set_ylabel('y')\n",
    "ax5.scatter(data['z'], data['y'],s=1)\n",
    "ax5.set_xlabel('z')\n",
    "ax5.set_ylabel('y')\n",
    "ax6.scatter(data['x'], data['y'], s=1)\n",
    "ax6.set_xlabel('x')\n",
    "ax6.set_ylabel('y')\n",
    "plt.suptitle('Correlation plots',size=26)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ca3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the ground truth ACE of the inputs on outputs\n",
    "#Remember: True ACEs contain both direct and indirect causal effects\n",
    "\n",
    "w_interventions = np.linspace(min(data['w']), max(data['w']), 1000)\n",
    "\n",
    "inp = data['w'].values.reshape(-1,1)\n",
    "out = data['y'].values\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "inp = poly.fit_transform(inp)\n",
    "clf = linear_model.Ridge()\n",
    "model = clf.fit(np.array(inp), np.array(out))\n",
    "\n",
    "gtw = []\n",
    "for alpha in w_interventions:\n",
    "    df1 = pd.DataFrame.copy(data[['w']])\n",
    "    df1['w'] = alpha\n",
    "    df1 = poly.transform(df1.values)\n",
    "    gtw.append(np.mean(model.predict(df1)))\n",
    "\n",
    "plt.plot(gtw-np.mean(gtw))\n",
    "plt.xlabel('intervention on $w$')\n",
    "plt.ylabel('ACE of $w$ on $y$')\n",
    "plt.show()\n",
    "\n",
    "# causal effect of z on y\n",
    "z_interventions = np.linspace(min(data['z']), max(data['z']), 1000)\n",
    "inp = data[['z','w']]\n",
    "out = data['y']\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "inp = poly.fit_transform(inp)\n",
    "clf = linear_model.Ridge()\n",
    "model = clf.fit(np.array(inp), np.array(out))\n",
    "gtz = []\n",
    "for alpha in z_interventions:\n",
    "    df1 = pd.DataFrame.copy(data[['z','w']])\n",
    "    df1['z'] = alpha\n",
    "    df1 = poly.transform(df1)\n",
    "    gtz.append(np.mean(model.predict(df1)))\n",
    "\n",
    "plt.plot(gtz-np.mean(gtz))\n",
    "plt.xlabel('intervention on $z$')\n",
    "plt.ylabel('ACE of $z$ on $y$')\n",
    "plt.show()\n",
    "\n",
    "# causal effect of x on y\n",
    "gtx = []\n",
    "inp = data[['x', 'z', 'w']]\n",
    "out = data['y']\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "inp = poly.fit_transform(inp)\n",
    "clf = linear_model.Ridge()\n",
    "model = clf.fit(np.array(inp), np.array(out))\n",
    "\n",
    "for alpha in np.linspace(min(data['x']), max(data['x']), 1000):\n",
    "    df1 = pd.DataFrame.copy(data[['x', 'z','w']])\n",
    "    df1['x'] = alpha\n",
    "    df1 = poly.transform(df1)\n",
    "    gtx.append(np.mean(model.predict(df1)))\n",
    "\n",
    "gtx = np.array(gtx)\n",
    "plt.plot(gtx-np.mean(gtx))\n",
    "plt.xlabel('intervention on $x$')\n",
    "plt.ylabel('ACE of $x$ on $y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21305bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aces_gt=[]\n",
    "aces_gt.append(gtw-np.mean(gtw))\n",
    "aces_gt.append(gtz-np.mean(gtz))\n",
    "aces_gt.append(gtx-np.mean(gtx))\n",
    "np.save('aces/aces_gt_syn1.npy',aces_gt,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a722d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list to store all the adces of inputs on outputs\n",
    "adces_gt = []\n",
    "natural_z = data['z']\n",
    "natural_w = data['w']\n",
    "natural_y = data['y']\n",
    "\n",
    "   \n",
    "#ADCE of w on y\n",
    "adces_w_gt = []\n",
    "natural_x = - natural_w - natural_z + np.random.normal(0,0.1,1000)\n",
    "natural_z = natural_w/2 + np.random.normal(0,0.1,1000)\n",
    "right = np.mean(natural_x**3 +np.log(natural_z**2+1e-5) + np.random.normal(0,0.1,1000))\n",
    "for intervention_w in np.linspace(min(data['w']), max(data['w']), 1000):\n",
    "    left = np.mean(natural_x**3 + np.log(natural_z**2+1e-5) + np.random.normal(0,0.1,1000))\n",
    "    adces_w_gt.append(left - right)\n",
    "adces_gt.append(np.array(adces_w_gt))\n",
    "plt.plot(np.array(adces_w_gt))\n",
    "plt.xlabel('intervention on $w$')\n",
    "plt.ylabel('ADCE of $w$ on $y$')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#ADCE of z on y\n",
    "natural_x = - natural_w - natural_z + np.random.normal(0,0.1,1000)\n",
    "adces_z_gt = []\n",
    "right = np.mean(natural_x**3 +np.log(natural_z**2+1e-5) + np.random.normal(0,0.1,1000))\n",
    "for intervention_z in np.linspace(min(data['z']), max(data['z']), 1000):\n",
    "    left = np.mean(natural_x**3 + np.log(intervention_z**2+1e-5)+ np.random.normal(0,0.1,size=1000))\n",
    "    adces_z_gt.append(left-right)\n",
    "adces_gt.append(np.array(adces_z_gt))\n",
    "plt.plot(np.array(adces_z_gt))\n",
    "plt.xlabel('intervention on $z$')\n",
    "plt.ylabel('ADCE of $z$ on $y$')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#ADCE of x on y\n",
    "adces_x_gt = []\n",
    "right = np.mean(natural_x**3 +np.log(natural_z**2+1e-5) + np.random.normal(0,0.1,1000))\n",
    "for intervention_x in np.linspace(min(data['x']), max(data['x']), 1000):\n",
    "    left = np.mean(intervention_x**3 + np.log(natural_z**2+1e-5) + np.random.normal(0,0.1,size=1000))\n",
    "    adces_x_gt.append(left-right)\n",
    "adces_gt.append(np.array(adces_x_gt))\n",
    "plt.plot(np.array(adces_x_gt))\n",
    "plt.xlabel('intervention on $x$')\n",
    "plt.ylabel('ADCE of $x$ on $y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list to store all the aices of inputs on outputs\n",
    "aices_gt = []\n",
    "natural_z = data['z']\n",
    "natural_x = data['x']\n",
    "natural_w = data['w']\n",
    "\n",
    "\n",
    "#AICE of w on y\n",
    "aices_w_gt = []\n",
    "right = np.mean(natural_x**3 +np.log(natural_z**2+1e-5) + np.random.normal(0,0.1,1000))\n",
    "for intervention_w in np.linspace(min(data['w']), max(data['w']), 1000):\n",
    "    actual_z = intervention_w/2 + np.random.normal(0,0.1,1000)\n",
    "    actual_x = -intervention_w - actual_z + np.random.normal(0,0.1,1000)\n",
    "    left = np.mean(actual_x**3 + np.log(actual_z**2+1e-5) + np.random.normal(0,0.1,1000))\n",
    "    aices_w_gt.append(left - right)\n",
    "aices_gt.append(np.array(aices_w_gt))\n",
    "plt.plot(np.array(aices_w_gt))\n",
    "plt.xlabel('intervention on $w$')\n",
    "plt.ylabel('AICE of $w$ on $y$')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#AICE of z on y\n",
    "aices_z_gt = []\n",
    "right = np.mean(natural_x**3 +np.log(natural_z**2+1e-5) + np.random.normal(0,0.1,1000))\n",
    "for intervention_z in np.linspace(min(data['z']), max(data['z']), 1000):\n",
    "    actual_x = -natural_w - intervention_z + np.random.normal(0,0.1,size=1000)\n",
    "    left = np.mean(actual_x**3 + np.log(natural_z**2+1e-5)+ np.random.normal(0,0.1,size=1000))\n",
    "    aices_z_gt.append(left-right)\n",
    "aices_gt.append(np.array(aices_z_gt))\n",
    "plt.plot(np.array(aices_z_gt))\n",
    "plt.xlabel('intervention on $z$')\n",
    "plt.ylabel('AICE of $z$ on $y$')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#AICE of z on y\n",
    "aices_x_gt = []\n",
    "right = np.mean(natural_x**3 +np.log(natural_z**2+1e-5) + np.random.normal(0,0.0,1000))\n",
    "for intervention_x in np.linspace(min(data['x']), max(data['x']), 1000):\n",
    "    left = np.mean(natural_x**3 + np.log(natural_z**2+1e-5)+ np.random.normal(0,0.0,size=1000))\n",
    "    aices_x_gt.append(left-right)\n",
    "aices_gt.append(np.array(aices_x_gt))\n",
    "plt.plot(np.array(aices_x_gt))\n",
    "plt.xlabel('intervention on $x$')\n",
    "plt.ylabel('AICE of $x$ on $y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f89fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, feature_dim, batch_size=64, device='cpu',sample_size=1000):\n",
    "        super(Model, self).__init__()\n",
    "        self.batchsize=batch_size\n",
    "        self.causal_link_w_z = nn.Linear(1,1)\n",
    "        self.causal_link_z_x = nn.Linear(1,1)\n",
    "        self.causal_link_w_x = nn.Linear(1,1)\n",
    "        self.first_layer = nn.Linear(3,2)\n",
    "        self.second_layer = nn.Linear(2,2)\n",
    "        self.third_layer = nn.Linear(2,2)\n",
    "        self.regression_layer = nn.Linear(2, 1)\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def forward(self, inp, phase='freeze', inde=0, alpha=0):\n",
    "        if phase=='freeze':\n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "            return prediction\n",
    "        elif phase=='train_dag':\n",
    "            w_sample = torch.tensor(inp[:,0].reshape(1,-1), dtype=torch.float)\n",
    "            z_sample = self.causal_link_w_z(w_sample)\n",
    "            x_sample = self.causal_link_w_x(w_sample)+self.causal_link_z_x(z_sample)\n",
    "            inp = torch.cat((w_sample, z_sample, x_sample),dim=1)\n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "\n",
    "            return prediction, z_sample, x_sample\n",
    "        elif phase=='sample':\n",
    "            if inde == 0:\n",
    "                w_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                z_sample = self.causal_link_w_z(torch.tensor(w_sample, dtype=torch.float))\n",
    "                x_sample = self.causal_link_w_x(torch.tensor(w_sample, dtype=torch.float))+self.causal_link_z_x(z_sample)\n",
    "                inp = torch.cat((torch.tensor(w_sample, dtype=torch.float), z_sample, x_sample),dim=1)\n",
    "                return inp\n",
    "            elif inde == 1:\n",
    "                w_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                z_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                x_sample = self.causal_link_w_x(torch.tensor(w_sample, dtype=torch.float))+self.causal_link_z_x(z_sample)\n",
    "                inp = torch.cat((torch.tensor(w_sample, dtype=torch.float), z_sample, x_sample),dim=1)\n",
    "                return inp\n",
    "            else:\n",
    "                w_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                z_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                x_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                inp = torch.cat((torch.tensor(w_sample, dtype=torch.float), torch.tensor(z_sample, dtype=torch.float), x_sample),dim=1)\n",
    "                return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size=5 # to get mean and std values\n",
    "batch_size = 1\n",
    "samplesize=1000\n",
    "values = list(data.columns.values)\n",
    "y = data[values[-1:]]\n",
    "y = np.array(y, dtype='float32')\n",
    "X = data[values[:-1]]\n",
    "X = np.array(X, dtype='float32')\n",
    "\n",
    "indices = np.random.choice(len(X), len(X), replace=False)\n",
    "X_values = X[indices]\n",
    "y_values = y[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 50\n",
    "val_size = 20\n",
    "\n",
    "interval = 5\n",
    "epoch = 20\n",
    "\n",
    "X_test = X_values[-test_size:]\n",
    "X_trainval = X_values[:-test_size]\n",
    "X_val = X_trainval[-val_size:]\n",
    "X_train = X_trainval[:-val_size]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "y_test = y_values[-test_size:]\n",
    "y_trainval = y_values[:-test_size]\n",
    "y_val = y_trainval[-val_size:]\n",
    "y_train = y_trainval[:-val_size]\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49754533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to load data from csv file\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe1, dataframe2):\n",
    "        self.data_points = dataframe1\n",
    "        self.targets = dataframe2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_point = torch.tensor(np.array(self.data_points[idx]), dtype=torch.float)\n",
    "        target_point = torch.tensor(np.array(self.targets[idx]), dtype=torch.float)\n",
    "        return input_point, target_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f0dd7",
   "metadata": {},
   "source": [
    "# ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "\n",
    "    # Interval / Epochs\n",
    "    trainval = DataSet(X_train,y_train)\n",
    "    train_loader = DataLoader(trainval, batch_size=batch_size)\n",
    "    \n",
    "    loss_func = nn.MSELoss()\n",
    "    erm_model = Model(3,sample_size=len(data))\n",
    "    optimizer = optim.Adam([{'params': erm_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "    losses = []\n",
    "\n",
    "    for ep in range(0,epoch): \n",
    "        loss_val = 0\n",
    "        for input_data, target in train_loader:\n",
    "            erm_model.zero_grad()\n",
    "            input_data.requires_grad=True\n",
    "            output = erm_model(input_data)\n",
    "            loss = torch.sqrt(loss_func(output,target))\n",
    "            loss_val += loss\n",
    "\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if ep%interval == 0:\n",
    "            print ('train_loss:', loss_val.item()/len(train_loader))\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=16)\n",
    "            val_loss = 0\n",
    "            for input_data, target in val_loader:\n",
    "                output = erm_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                val_loss += loss\n",
    "            print ('validation_loss:', val_loss/len(val_loader))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            loss_val = 0\n",
    "            for input_data, target in test_loader:\n",
    "                output = erm_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                loss_val += loss\n",
    "            print('test loss:', loss_val/len(test_loader))\n",
    "            print()\n",
    "    print(\"************\")\n",
    "    torch.save(erm_model, \"models/erm_s1_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c133fd",
   "metadata": {},
   "source": [
    "# Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c452d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    print(ensemble)\n",
    "    model = torch.load(\"models/erm_s1_\"+str(ensemble+1))\n",
    "    ig = IntegratedGradients(model)\n",
    "    \n",
    "    data_w = data['w'].values[:1000]\n",
    "    data_z = data['z'].values[:1000]\n",
    "    data_x = data['x'].values[:1000]\n",
    "   \n",
    "    do_w = np.linspace(min(data['w']), max(data['w']), 1000)\n",
    "    do_z = np.linspace(min(data['z']), max(data['z']), 1000)\n",
    "    do_x = np.linspace(min(data['x']), max(data['x']), 1000)\n",
    "    \n",
    "    test_array_w = np.stack((do_w, data_z, data_x), axis=1)\n",
    "    test_array_w = torch.from_numpy(test_array_w).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_z = np.stack((data_w, do_z, data_x), axis=1)\n",
    "    test_array_z = torch.from_numpy(test_array_z).type(torch.FloatTensor)    \n",
    "    \n",
    "    test_array_x = np.stack((data_w, data_z, do_x), axis=1)\n",
    "    test_array_x = torch.from_numpy(test_array_x).type(torch.FloatTensor)\n",
    "    \n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    ig_attr_test_w, delta = ig.attribute(test_array_w, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_z, delta = ig.attribute(test_array_z, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_x, delta = ig.attribute(test_array_x, n_steps=50, return_convergence_delta=True)\n",
    "   \n",
    "    rmse_results.append([rmse(aces_gt[0], np.array(ig_attr_test_w[:,0])),\n",
    "                         rmse(aces_gt[1], np.array(ig_attr_test_z[:,1])),\n",
    "                         rmse(aces_gt[2], np.array(ig_attr_test_x[:,2]))\n",
    "                        ])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(1000,1), ig_attr_test_w[:,0].reshape(1000,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(1000,1), ig_attr_test_z[:,1].reshape(1000,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(1000,1), ig_attr_test_x[:,2].reshape(1000,1))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dce209",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=1\n",
    "num_feat=3\n",
    "num_alpha=1000\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_ca_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_ca_total = []\n",
    "    model = torch.load(\"models/erm_s1_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_ca_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "    aces_ca_total.append(ace_ca_total)\n",
    "np.save('./aces/s1_ca_total.npy',aces_ca_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68976d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_ca_total[ensemble][0]),\n",
    "                         rmse(aces_gt[1], aces_ca_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_ca_total[ensemble][2])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(1000,1), aces_ca_total[ensemble][0].reshape(1000,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(1000,1), aces_ca_total[ensemble][1].reshape(1000,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(1000,1), aces_ca_total[ensemble][2].reshape(1000,1))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370826b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5db774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability is taken over indices of baseline only\n",
    "def get_probabiity(unique_count, x_hat, indices_baseline, n):\n",
    "    if len(indices_baseline) > 0:\n",
    "        count = 0\n",
    "        for i in unique_count:\n",
    "            check = True\n",
    "            key = np.asarray(i)\n",
    "            for j in indices_baseline:\n",
    "                check = check and key[j] == x_hat[j]\n",
    "            if check:\n",
    "                count += unique_count[i]\n",
    "        return count / n\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def conditional_prob(unique_count, x_hat, indices, indices_baseline, n):\n",
    "    numerator_indices = indices + indices_baseline\n",
    "    numerator = get_probabiity(unique_count, x_hat, numerator_indices, n)\n",
    "    denominator = get_probabiity(unique_count, x_hat, indices, n)\n",
    "    try:\n",
    "        kk = numerator / denominator\n",
    "    except ZeroDivisionError:\n",
    "        denominator = 1e-7\n",
    "        # pass\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def causal_prob(unique_count, x_hat, indices, indices_baseline, causal_struc, n):\n",
    "    p = 1\n",
    "    for i in indices_baseline:\n",
    "        intersect_s, intersect_s_hat = [], []\n",
    "        intersect_s_hat.append(i)\n",
    "        if len(causal_struc[str(i)]) > 0:\n",
    "            for index in causal_struc[str(i)]:\n",
    "                if index in indices or index in indices_baseline:\n",
    "                    intersect_s.append(index)\n",
    "            p *= conditional_prob(unique_count, x_hat, intersect_s, intersect_s_hat, n)\n",
    "        else:\n",
    "            p *= get_probabiity(unique_count, x_hat, intersect_s_hat, n)\n",
    "    return p\n",
    "\n",
    "def get_baseline(X, model):\n",
    "    fx = 0\n",
    "    n_features = X.shape[1]\n",
    "    X = np.reshape(X, (len(X), 1, n_features))\n",
    "    for i in X:\n",
    "        fx += model(torch.tensor(i, dtype=torch.float))\n",
    "    return fx / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns value from using function for different versions\n",
    "def get_value(version, permutation, X, x, unique_count, causal_struct, model, N, xi):\n",
    "    # intializing returns\n",
    "    absolute_diff, f1, f2 = 0, 0, 0\n",
    "    xi_index = permutation.index(xi)\n",
    "    indices = permutation[:xi_index + 1]\n",
    "    indices_baseline = permutation[xi_index + 1:]\n",
    "    x_hat = np.zeros(N)\n",
    "    x_hat_2 = np.zeros(N)\n",
    "    len_X = len(X)\n",
    "    for j in indices:\n",
    "        x_hat[j] = x[j]\n",
    "        x_hat_2[j] = x[j]\n",
    "    if version == '2' or version == '3' or version == '4':\n",
    "        proba1, proba2 = 0, 0\n",
    "        baseline_check_1, baseline_check_2 = [], []\n",
    "        f1, f2 = 0, 0\n",
    "        indices_baseline_2 = indices_baseline[:]\n",
    "        for i in unique_count:\n",
    "            X = np.asarray(i)\n",
    "            for j in indices_baseline:\n",
    "                x_hat[j] = X[j]\n",
    "                x_hat_2[j] = X[j]\n",
    "\n",
    "            # No repetition\n",
    "            # Eg if baseline_indices is null, it'll only run once as x_hat will stay the same over each iteration\n",
    "            if x_hat.tolist() not in baseline_check_1:\n",
    "                baseline_check_1.append(x_hat.tolist())\n",
    "                if version == '2':\n",
    "                    prob_x_hat = get_probabiity(unique_count, x_hat, indices_baseline, len_X)\n",
    "                elif version == '3':\n",
    "                    prob_x_hat = conditional_prob(unique_count, x_hat, indices, indices_baseline, len_X)\n",
    "                else:\n",
    "                    prob_x_hat = causal_prob(unique_count, x_hat, indices, indices_baseline, causal_struct, len_X)\n",
    "                proba1 += prob_x_hat\n",
    "                x_hat = np.reshape(x_hat, (1, N))\n",
    "                # print(x_hat.shape)\n",
    "                f1 = f1 + (model(torch.tensor(x_hat, dtype=torch.float)) * prob_x_hat)\n",
    "\n",
    "            # xi index will be given to baseline for f2\n",
    "            x_hat_2[xi] = X[xi]\n",
    "            if xi not in indices_baseline_2:\n",
    "                indices_baseline_2.append(xi)\n",
    "\n",
    "            # No repetition\n",
    "            indices_2 = indices[:]\n",
    "            indices_2.remove(xi)\n",
    "            if x_hat_2.tolist() not in baseline_check_2:\n",
    "                baseline_check_2.append(x_hat_2.tolist())\n",
    "                if version == '2':\n",
    "                    prob_x_hat_2 = get_probabiity(unique_count, x_hat_2, indices_baseline_2, len_X)\n",
    "                elif version == '3':\n",
    "                    prob_x_hat_2 = conditional_prob(unique_count, x_hat_2, indices_2, indices_baseline_2, len_X)\n",
    "                else:\n",
    "                    prob_x_hat_2 = causal_prob(unique_count, x_hat_2, indices_2, indices_baseline_2, causal_struct,\n",
    "                                               len_X)\n",
    "                proba2 += prob_x_hat_2\n",
    "                x_hat_2 = np.reshape(x_hat_2, (1, N))\n",
    "                f2 = f2 + model(torch.tensor(x_hat_2, dtype=torch.float)) * prob_x_hat_2\n",
    "            x_hat = np.squeeze(x_hat)\n",
    "            x_hat_2 = np.squeeze(x_hat_2)\n",
    "        absolute_diff = abs(f1 - f2)\n",
    "    elif version == '1':\n",
    "        f1, f2 = 0, 0\n",
    "        for i in range(len(X)):\n",
    "            for j in indices_baseline:\n",
    "                x_hat[j] = X[i][j]\n",
    "                x_hat_2[j] = X[i][j]\n",
    "            x_hat = np.reshape(x_hat, (1, N))\n",
    "            f1 += model(torch.tensor(x_hat, dtype=torch.float))\n",
    "            x_hat_2[xi] = X[i][xi]\n",
    "            x_hat_2 = np.reshape(x_hat_2, (1, N))\n",
    "            f2 += model(torch.tensor(x_hat_2, dtype=torch.float))\n",
    "            x_hat = np.squeeze(x_hat)\n",
    "            x_hat_2 = np.squeeze(x_hat_2)\n",
    "        absolute_diff = abs(f1 - f2) / len_X\n",
    "        f1 = f1 / len_X\n",
    "        f2 = f2 / len_X\n",
    "    return absolute_diff, f1, f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_shapley(version, xi, N, X, x, m, model, unique_count, causal_struct,\n",
    "                        global_shap=False):\n",
    "    R = list(itertools.permutations(range(N)))\n",
    "    random.shuffle(R)\n",
    "    score = 0\n",
    "    count_negative = 0\n",
    "    vf1, vf2 = 0, 0\n",
    "    for i in range(m):\n",
    "        abs_diff, f1, f2 = get_value(version, list(R[i]), X, x, unique_count, causal_struct, model, N, xi)\n",
    "        vf1 += f1\n",
    "        vf2 += f2\n",
    "        score += abs_diff\n",
    "        if not global_shap:\n",
    "            if vf2 > vf1:\n",
    "                count_negative -= 1\n",
    "            else:\n",
    "                count_negative += 1\n",
    "    if count_negative < 0 and not global_shap:\n",
    "        score = -1 * score\n",
    "    return score / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d90351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapley(model, version, local_shap=0):\n",
    "    sigma_phi = 0\n",
    "    global_shap=True\n",
    "    causal_struct = None\n",
    "    try:\n",
    "        causal_struct = json.load(open('s1.json', 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    n_features = 3\n",
    "    unique_count = collections.Counter(map(tuple, X_train[:100]))\n",
    "    ##### f(x) with baseline\n",
    "    \n",
    "    ind = np.random.choice(len(X), 100, replace=False)\n",
    "    X_v = X[ind]\n",
    "    f_o = get_baseline(X_v, model)[0]\n",
    "    baseline = np.mean(X_v, axis=0)\n",
    "    rmse_shapley_values = []\n",
    "    frechet_shapley_values = []\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        global_shap_score = Parallel(n_jobs=-1)(\n",
    "            delayed(approximate_shapley)(version, feature, n_features, X_v, x, math.factorial(n_features), model,\n",
    "                                         unique_count, causal_struct, global_shap) for i, x in\n",
    "            enumerate(X_v))\n",
    "        rmse_shapley_values.append(rmse(aces_gt[feature][ind], np.array([i.detach().numpy()[0][0] for i in global_shap_score])))\n",
    "        frechet_shapley_values.append(frechet_dist(aces_gt[feature][ind].reshape(-1,1), np.array([i.detach().numpy()[0][0] for i in global_shap_score]).reshape(-1,1)))\n",
    "    return rmse_shapley_values, frechet_shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "for ensemble in range(ensemble_size):\n",
    "    print(ensemble)\n",
    "    model = torch.load(\"models/erm_s1_\"+str(ensemble+1))\n",
    "    r, f = shapley(model, version='4', local_shap=12)\n",
    "    rmse_results.append(r)\n",
    "    frechet_results.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./models/cshapresults_indirect.npy', np.array([rmse_results, frechet_results]))\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapley direct effects\n",
    "def shapley(model, version, file='s1_direct.json', local_shap=0):\n",
    "    sigma_phi = 0\n",
    "    global_shap=True\n",
    "    causal_struct = None\n",
    "    try:\n",
    "        causal_struct = json.load(open(file, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    n_features = 3\n",
    "    unique_count = collections.Counter(map(tuple, X_train[:100]))\n",
    "    ##### f(x) with baseline\n",
    "    \n",
    "    ind = np.random.choice(len(X), 100, replace=False)\n",
    "    X_v = X[ind]\n",
    "    f_o = get_baseline(X_v, model)[0]\n",
    "    baseline = np.mean(X_v, axis=0)\n",
    "    shapley_values = []\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        global_shap_score = Parallel(n_jobs=-1)(\n",
    "            delayed(approximate_shapley)(version, feature, n_features, X_v, x, math.factorial(n_features), model,\n",
    "                                         unique_count, causal_struct, global_shap) for i, x in\n",
    "            enumerate(X_v))\n",
    "        shapley_values.append(np.array([i.detach().numpy()[0][0] for i in global_shap_score]))\n",
    "    return shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dac0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_direct = []\n",
    "shapley_total = []\n",
    "for ensemble in range(ensemble_size):\n",
    "    print(ensemble)\n",
    "    model = torch.load(\"models/erm_s1_\"+str(ensemble+1))\n",
    "    s = shapley(model, version='4', file='s1_direct.json', local_shap=12)\n",
    "    shapley_direct.append(s)\n",
    "    s = shapley(model, version='4', file='s1_total.json', local_shap=12)    \n",
    "    shapley_total.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a71ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_indirect = np.array(shapley_total)-np.array(shapley_direct)\n",
    "rmse_results = []\n",
    "frechet_results = []\n",
    "for i in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aices_gt[0][indices[:100]], shapley_indirect[ensemble][0]),\n",
    "                         rmse(aices_gt[1][indices[:100]], shapley_indirect[ensemble][1]),\n",
    "                         rmse(aices_gt[2][indices[:100]], shapley_indirect[ensemble][2])])\n",
    "    frechet_results.append([frechet_dist(aices_gt[0][indices[:100]].reshape(100,1), shapley_indirect[ensemble][0].reshape(100,1)),\n",
    "                            frechet_dist(aices_gt[1][indices[:100]].reshape(100,1), shapley_indirect[ensemble][1].reshape(100,1)),\n",
    "                            frechet_dist(aices_gt[2][indices[:100]].reshape(100,1), shapley_indirect[ensemble][2].reshape(100,1))\n",
    "                            ])\n",
    "\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862fcc9",
   "metadata": {},
   "source": [
    "# CREDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92eb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = {0:(lambda ii:(4/ii)+ ((-3/2)**3)*3*(ii**2)), 1:(lambda ii:2/ii + 3*ii**2), \n",
    "         2:(lambda ii:3*ii**2)}\n",
    "\n",
    "def get_grad(x, prior):\n",
    "    a = x.clone().detach().requires_grad_(True)\n",
    "    for f in prior.keys():\n",
    "        z = prior[f]\n",
    "        z = torch.sum(z(a[0][f]), dim=0)\n",
    "        z.backward()\n",
    "    return a.grad\n",
    "\n",
    "def get_grads_to_match(ip, prior):\n",
    "    return get_grad(ip, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    \n",
    "    trainval = DataSet(X_train,y_train)\n",
    "    train_loader = DataLoader(trainval, batch_size=1)\n",
    "    \n",
    "    testval = DataSet(X_test,y_test)\n",
    "    test_loader = DataLoader(testval, batch_size=1)\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    credo_model = Model(3,sample_size=len(data))\n",
    "\n",
    "    optimizer = optim.Adam([{'params': credo_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "    losses = []\n",
    "    for ep in range(0,epoch): \n",
    "        loss_val = 0\n",
    "        for input_data, target in train_loader:\n",
    "            credo_model.zero_grad()\n",
    "            input_data.requires_grad=True\n",
    "            output = credo_model(input_data)\n",
    "\n",
    "            calc_grads = (autograd.grad(torch.sum(output, dim=0), input_data, retain_graph=True, create_graph=True)[0])\n",
    "            grads_to_match = get_grads_to_match(input_data, prior) \n",
    "            hinge_input = torch.abs(grads_to_match - calc_grads)\n",
    "            \n",
    "            loss = torch.sqrt(loss_func(output,target)) + 0.1 * torch.norm(torch.clamp(hinge_input, min=0), p=1)\n",
    "            loss_val = loss\n",
    "            \n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%interval == 0:\n",
    "            print ('train_loss:', loss_val.item()/len(train_loader))\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=16)\n",
    "            val_loss = 0\n",
    "            for input_data, target in val_loader:\n",
    "                output = credo_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                val_loss += loss\n",
    "            print ('validation_loss:', val_loss/len(val_loader))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            loss_val = 0\n",
    "            for input_data, target in test_loader:\n",
    "                output = credo_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                loss_val += loss\n",
    "            print('test loss_'+str(ensemble+1), loss_val/len(test_loader))\n",
    "    print(\"***********\")\n",
    "    torch.save(credo_model, \"./models/credo_s1_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=1\n",
    "num_feat=3\n",
    "num_alpha=1000\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_credo_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_credo_total = []\n",
    "    model = torch.load(\"models/credo_s1_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_credo_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "    aces_credo_total.append(ace_credo_total)\n",
    "np.save('./aces/s1_credo_total.npy',aces_credo_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_credo_total[ensemble][0]),\n",
    "                         rmse(aces_gt[1], aces_credo_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_credo_total[ensemble][2])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(1000,1), aces_credo_total[ensemble][0].reshape(1000,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(1000,1), aces_credo_total[ensemble][1].reshape(1000,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(1000,1), aces_credo_total[ensemble][2].reshape(1000,1))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c638aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c540b7",
   "metadata": {},
   "source": [
    "# AHCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    # Interval / Epochs\n",
    "    trainval = DataSet(X_train,y_train)\n",
    "    train_loader = DataLoader(trainval, batch_size=1)\n",
    "    \n",
    "    testval = DataSet(X_test,y_test)\n",
    "    test_loader = DataLoader(testval, batch_size=1)\n",
    "    \n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    ahce_model = Model(3,sample_size=len(data))\n",
    "\n",
    "    optimizer = optim.Adam([{'params': ahce_model.parameters()}], lr = 0.0001, weight_decay=1e-4)\n",
    "    losses = []\n",
    "    freeze_losses = []\n",
    "    unfreeze_losses = []\n",
    "    for ep in range(0,30):\n",
    "        p1_loss = 0\n",
    "        p2_loss = 0\n",
    "        for input_data, target in train_loader: \n",
    "            loss_val = 0\n",
    "            for phase in ['train_dag','freeze']:\n",
    "                ahce_model.zero_grad()\n",
    "                input_data.requires_grad=False\n",
    "                if phase == 'freeze':\n",
    "                    output = ahce_model(input_data)\n",
    "                    loss = torch.sqrt(loss_func(output,target))\n",
    "                    loss_val += loss\n",
    "                    losses.append(loss)\n",
    "                    p1_loss+=loss.item()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "\n",
    "                else:\n",
    "                    output, z_sample, x_sample  = ahce_model(input_data, phase='train_dag')\n",
    "                    lam_bda = 0.5\n",
    "                    loss = lam_bda*torch.sqrt(loss_func(z_sample, input_data[:,1]))\n",
    "                    loss_val += loss\n",
    "                    p2_loss += loss.item()\n",
    "                    loss.backward(retain_graph=True) \n",
    "\n",
    "                    loss = lam_bda*torch.sqrt(loss_func(x_sample, input_data[:,2]))\n",
    "                    loss_val += loss\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    p2_loss += loss.item()\n",
    "\n",
    "                    loss = torch.sqrt(loss_func(output,target))\n",
    "                    loss_val += loss\n",
    "                    p2_loss += loss.item()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "        freeze_losses.append(p1_loss)        \n",
    "        unfreeze_losses.append(p2_loss)\n",
    "        \n",
    "        if ep%interval == 0:\n",
    "            print (phase, 'train_loss:', loss_val.item()/len(train_loader))\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=16)\n",
    "            val_loss = 0\n",
    "            for input_data, target in val_loader:\n",
    "                output = ahce_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                val_loss += loss\n",
    "            print (phase, 'validation_loss:', val_loss/len(val_loader))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            loss_val = 0\n",
    "            for input_data, target in test_loader:\n",
    "                output = ahce_model(input_data)\n",
    "                loss = loss_func(output,target)\n",
    "                loss_val += loss\n",
    "            print(phase, 'test loss_'+str(ensemble+1), loss_val/len(test_loader))\n",
    "            print()\n",
    "    np.save('./losses/ahce_s1_freeze_'+str(ensemble+1)+'.npy', freeze_losses)\n",
    "    np.save('./losses/ahce_s1_unfreeze_'+str(ensemble+1)+'.npy', unfreeze_losses)\n",
    "    torch.save(ahce_model, \"./models/ahce_s1_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=1\n",
    "num_c=3#no. of features\n",
    "num_alpha=1000\n",
    "\n",
    "aces_ahce_total = []\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_ahce_total = []\n",
    "    model =  torch.load(\"./models/ahce_s1_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):#For every class\n",
    "        #plt.figure()\n",
    "        for t in range(0,num_c):#For every feature\n",
    "            expectation_do_x = []\n",
    "            for x in np.linspace(0, 1, num_alpha):\n",
    "                X_values[:,t] = x\n",
    "                sample_data = model(X_values, phase='sample', inde=t, alpha=x).detach().numpy()\n",
    "                cov = np.cov(sample_data, rowvar=False)\n",
    "                means = np.mean(sample_data, axis=0)\n",
    "                cov=np.array(cov)\n",
    "                mean_vector = np.array(means)\n",
    "                inp=copy.deepcopy(mean_vector)\n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "\n",
    "                output=model(input_torchvar)\n",
    "\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_c):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_c)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "\n",
    "            ace_ahce_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "\n",
    "    aces_ahce_total.append(ace_ahce_total)\n",
    "np.save('./aces/s1_ahce_total.npy',aces_ahce_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dbfcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in [0,1,2,3,4]:\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_ahce_total[ensemble][0]),\n",
    "                         rmse(aces_gt[1], aces_ahce_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_ahce_total[ensemble][2])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(1000,1), aces_ahce_total[ensemble][0].reshape(1000,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(1000,1), aces_ahce_total[ensemble][1].reshape(1000,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(1000,1), aces_ahce_total[ensemble][2].reshape(1000,1))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e801bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=1\n",
    "num_feat=3\n",
    "num_alpha=1000\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_ahce_direct = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_ahce_direct = []\n",
    "    model = torch.load(\"models/ahce_s1_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_ahce_direct.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "    aces_ahce_direct.append(ace_ahce_direct)\n",
    "np.save('./aces/s1_ahce_direct.npy',aces_ahce_direct,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601057a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ahce_total = np.load('./aces/s1_ahce_total.npy')\n",
    "ahce_direct = np.load('./aces/s1_ahce_direct.npy')\n",
    "\n",
    "ahce_indirect = ahce_total-ahce_direct\n",
    "\n",
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in [0,4]:\n",
    "    rmse_results.append([rmse(aices_gt[0], ahce_indirect[ensemble][0]),\n",
    "                         rmse(aices_gt[1], ahce_indirect[ensemble][1]),\n",
    "                         rmse(aices_gt[2], ahce_indirect[ensemble][2])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aices_gt[0].reshape(1000,1), ahce_indirect[ensemble][0].reshape(1000,1)),\n",
    "                            frechet_dist(aices_gt[1].reshape(1000,1), ahce_indirect[ensemble][1].reshape(1000,1)),\n",
    "                            frechet_dist(aices_gt[2].reshape(1000,1), ahce_indirect[ensemble][2].reshape(1000,1))\n",
    "                            ])\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
