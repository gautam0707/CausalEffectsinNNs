{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "356d5cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os,csv,math,sys, joblib\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import collections\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "import random\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.model_selection, sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "#import pydot\n",
    "from similaritymeasures import frechet_dist\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib\n",
    "seed = 99 # To reproduce the results\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b1d88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sachs_10000.csv\")\n",
    "data.drop(data.columns[[0]], axis=1, inplace=True)\n",
    "cols = data.columns\n",
    "for i in cols:\n",
    "    data[i] = data[i].map({'LOW': 0, 'AVG': 1, 'HIGH': 2})\n",
    "mms = MinMaxScaler()\n",
    "data[['PKC', 'PKA', 'Raf', 'Mek', 'Erk', 'Jnk', 'P38']] = mms.fit_transform(data[['PKC', 'PKA', 'Raf', 'Mek', 'Erk', 'Jnk', 'P38']])\n",
    "data = data[['PKC', 'PKA', 'Raf', 'Mek', 'Erk', 'Jnk', 'P38', 'Akt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca177d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PKC</th>\n",
       "      <th>PKA</th>\n",
       "      <th>Raf</th>\n",
       "      <th>Mek</th>\n",
       "      <th>Erk</th>\n",
       "      <th>Jnk</th>\n",
       "      <th>P38</th>\n",
       "      <th>Akt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PKC  PKA  Raf  Mek  Erk  Jnk  P38  Akt\n",
       "0  0.0  0.5  0.5  0.5  1.0  0.0  0.0    1\n",
       "1  0.0  0.5  0.5  0.0  0.0  0.5  0.0    0\n",
       "2  0.5  0.5  0.5  0.5  1.0  0.5  0.5    1\n",
       "3  0.5  0.5  0.5  0.0  0.0  0.5  0.0    1\n",
       "4  0.0  0.5  1.0  0.0  1.0  0.5  0.0    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4909b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkc_interventions = np.linspace(min(data['PKC']), max(data['PKC']), 3)\n",
    "reg = MLPClassifier().fit(data['PKC'].values.reshape(-1,1), data['Akt'].values)\n",
    "gt_pkc = reg.predict(pkc_interventions.reshape(-1,1))\n",
    "\n",
    "\n",
    "pka_interventions = np.linspace(min(data['PKA']), max(data['PKA']), 3)\n",
    "reg = MLPClassifier().fit(data[['PKA','PKC']].values, data['Akt'].values)\n",
    "gt_pka = []\n",
    "for alpha in np.linspace(min(data['PKA']), max(data['PKA']), 3):\n",
    "    df1 = pd.DataFrame.copy(data[['PKA','PKC']])\n",
    "    df1['PKA'] = alpha\n",
    "    gt_pka.append(np.mean(reg.predict(df1.values)))\n",
    "    \n",
    "\n",
    "raf_interventions = np.linspace(min(data['Raf']), max(data['Raf']), 3)\n",
    "reg = MLPClassifier().fit(data[['Raf','PKC', 'PKA']].values, data['Akt'].values)\n",
    "gt_raf = []\n",
    "for alpha in np.linspace(min(data['Raf']), max(data['Raf']), 3):\n",
    "    df1 = pd.DataFrame.copy(data[['Raf','PKC', 'PKA']])\n",
    "    df1['Raf'] = alpha\n",
    "    gt_raf.append(np.mean(reg.predict(df1.values)))\n",
    "    \n",
    "\n",
    "reg = MLPClassifier().fit(data[['Mek', 'PKA']].values, data['Akt'].values)\n",
    "gt_mek = []\n",
    "for alpha in np.linspace(min(data['Mek']), max(data['Mek']), 3):\n",
    "    df1 = pd.DataFrame.copy(data[['Mek', 'PKA']])\n",
    "    df1['Mek'] = alpha\n",
    "    gt_mek.append(np.mean(reg.predict(df1.values)))\n",
    "    \n",
    "\n",
    "reg = MLPClassifier().fit(data[['Erk', 'PKA']].values, data['Akt'].values)\n",
    "gt_erk = []\n",
    "for alpha in np.linspace(min(data['Erk']), max(data['Erk']), 3):\n",
    "    df1 = pd.DataFrame.copy(data[['Erk', 'PKA']])\n",
    "    df1['Erk'] = alpha\n",
    "    gt_erk.append(np.mean(reg.predict(df1.values)))\n",
    "    \n",
    "gt_jnk = np.zeros_like(gt_erk)    \n",
    "gt_p38 = np.zeros_like(gt_erk)\n",
    "    \n",
    "aces_gt=[]\n",
    "aces_gt.append(gt_pkc-np.mean(gt_pkc))\n",
    "aces_gt.append(gt_pka-np.mean(gt_pka))\n",
    "aces_gt.append(gt_raf-np.mean(gt_raf))\n",
    "aces_gt.append(gt_mek-np.mean(gt_mek))\n",
    "aces_gt.append(gt_erk-np.mean(gt_erk))\n",
    "aces_gt.append(gt_jnk-np.mean(gt_jnk))\n",
    "aces_gt.append(gt_p38-np.mean(gt_p38))\n",
    "np.save('./aces/aces_gt.npy',aces_gt,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54475f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0.]),\n",
       " array([ 0.5624, -0.2812, -0.2812]),\n",
       " array([-0.2254,  0.0988,  0.1266]),\n",
       " array([-0.1915,  0.    ,  0.1915]),\n",
       " array([-0.461 , -0.2695,  0.7305]),\n",
       " array([0., 0., 0.]),\n",
       " array([0., 0., 0.])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aces_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a610a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49f7aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size=5 # to get mean and std values\n",
    "\n",
    "values = list(data.columns.values)\n",
    "y = data[values[-1:]]\n",
    "y = np.array(y, dtype='long')\n",
    "X = data[values[:-1]]\n",
    "X = np.array(X)\n",
    "\n",
    "indices = np.random.choice(len(X), len(X), replace=False)\n",
    "X_values = X[indices]\n",
    "y_values = y[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 1000\n",
    "val_size = 1000\n",
    "\n",
    "X_test = X_values[-test_size:]\n",
    "X_trainval = X_values[:-test_size]\n",
    "X_val = X_trainval[-val_size:]\n",
    "X_train = X_trainval[:-val_size]\n",
    "\n",
    "y_test = y_values[-test_size:]\n",
    "y_trainval = y_values[:-test_size]\n",
    "y_val = y_trainval[-val_size:]\n",
    "y_train = y_trainval[:-val_size]\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "# Interval / Epochs\n",
    "interval = 5\n",
    "epoch = 50\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6d7ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class samp_network(nn.Module):\n",
    "    def __init__(self, input_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.fc1 = nn.Linear(self.input_size, 2)\n",
    "        self.fc2 = nn.Linear(2, 2)\n",
    "        self.fc3 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee51d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, feature_dim, batch_size=64, device='cpu',sample_size=50000):\n",
    "        super(Model, self).__init__()\n",
    "        self.batchsize=batch_size\n",
    "\n",
    "        self.causal_link_pkc_pka = samp_network()\n",
    "        self.causal_link_pkc_pka_raf = samp_network(input_size=2)\n",
    "        self.causal_link_pkc_pka_raf_mek = samp_network(input_size=3)\n",
    "        self.causal_link_mek_pka_erk = samp_network(input_size=2)\n",
    "        self.causal_link_pkc_pka_jnk = samp_network(input_size=2)\n",
    "        self.causal_link_pkc_pka_p38 = samp_network(input_size=2)\n",
    "\n",
    "        self.batchsize=batch_size\n",
    "        self.first_layer = nn.Linear(7,6)\n",
    "        self.second_layer = nn.Linear(6,5)\n",
    "        self.third_layer = nn.Linear(5,5)\n",
    "        self.fourth_layer = nn.Linear(5,4)\n",
    "        self.regression_layer = nn.Linear(4, 3)\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def forward(self, inp, phase='freeze', inde=0, alpha=0):\n",
    "        if phase=='freeze':\n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            x = F.relu(self.fourth_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "            return prediction\n",
    "\n",
    "        elif phase=='train_dag':\n",
    "\n",
    "            batch_size = inp.shape[0]\n",
    "            pkc_sample = inp[:,0].reshape(batch_size, -1)\n",
    "\n",
    "            pka_sample = self.causal_link_pkc_pka(pkc_sample)\n",
    "            raf_sample = self.causal_link_pkc_pka_raf(torch.cat((pkc_sample, pka_sample), dim=1))\n",
    "            mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((pkc_sample, pka_sample, raf_sample), dim=1))\n",
    "            erk_sample = self.causal_link_mek_pka_erk(torch.cat((mek_sample, pka_sample), dim=1))\n",
    "            jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((pkc_sample, pka_sample), dim=1))\n",
    "            p38_sample = self.causal_link_pkc_pka_p38(torch.cat((pkc_sample, pka_sample), dim=1))\n",
    "            inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "\n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            x = F.relu(self.fourth_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "            \n",
    "            return prediction, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample\n",
    "\n",
    "        elif phase=='sample':\n",
    "            if inde == 0:\n",
    "                pkc_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                pka_sample = self.causal_link_pkc_pka(torch.tensor(pkc_sample, dtype=torch.float))\n",
    "                raf_sample = self.causal_link_pkc_pka_raf(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), pka_sample), dim=1))\n",
    "                mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
    "                erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 1:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                raf_sample = self.causal_link_pkc_pka_raf(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), pka_sample), dim=1))\n",
    "                mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
    "                erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 2:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
    "                erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 3:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                mek_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 4:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                mek_sample = torch.tensor(inp[:,3].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                erk_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "\n",
    "            elif inde == 5:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                mek_sample = torch.tensor(inp[:,3].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                erk_sample = torch.tensor(inp[:,4].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                jnk_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 6:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                mek_sample = torch.tensor(inp[:,3].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                erk_sample = torch.tensor(inp[:,4].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                jnk_sample = torch.tensor(inp[:,5].reshape(self.sample_size,-1))\n",
    "                p38_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20805d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to load data from csv file\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe1, dataframe2):\n",
    "        self.data_points = dataframe1\n",
    "        self.targets = dataframe2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_point = torch.tensor(np.array(self.data_points[idx]), dtype=torch.float)\n",
    "        target_point = torch.tensor(np.array(self.targets[idx]), dtype=torch.long)\n",
    "        return input_point, target_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef8209",
   "metadata": {},
   "source": [
    "# ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aaaef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 78.5999984741211\n",
      "test accuracy: 80.4000015258789\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.5\n",
      "test accuracy: 80.5\n",
      "\n",
      "15 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 80.5999984741211\n",
      "\n",
      "20 5\n",
      "validation accuracy: 78.9000015258789\n",
      "test accuracy: 81.0999984741211\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "10 5\n",
      "validation accuracy: 73.69999694824219\n",
      "test accuracy: 73.30000305175781\n",
      "\n",
      "15 5\n",
      "validation accuracy: 76.80000305175781\n",
      "test accuracy: 78.69999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 77.19999694824219\n",
      "test accuracy: 78.9000015258789\n",
      "\n",
      "25 5\n",
      "validation accuracy: 77.9000015258789\n",
      "test accuracy: 80.19999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "35 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "40 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "45 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.19999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 68.80000305175781\n",
      "test accuracy: 68.0\n",
      "\n",
      "10 5\n",
      "validation accuracy: 77.4000015258789\n",
      "test accuracy: 79.80000305175781\n",
      "\n",
      "15 5\n",
      "validation accuracy: 78.4000015258789\n",
      "test accuracy: 81.0\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.0\n",
      "test accuracy: 81.19999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.0\n",
      "test accuracy: 81.5\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 78.5999984741211\n",
      "test accuracy: 80.30000305175781\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "15 5\n",
      "validation accuracy: 78.80000305175781\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 72.0999984741211\n",
      "test accuracy: 73.0\n",
      "\n",
      "10 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    erm_model = Model(7,sample_size=len(data))\n",
    "    optimizer = optim.Adam([{'params': erm_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "\n",
    "    for ep in range(0,epoch): \n",
    "        trainval = DataSet(X_train,y_train)\n",
    "        train_loader = DataLoader(trainval, batch_size=batch_size)\n",
    "        for input_data, target in train_loader:\n",
    "            erm_model.zero_grad()\n",
    "            output = erm_model(input_data)\n",
    "            loss = loss_func(output,target.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if ep%interval == 0:\n",
    "            print(ep, interval)\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=1)\n",
    "            acc_val = 0\n",
    "            acc_test = 0\n",
    "            \n",
    "            for input_data, target in val_loader:\n",
    "                output = erm_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_val += acc\n",
    "\n",
    "            print ('validation accuracy:', float(acc_val/len(val_loader)))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            \n",
    "            for input_data, target in test_loader:\n",
    "                output = erm_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_test += acc\n",
    "                \n",
    "            print('test accuracy:', float(acc_test/len(test_loader)))\n",
    "            print()\n",
    "    print(\"************\")\n",
    "    torch.save(erm_model, \"models/erm_sachs_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83d0d6",
   "metadata": {},
   "source": [
    "# Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcada9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frechet_results = []\n",
    "rmse_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    print(ensemble)\n",
    "    model = torch.load(\"models/erm_sachs_\"+str(ensemble+1))\n",
    "\n",
    "    data_pkc = data['PKC'].values[:3]\n",
    "    data_pka = data['PKA'].values[:3]\n",
    "    data_raf = data['Raf'].values[:3]\n",
    "    data_mek = data['Mek'].values[:3]\n",
    "    data_erk = data['Erk'].values[:3]\n",
    "    data_jnk = data['Jnk'].values[:3]\n",
    "    data_p38 = data['P38'].values[:3]\n",
    "\n",
    "    do_pkc = np.linspace(min(data['PKC']), max(data['PKC']), 3)\n",
    "    do_pka = np.linspace(min(data['PKA']), max(data['PKA']), 3)\n",
    "    do_raf = np.linspace(min(data['Raf']), max(data['Raf']), 3)\n",
    "    do_mek = np.linspace(min(data['Mek']), max(data['Mek']), 3)\n",
    "    do_erk = np.linspace(min(data['Erk']), max(data['Erk']), 3)\n",
    "    do_jnk = np.linspace(min(data['Jnk']), max(data['Jnk']), 3)\n",
    "    do_p38 = np.linspace(min(data['P38']), max(data['P38']), 3)\n",
    "\n",
    "\n",
    "    test_array_pkc = np.stack((data_pkc, data_pka, data_raf, data_mek, data_erk, data_jnk, data_p38), axis=1)\n",
    "    test_array_pkc = torch.from_numpy(test_array_pkc).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_pka = np.stack((data_pkc, data_pka, data_raf, data_mek, data_erk, data_jnk, data_p38), axis=1)\n",
    "    test_array_pka = torch.from_numpy(test_array_pka).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_raf = np.stack((data_pkc, data_pka, data_raf, data_mek, data_erk, data_jnk, data_p38), axis=1)\n",
    "    test_array_raf = torch.from_numpy(test_array_raf).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_mek = np.stack((data_pkc, data_pka, data_raf, data_mek, data_erk, data_jnk, data_p38), axis=1)\n",
    "    test_array_mek = torch.from_numpy(test_array_mek).type(torch.FloatTensor)\n",
    "\n",
    "    \n",
    "    test_array_erk = np.stack((data_pkc, data_pka, data_raf, data_mek, data_erk, data_jnk, data_p38), axis=1)\n",
    "    test_array_erk = torch.from_numpy(test_array_erk).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_jnk = np.stack((data_pkc, data_pka, data_raf, data_mek, data_erk, data_jnk, data_p38), axis=1)\n",
    "    test_array_jnk = torch.from_numpy(test_array_jnk).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_p38 = np.stack((data_pkc, data_pka, data_raf, data_mek, data_erk, data_jnk, do_p38), axis=1)\n",
    "    test_array_p38 = torch.from_numpy(test_array_p38).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "    ig = IntegratedGradients(model)\n",
    "    # print(gt_aces[0].shape)\n",
    "\n",
    "    ig_attr_test_pkc, delta = ig.attribute(test_array_pkc, n_steps=50, return_convergence_delta=True, target=2)\n",
    "    ig_attr_test_pka, delta = ig.attribute(test_array_pka, n_steps=50, return_convergence_delta=True, target=2)\n",
    "    ig_attr_test_raf, delta = ig.attribute(test_array_raf, n_steps=50, return_convergence_delta=True, target=2)\n",
    "    ig_attr_test_mek, delta = ig.attribute(test_array_mek, n_steps=50, return_convergence_delta=True, target=2)\n",
    "    ig_attr_test_erk, delta = ig.attribute(test_array_erk, n_steps=50, return_convergence_delta=True, target=2)\n",
    "    ig_attr_test_jnk, delta = ig.attribute(test_array_jnk, n_steps=50, return_convergence_delta=True, target=2)\n",
    "    ig_attr_test_p38, delta = ig.attribute(test_array_p38, n_steps=50, return_convergence_delta=True, target=2)\n",
    "\n",
    "    rmse_results.append([rmse(aces_gt[0], np.array(ig_attr_test_pkc[:,0])),\n",
    "                         rmse(aces_gt[1], np.array(ig_attr_test_pka[:,1])),\n",
    "                         rmse(aces_gt[2], np.array(ig_attr_test_raf[:,2])),\n",
    "                         rmse(aces_gt[3], np.array(ig_attr_test_mek[:,3])),\n",
    "                         rmse(aces_gt[4], np.array(ig_attr_test_erk[:,4])),\n",
    "                         rmse(aces_gt[5], np.array(ig_attr_test_jnk[:,5])),\n",
    "                         rmse(aces_gt[6], np.array(ig_attr_test_p38[:,6]))])\n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(-1, 1), np.array(ig_attr_test_pkc[:,0]).reshape(-1, 1)),\n",
    "                         frechet_dist(aces_gt[1].reshape(-1, 1), np.array(ig_attr_test_pka[:,1]).reshape(-1, 1)),\n",
    "                         frechet_dist(aces_gt[2].reshape(-1, 1), np.array(ig_attr_test_raf[:,2]).reshape(-1, 1)),\n",
    "                         frechet_dist(aces_gt[3].reshape(-1, 1), np.array(ig_attr_test_mek[:,3]).reshape(-1, 1)),\n",
    "                         frechet_dist(aces_gt[4].reshape(-1, 1), np.array(ig_attr_test_erk[:,4]).reshape(-1, 1)),\n",
    "                         frechet_dist(aces_gt[5].reshape(-1, 1), np.array(ig_attr_test_jnk[:,5]).reshape(-1, 1)),\n",
    "                         frechet_dist(aces_gt[6].reshape(-1, 1), np.array(ig_attr_test_p38[:,6]).reshape(-1, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9c0b1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.08236072 2.29456244 0.15260043 0.20471391 4.33897528 0.08225392\n",
      " 0.26346759]\n",
      "rmse std:  [0.072581   1.40182269 0.03694011 0.04234252 3.25117804 0.04907327\n",
      " 0.18473028]\n",
      "rmse all features mean:  1.059847755435633\n",
      "rmse all features std:  0.7198097026594029\n",
      "frechet mean:  [0.14265295 2.89081229 0.2187971  0.33942284 5.63291074 0.12051722\n",
      " 0.41775694]\n",
      "frechet std:  [0.12571399 1.6272485  0.05619806 0.08208235 4.04983256 0.06859059\n",
      " 0.30959137]\n",
      "frechet all features mean:  1.3946957246431217\n",
      "frechet all features std:  0.9027510582110259\n"
     ]
    }
   ],
   "source": [
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab705dfc",
   "metadata": {},
   "source": [
    "# Causal Attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6803a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=3\n",
    "num_feat=7\n",
    "num_alpha=3\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_ca_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_ca_total = []\n",
    "    model = torch.load(\"models/erm_sachs_\"+str(ensemble+1))\n",
    "    for output_index in range(0,1):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_ca_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "    aces_ca_total.append(ace_ca_total)\n",
    "np.save('./aces/sachs_ca_total.npy',aces_ca_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1b4766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.10176131 2.19258358 0.11545693 0.21651297 2.23200216 0.0742025\n",
      " 0.09842272]\n",
      "rmse std:  [0.09715836 0.9016019  0.05735296 0.13859572 0.63006274 0.0440616\n",
      " 0.06976197]\n",
      "rmse all features mean:  0.7187060241403819\n",
      "rmse all features std:  0.27694218134075055\n",
      "frechet mean:  [0.13223125 2.97188839 0.16011002 0.27970836 3.12497292 0.09462399\n",
      " 0.12896724]\n",
      "frechet std:  [0.12704554 1.14069842 0.08258818 0.18027261 0.90048447 0.05460573\n",
      " 0.09408247]\n",
      "frechet all features mean:  0.9846431686074393\n",
      "frechet all features std:  0.36853963237833876\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_ca_total[ensemble][0]),\n",
    "                         rmse(aces_gt[1], aces_ca_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_ca_total[ensemble][2]),\n",
    "                         rmse(aces_gt[3], aces_ca_total[ensemble][3]),\n",
    "                         rmse(aces_gt[4], aces_ca_total[ensemble][4]),\n",
    "                         rmse(aces_gt[5], aces_ca_total[ensemble][5]),\n",
    "                         rmse(aces_gt[6], aces_ca_total[ensemble][6])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(3,1), aces_ca_total[ensemble][0].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(3,1), aces_ca_total[ensemble][1].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(3,1), aces_ca_total[ensemble][2].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(3,1), aces_ca_total[ensemble][3].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(3,1), aces_ca_total[ensemble][4].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[5].reshape(3,1), aces_ca_total[ensemble][5].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[6].reshape(3,1), aces_ca_total[ensemble][6].reshape(3,1))])\n",
    "    \n",
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c09b4",
   "metadata": {},
   "source": [
    "# Causal Shapley Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a6dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability is taken over indices of baseline only\n",
    "def get_probabiity(unique_count, x_hat, indices_baseline, n):\n",
    "    if len(indices_baseline) > 0:\n",
    "        count = 0\n",
    "        for i in unique_count:\n",
    "            check = True\n",
    "            key = np.asarray(i)\n",
    "            for j in indices_baseline:\n",
    "                check = check and key[j] == x_hat[j]\n",
    "            if check:\n",
    "                count += unique_count[i]\n",
    "        return count / n\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def conditional_prob(unique_count, x_hat, indices, indices_baseline, n):\n",
    "    numerator_indices = indices + indices_baseline\n",
    "    numerator = get_probabiity(unique_count, x_hat, numerator_indices, n)\n",
    "    denominator = get_probabiity(unique_count, x_hat, indices, n)\n",
    "    try:\n",
    "        kk = numerator / denominator\n",
    "    except ZeroDivisionError:\n",
    "        denominator = 1e-7\n",
    "        # pass\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def causal_prob(unique_count, x_hat, indices, indices_baseline, causal_struc, n):\n",
    "    p = 1\n",
    "    for i in indices_baseline:\n",
    "        intersect_s, intersect_s_hat = [], []\n",
    "        intersect_s_hat.append(i)\n",
    "        if len(causal_struc[str(i)]) > 0:\n",
    "            for index in causal_struc[str(i)]:\n",
    "                if index in indices or index in indices_baseline:\n",
    "                    intersect_s.append(index)\n",
    "            p *= conditional_prob(unique_count, x_hat, intersect_s, intersect_s_hat, n)\n",
    "        else:\n",
    "            p *= get_probabiity(unique_count, x_hat, intersect_s_hat, n)\n",
    "    return p\n",
    "\n",
    "\n",
    "def get_baseline(X, model):\n",
    "    fx = 0\n",
    "    n_features = X.shape[1]\n",
    "    X = np.reshape(X, (len(X), 1, n_features))\n",
    "    for i in X:\n",
    "        fx += torch.softmax(model(torch.tensor(i, dtype=torch.float)), 0)[0]\n",
    "    return fx / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fd54c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns value from using function for different versions\n",
    "def get_value(version, permutation, X, x, unique_count, causal_struct, model, N, is_classification, xi):\n",
    "    # intializing returns\n",
    "    absolute_diff, f1, f2 = 0, 0, 0\n",
    "    xi_index = permutation.index(xi)\n",
    "    indices = permutation[:xi_index + 1]\n",
    "    indices_baseline = permutation[xi_index + 1:]\n",
    "    x_hat = np.zeros(N)\n",
    "    x_hat_2 = np.zeros(N)\n",
    "    len_X = len(X)\n",
    "    for j in indices:\n",
    "        x_hat[j] = x[j]\n",
    "        x_hat_2[j] = x[j]\n",
    "    if version == '2' or version == '3' or version == '4':\n",
    "        proba1, proba2 = 0, 0\n",
    "        baseline_check_1, baseline_check_2 = [], []\n",
    "        f1, f2 = 0, 0\n",
    "        indices_baseline_2 = indices_baseline[:]\n",
    "        for i in unique_count:\n",
    "            X = np.asarray(i)\n",
    "            for j in indices_baseline:\n",
    "                x_hat[j] = X[j]\n",
    "                x_hat_2[j] = X[j]\n",
    "\n",
    "            # No repetition\n",
    "            # Eg if baseline_indices is null, it'll only run once as x_hat will stay the same over each iteration\n",
    "            if x_hat.tolist() not in baseline_check_1:\n",
    "                baseline_check_1.append(x_hat.tolist())\n",
    "                if version == '2':\n",
    "                    prob_x_hat = get_probabiity(unique_count, x_hat, indices_baseline, len_X)\n",
    "                elif version == '3':\n",
    "                    prob_x_hat = conditional_prob(unique_count, x_hat, indices, indices_baseline, len_X)\n",
    "                else:\n",
    "                    prob_x_hat = causal_prob(unique_count, x_hat, indices, indices_baseline, causal_struct, len_X)\n",
    "                proba1 += prob_x_hat\n",
    "                x_hat = np.reshape(x_hat, (1, N))\n",
    "                f1 = f1 + (torch.softmax(model(torch.tensor(x_hat, dtype=torch.float)), 0)[0] * prob_x_hat if is_classification else model.predict(\n",
    "                    x_hat) * prob_x_hat)\n",
    "\n",
    "            # xi index will be given to baseline for f2\n",
    "            x_hat_2[xi] = X[xi]\n",
    "            if xi not in indices_baseline_2:\n",
    "                indices_baseline_2.append(xi)\n",
    "\n",
    "            # No repetition\n",
    "            indices_2 = indices[:]\n",
    "            indices_2.remove(xi)\n",
    "            if x_hat_2.tolist() not in baseline_check_2:\n",
    "                baseline_check_2.append(x_hat_2.tolist())\n",
    "                if version == '2':\n",
    "                    prob_x_hat_2 = get_probabiity(unique_count, x_hat_2, indices_baseline_2, len_X)\n",
    "                elif version == '3':\n",
    "                    prob_x_hat_2 = conditional_prob(unique_count, x_hat_2, indices_2, indices_baseline_2, len_X)\n",
    "                else:\n",
    "                    prob_x_hat_2 = causal_prob(unique_count, x_hat_2, indices_2, indices_baseline_2, causal_struct,\n",
    "                                               len_X)\n",
    "                proba2 += prob_x_hat_2\n",
    "                x_hat_2 = np.reshape(x_hat_2, (1, N))\n",
    "                f2 = f2 + (torch.softmax(model(torch.tensor(x_hat, dtype=torch.float)), 0)[0] * prob_x_hat_2 if is_classification else model.predict(\n",
    "                    x_hat_2) * prob_x_hat_2)\n",
    "            x_hat = np.squeeze(x_hat)\n",
    "            x_hat_2 = np.squeeze(x_hat_2)\n",
    "        absolute_diff = abs(f1 - f2)\n",
    "    elif version == '1':\n",
    "        f1, f2 = 0, 0\n",
    "        for i in range(len(X)):\n",
    "            for j in indices_baseline:\n",
    "                x_hat[j] = X[i][j]\n",
    "                x_hat_2[j] = X[i][j]\n",
    "            x_hat = np.reshape(x_hat, (1, N))\n",
    "            f1 += torch.softmax(model(torch.tensor(x_hat, dtype=torch.float)), 0)[0] if is_classification else model.predict(x_hat)\n",
    "            x_hat_2[xi] = X[i][xi]\n",
    "            x_hat_2 = np.reshape(x_hat_2, (1, N))\n",
    "            f2 += torch.softmax(model(torch.tensor(x_hat, dtype=torch.float)), 0)[0] if is_classification else model.predict(x_hat_2)\n",
    "            x_hat = np.squeeze(x_hat)\n",
    "            x_hat_2 = np.squeeze(x_hat_2)\n",
    "        absolute_diff = abs(f1 - f2) / len_X\n",
    "        f1 = f1 / len_X\n",
    "        f2 = f2 / len_X\n",
    "    return absolute_diff, f1, f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7d9d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_shapley(version, xi, N, X, x, m, model, unique_count, causal_struct, is_classification,\n",
    "                        global_shap=False):\n",
    "    R = list(itertools.permutations(range(N)))\n",
    "    random.shuffle(R)\n",
    "    score = 0\n",
    "    count_negative = 0\n",
    "    vf1, vf2 = 0, 0\n",
    "    for i in range(m):\n",
    "        abs_diff, f1, f2 = get_value(version, list(R[i]), X, x, unique_count, causal_struct, model, N,\n",
    "                                     is_classification, xi)\n",
    "        vf1 += f1\n",
    "        vf2 += f2\n",
    "        score += abs_diff\n",
    "        if not global_shap:\n",
    "            if vf2 > vf1:\n",
    "                count_negative -= 1\n",
    "            else:\n",
    "                count_negative += 1\n",
    "    if count_negative < 0 and not global_shap:\n",
    "        score = -1 * score\n",
    "    return score / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f65ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapley(model, version, local_shap=0):\n",
    "    sigma_phi = 0\n",
    "    global_shap=True\n",
    "    causal_struct = None\n",
    "    try:\n",
    "        causal_struct = json.load(open('sachs.json', 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    n_features = 7\n",
    "    unique_count = collections.Counter(map(tuple, X_train[50:75]))\n",
    "    ##### f(x) with baseline\n",
    "    f_o = get_baseline(X_train[50:75], model)\n",
    "    rmse_shapley_values = []\n",
    "    frechet_shapley_values = []\n",
    "    shapley_vals = []\n",
    "    for feature in range(n_features):\n",
    "        global_shap_score = Parallel(n_jobs=-1)(\n",
    "            delayed(approximate_shapley)(version, feature, n_features, X_train[50:75], x, math.factorial(n_features), model,\n",
    "                                         unique_count, causal_struct, True, global_shap) for i, x in\n",
    "            enumerate(X_train[50:75]))\n",
    "        \n",
    "        shapley_vals.append(np.array([i.detach().numpy() for i in global_shap_score]))\n",
    "    return shapley_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ff3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "shapley_values = []\n",
    "for ensemble in range(ensemble_size):\n",
    "    print(ensemble)\n",
    "    model = torch.load(\"models/erm_sachs_\"+str(ensemble+1))\n",
    "    s = shapley(model, version='4', local_shap=12)\n",
    "    shapley_values.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "203c3c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.1999237  0.46053444 0.24783358 0.23926059 0.53359662 0.25573431\n",
      " 0.31379661]\n",
      "rmse std:  [0.00137769 0.00446622 0.00087216 0.00107753 0.0040468  0.00303377\n",
      " 0.00321371]\n",
      "rmse all features mean:  0.3215256935169771\n",
      "rmse all features std:  0.002583983476831131\n",
      "frechet mean:  [0.3093913  0.29537311 0.27856761 0.37394828 0.3639802  0.36520084\n",
      " 0.41422786]\n",
      "frechet std:  [0.00254364 0.00022662 0.00390349 0.00066014 0.00280709 0.00546883\n",
      " 0.0025095 ]\n",
      "frechet all features mean:  0.34295560100248884\n",
      "frechet all features std:  0.002588472231213391\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "shapley_values = np.array(shapley_values)\n",
    "for ensemble in range(ensemble_size-1):\n",
    "    rmses = []\n",
    "    frechets = []\n",
    "    for feature in range(7):\n",
    "        shaps = []\n",
    "        for inte in [0,0.5,1]:\n",
    "            indices = X_train[50:75][:, feature] == inte\n",
    "            shaps.append(np.mean(shapley_values[ensemble][feature][indices,0]))\n",
    "\n",
    "        rmses.append(rmse(shaps, aces_gt[feature]))\n",
    "        frechets.append(frechet_dist(np.array(shaps).reshape(-1,1),\n",
    "        np.mean(aces_gt[feature]).reshape(-1,1)))\n",
    "    rmse_results.append(rmses)\n",
    "    frechet_results.append(frechets)\n",
    "    \n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471a2a7",
   "metadata": {},
   "source": [
    "# CREDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e73c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = {0:(lambda ii:ii), 1:(lambda ii:ii), 2:(lambda ii:ii),\n",
    "        3:(lambda ii:ii), 4:(lambda ii:ii), 5:(lambda ii:0*ii),\n",
    "        6:(lambda ii:0*ii)}\n",
    "\n",
    "def get_grad(x, prior):\n",
    "    a = x.clone().detach().requires_grad_(True)\n",
    "    for f in prior.keys():\n",
    "        z = prior[f]\n",
    "        z = torch.sum(z(a[0][f]), dim=0)\n",
    "        z.backward()\n",
    "    return a.grad\n",
    "\n",
    "def get_grads_to_match(ip, prior):\n",
    "    return get_grad(ip, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d41799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 78.5\n",
      "test accuracy: 80.0\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.5\n",
      "test accuracy: 80.9000015258789\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.5\n",
      "test accuracy: 81.19999694824219\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 69.30000305175781\n",
      "test accuracy: 69.0\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 30.700000762939453\n",
      "test accuracy: 31.700000762939453\n",
      "\n",
      "5 5\n",
      "validation accuracy: 68.5999984741211\n",
      "test accuracy: 67.5999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.5\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.0\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.0\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 30.700000762939453\n",
      "test accuracy: 31.700000762939453\n",
      "\n",
      "5 5\n",
      "validation accuracy: 77.19999694824219\n",
      "test accuracy: 79.80000305175781\n",
      "\n",
      "10 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    credo_model = Model(7,sample_size=len(data))\n",
    "    optimizer = optim.Adam([{'params': credo_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "\n",
    "    for ep in range(0,epoch): \n",
    "        trainval = DataSet(X_train,y_train)\n",
    "        train_loader = DataLoader(trainval, batch_size=batch_size)\n",
    "        for input_data, target in train_loader:\n",
    "            credo_model.zero_grad()\n",
    "            input_data.requires_grad=True\n",
    "            output = credo_model(input_data)\n",
    "            \n",
    "            calc_grads = (autograd.grad(torch.sum(output[0], dim=0), input_data, retain_graph=True, create_graph=True)[0])\n",
    "            grads_to_match = get_grads_to_match(input_data, prior) \n",
    "            hinge_input = torch.abs(grads_to_match - calc_grads)\n",
    "            loss = loss_func(output,target.squeeze()) + 0.01 * torch.norm(torch.clamp(hinge_input, min=0), p=1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if ep%interval == 0:\n",
    "            print(ep, interval)\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=1)\n",
    "            acc_val = 0\n",
    "            acc_test = 0\n",
    "            \n",
    "            for input_data, target in val_loader:\n",
    "                output = credo_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_val += acc\n",
    "\n",
    "            print ('validation accuracy:', float(acc_val/len(val_loader)))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            \n",
    "            for input_data, target in test_loader:\n",
    "                output = credo_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_test += acc\n",
    "                \n",
    "            print('test accuracy:', float(acc_test/len(test_loader)))\n",
    "            print()\n",
    "    print(\"************\")\n",
    "    torch.save(credo_model, \"models/credo_sachs_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa05775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=3\n",
    "num_feat=7\n",
    "num_alpha=3\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_credo_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_credo_total = []\n",
    "    model = torch.load(\"models/credo_sachs_\"+str(ensemble+1))\n",
    "    for output_index in range(0,1):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_credo_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "    aces_credo_total.append(ace_ca_total)\n",
    "np.save('./aces/sachs_credo_total.npy',aces_credo_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89a1e1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.08285197 3.81488683 0.02983257 0.42860937 2.87066199 0.13646983\n",
      " 0.04895687]\n",
      "rmse std:  [0.00000000e+00 4.44089210e-16 0.00000000e+00 5.55111512e-17\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "rmse all features mean:  1.0588956327825614\n",
      "rmse all features std:  7.137148015447435e-17\n",
      "frechet mean:  [0.11701838 5.02455987 0.03905235 0.56964442 4.04197361 0.1671408\n",
      " 0.0688235 ]\n",
      "frechet std:  [0. 0. 0. 0. 0. 0. 0.]\n",
      "frechet all features mean:  1.4326018477303641\n",
      "frechet all features std:  0.0\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_credo_total[ensemble][0]),\n",
    "                         rmse(aces_gt[1], aces_credo_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_credo_total[ensemble][2]),\n",
    "                         rmse(aces_gt[3], aces_credo_total[ensemble][3]),\n",
    "                         rmse(aces_gt[4], aces_credo_total[ensemble][4]),\n",
    "                         rmse(aces_gt[5], aces_credo_total[ensemble][5]),\n",
    "                         rmse(aces_gt[6], aces_credo_total[ensemble][6])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(3,1), aces_credo_total[ensemble][0].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(3,1), aces_credo_total[ensemble][1].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(3,1), aces_credo_total[ensemble][2].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(3,1), aces_credo_total[ensemble][3].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(3,1), aces_credo_total[ensemble][4].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[5].reshape(3,1), aces_credo_total[ensemble][5].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[6].reshape(3,1), aces_credo_total[ensemble][6].reshape(3,1))])\n",
    "    \n",
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958d8e0",
   "metadata": {},
   "source": [
    "# AHCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffb8cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 81.5999984741211\n",
      "test accuracy: 80.3499984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 82.19999694824219\n",
      "test accuracy: 81.25\n",
      "\n",
      "15 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "0 5\n",
      "validation accuracy: 9.300000190734863\n",
      "test accuracy: 7.25\n",
      "\n",
      "5 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "10 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "15 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "20 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "25 5\n",
      "validation accuracy: 80.80000305175781\n",
      "test accuracy: 79.5999984741211\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.4000015258789\n",
      "test accuracy: 81.0999984741211\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5\n",
      "test accuracy: 81.3499984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.4000015258789\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.19999694824219\n",
      "test accuracy: 81.25\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.30000305175781\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.4000015258789\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "0 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 73.5\n",
      "test accuracy: 72.8499984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 82.4000015258789\n",
      "test accuracy: 80.94999694824219\n",
      "\n",
      "15 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.3499984741211\n",
      "\n",
      "20 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "25 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.3499984741211\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.3499984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "0 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 71.9000015258789\n",
      "test accuracy: 73.55000305175781\n",
      "\n",
      "10 5\n",
      "validation accuracy: 82.30000305175781\n",
      "test accuracy: 80.80000305175781\n",
      "\n",
      "15 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "25 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "0 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 66.5\n",
      "test accuracy: 68.9000015258789\n",
      "\n",
      "10 5\n",
      "validation accuracy: 82.30000305175781\n",
      "test accuracy: 80.75\n",
      "\n",
      "15 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    # Interval / Epochs\n",
    "    \n",
    "    mse_loss_func = nn.MSELoss()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    ahce_model = Model(7,sample_size=len(data))\n",
    "\n",
    "    optimizer = optim.Adam([{'params': ahce_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "\n",
    "    for ep in range(0,100):\n",
    "        trainval = DataSet(X_train,y_train)\n",
    "        train_loader = DataLoader(trainval, batch_size=64)\n",
    "        for input_data, target in train_loader: \n",
    "            for phase in ['train_dag', 'freeze']:\n",
    "                ahce_model.zero_grad()\n",
    "                if phase == 'freeze':\n",
    "                    output = ahce_model(input_data)\n",
    "                    loss = loss_func(output,target.squeeze())\n",
    "                    loss.backward(retain_graph=True)\n",
    "\n",
    "                else:\n",
    "                    output, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample  = ahce_model(input_data, phase='train_dag')             \n",
    "                    \n",
    "                    loss = 0.01*loss_func(output,target.squeeze()) \n",
    "                    loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "\n",
    "        if ep%interval == 0:\n",
    "            print(ep, interval)\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=1)\n",
    "            acc_val = 0\n",
    "            acc_test = 0\n",
    "\n",
    "            for input_data, target in val_loader:\n",
    "                output = ahce_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_val += acc\n",
    "\n",
    "            print ('validation accuracy:', float(acc_val/len(val_loader)))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "\n",
    "            for input_data, target in test_loader:\n",
    "                output = ahce_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_test += acc\n",
    "\n",
    "            print('test accuracy:', float(acc_test/len(test_loader)))\n",
    "            print()\n",
    "\n",
    "    torch.save(ahce_model, \"./models/ahce_sachs_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73423e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1986/930309753.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pka_sample = self.causal_link_pkc_pka(torch.tensor(pkc_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_1986/930309753.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  raf_sample = self.causal_link_pkc_pka_raf(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), pka_sample), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  raf_sample = self.causal_link_pkc_pka_raf(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), pka_sample), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n"
     ]
    }
   ],
   "source": [
    "n_classes=3\n",
    "num_c=7#no. of features\n",
    "num_alpha=3\n",
    "\n",
    "aces_ahce_total = []\n",
    "for ensemble in range(5):\n",
    "    ace_ahce_total = []\n",
    "    model =  torch.load(\"./models/ahce_sachs_\"+str(ensemble+1))\n",
    "    for output_index in range(1,2):#For every class\n",
    "        #plt.figure()\n",
    "        for t in range(0,num_c):#For every feature\n",
    "            expectation_do_x = []\n",
    "            for x in [0,0.5,1]:\n",
    "                X_values[:,t] = x\n",
    "                sample_data = model(X_values, phase='sample', inde=t, alpha=x).detach().numpy()\n",
    "                cov = np.cov(sample_data, rowvar=False)\n",
    "                means = np.mean(sample_data, axis=0)\n",
    "                cov=np.array(cov)\n",
    "                mean_vector = np.array(means)\n",
    "                inp=copy.deepcopy(mean_vector)\n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "\n",
    "                output=model(input_torchvar)\n",
    "\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_c):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_c)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "\n",
    "            ace_ahce_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "\n",
    "    aces_ahce_total.append(ace_ahce_total)\n",
    "np.save('./aces/sachs_ahce_total.npy',aces_ahce_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4bb63dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.12475822 0.65547383 0.12277304 0.14353915 0.51898169 0.0186962\n",
      " 0.02440762]\n",
      "rmse std:  [0.06767317 0.1756164  0.03766031 0.01556702 0.34269087 0.01361813\n",
      " 0.01961943]\n",
      "rmse all features mean:  0.2298042509487662\n",
      "rmse all features std:  0.0960636188516346\n",
      "frechet mean:  [0.171739   0.91099781 0.1714031  0.17648882 0.70143739 0.02320759\n",
      " 0.02991145]\n",
      "frechet std:  [0.09771076 0.23724553 0.05571379 0.01884757 0.45834208 0.01672321\n",
      " 0.0240606 ]\n",
      "frechet all features mean:  0.3121693101549147\n",
      "frechet all features std:  0.12980622024131902\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in [0,1,2,3,4]:\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_ahce_total[ensemble][0]),\n",
    "                         rmse(aces_gt[1], aces_ahce_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_ahce_total[ensemble][2]),\n",
    "                         rmse(aces_gt[3], aces_ahce_total[ensemble][3]),\n",
    "                         rmse(aces_gt[4], aces_ahce_total[ensemble][4]),\n",
    "                         rmse(aces_gt[5], aces_ahce_total[ensemble][5]),\n",
    "                         rmse(aces_gt[6], aces_ahce_total[ensemble][6])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(3,1), aces_ahce_total[ensemble][0].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(3,1), aces_ahce_total[ensemble][1].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(3,1), aces_ahce_total[ensemble][2].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(3,1), aces_ahce_total[ensemble][3].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(3,1), aces_ahce_total[ensemble][4].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[5].reshape(3,1), aces_ahce_total[ensemble][5].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[6].reshape(3,1), aces_ahce_total[ensemble][6].reshape(3,1))])\n",
    "    \n",
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08444249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
