{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "356d5cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os,csv,math,sys, joblib\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import itertools\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "import random\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.model_selection, sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "#import pydot\n",
    "from similaritymeasures import frechet_dist\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib\n",
    "seed = 99 # To reproduce the results\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b1d88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sachs_10000.csv\")\n",
    "data.drop(data.columns[[0]], axis=1, inplace=True)\n",
    "cols = data.columns\n",
    "for i in cols:\n",
    "    data[i] = data[i].map({'LOW': 0, 'AVG': 1, 'HIGH': 2})\n",
    "mms = MinMaxScaler()\n",
    "data[['PKC', 'PKA', 'Raf', 'Mek', 'Erk', 'Jnk', 'P38']] = mms.fit_transform(data[['PKC', 'PKA', 'Raf', 'Mek', 'Erk', 'Jnk', 'P38']])\n",
    "data = data[['PKC', 'PKA', 'Raf', 'Mek', 'Erk', 'Jnk', 'P38', 'Akt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca177d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PKC</th>\n",
       "      <th>PKA</th>\n",
       "      <th>Raf</th>\n",
       "      <th>Mek</th>\n",
       "      <th>Erk</th>\n",
       "      <th>Jnk</th>\n",
       "      <th>P38</th>\n",
       "      <th>Akt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PKC  PKA  Raf  Mek  Erk  Jnk  P38  Akt\n",
       "0  0.0  0.5  0.5  0.5  1.0  0.0  0.0    1\n",
       "1  0.0  0.5  0.5  0.0  0.0  0.5  0.0    0\n",
       "2  0.5  0.5  0.5  0.5  1.0  0.5  0.5    1\n",
       "3  0.5  0.5  0.5  0.0  0.0  0.5  0.0    1\n",
       "4  0.0  0.5  1.0  0.0  1.0  0.5  0.0    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4909b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkc_interventions = np.linspace(min(data['PKC']), max(data['PKC']), 3)\n",
    "reg = MLPClassifier().fit(data['PKC'].values.reshape(-1,1), data['Akt'].values)\n",
    "gt_pkc = reg.predict(pkc_interventions.reshape(-1,1))\n",
    "\n",
    "\n",
    "pka_interventions = np.linspace(min(data['PKA']), max(data['PKA']), 3)\n",
    "reg = MLPClassifier().fit(data[['PKA','PKC']].values, data['Akt'].values)\n",
    "gt_pka = []\n",
    "for alpha in np.linspace(min(data['PKA']), max(data['PKA']), 3):\n",
    "    df1 = pd.DataFrame.copy(data[['PKA','PKC']])\n",
    "    df1['PKA'] = alpha\n",
    "    gt_pka.append(np.mean(reg.predict(df1.values)))\n",
    "    \n",
    "\n",
    "raf_interventions = np.linspace(min(data['Raf']), max(data['Raf']), 3)\n",
    "reg = MLPClassifier().fit(data[['Raf','PKC', 'PKA']].values, data['Akt'].values)\n",
    "gt_raf = []\n",
    "for alpha in np.linspace(min(data['Raf']), max(data['Raf']), 3):\n",
    "    df1 = pd.DataFrame.copy(data[['Raf','PKC', 'PKA']])\n",
    "    df1['Raf'] = alpha\n",
    "    gt_raf.append(np.mean(reg.predict(df1.values)))\n",
    "    \n",
    "\n",
    "reg = MLPClassifier().fit(data[['Mek', 'PKA']].values, data['Akt'].values)\n",
    "gt_mek = []\n",
    "for alpha in np.linspace(min(data['Mek']), max(data['Mek']), 3):\n",
    "    df1 = pd.DataFrame.copy(data[['Mek', 'PKA']])\n",
    "    df1['Mek'] = alpha\n",
    "    gt_mek.append(np.mean(reg.predict(df1.values)))\n",
    "    \n",
    "\n",
    "reg = MLPClassifier().fit(data[['Erk', 'PKA']].values, data['Akt'].values)\n",
    "gt_erk = []\n",
    "for alpha in np.linspace(min(data['Erk']), max(data['Erk']), 3):\n",
    "    df1 = pd.DataFrame.copy(data[['Erk', 'PKA']])\n",
    "    df1['Erk'] = alpha\n",
    "    gt_erk.append(np.mean(reg.predict(df1.values)))\n",
    "    \n",
    "gt_jnk = np.zeros_like(gt_erk)    \n",
    "gt_p38 = np.zeros_like(gt_erk)\n",
    "    \n",
    "aces_gt=[]\n",
    "aces_gt.append(gt_pkc-np.mean(gt_pkc))\n",
    "aces_gt.append(gt_pka-np.mean(gt_pka))\n",
    "aces_gt.append(gt_raf-np.mean(gt_raf))\n",
    "aces_gt.append(gt_mek-np.mean(gt_mek))\n",
    "aces_gt.append(gt_erk-np.mean(gt_erk))\n",
    "aces_gt.append(gt_jnk-np.mean(gt_jnk))\n",
    "aces_gt.append(gt_p38-np.mean(gt_p38))\n",
    "np.save('./aces/aces_gt.npy',aces_gt,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54475f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0.]),\n",
       " array([ 0.5624, -0.2812, -0.2812]),\n",
       " array([-0.2254,  0.0988,  0.1266]),\n",
       " array([-0.1915,  0.    ,  0.1915]),\n",
       " array([-0.461 , -0.2695,  0.7305]),\n",
       " array([0., 0., 0.]),\n",
       " array([0., 0., 0.])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aces_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a610a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49f7aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size=5 # to get mean and std values\n",
    "\n",
    "values = list(data.columns.values)\n",
    "y = data[values[-1:]]\n",
    "y = np.array(y, dtype='long')\n",
    "X = data[values[:-1]]\n",
    "X = np.array(X)\n",
    "\n",
    "indices = np.random.choice(len(X), len(X), replace=False)\n",
    "X_values = X[indices]\n",
    "y_values = y[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 1000\n",
    "val_size = 1000\n",
    "\n",
    "X_test = X_values[-test_size:]\n",
    "X_trainval = X_values[:-test_size]\n",
    "X_val = X_trainval[-val_size:]\n",
    "X_train = X_trainval[:-val_size]\n",
    "\n",
    "y_test = y_values[-test_size:]\n",
    "y_trainval = y_values[:-test_size]\n",
    "y_val = y_trainval[-val_size:]\n",
    "y_train = y_trainval[:-val_size]\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "# Interval / Epochs\n",
    "interval = 5\n",
    "epoch = 50\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6d7ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class samp_network(nn.Module):\n",
    "    def __init__(self, input_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.fc1 = nn.Linear(self.input_size, 2)\n",
    "        self.fc2 = nn.Linear(2, 2)\n",
    "        self.fc3 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee51d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, feature_dim, batch_size=64, device='cpu',sample_size=50000):\n",
    "        super(Model, self).__init__()\n",
    "        self.batchsize=batch_size\n",
    "\n",
    "        self.causal_link_pkc_pka = samp_network()\n",
    "        self.causal_link_pkc_pka_raf = samp_network(input_size=2)\n",
    "        self.causal_link_pkc_pka_raf_mek = samp_network(input_size=3)\n",
    "        self.causal_link_mek_pka_erk = samp_network(input_size=2)\n",
    "        self.causal_link_pkc_pka_jnk = samp_network(input_size=2)\n",
    "        self.causal_link_pkc_pka_p38 = samp_network(input_size=2)\n",
    "\n",
    "        self.batchsize=batch_size\n",
    "        self.first_layer = nn.Linear(7,6)\n",
    "        self.second_layer = nn.Linear(6,5)\n",
    "        self.third_layer = nn.Linear(5,5)\n",
    "        self.fourth_layer = nn.Linear(5,4)\n",
    "        self.regression_layer = nn.Linear(4, 3)\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def forward(self, inp, phase='freeze', inde=0, alpha=0):\n",
    "        if phase=='freeze':\n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            x = F.relu(self.fourth_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "            return prediction\n",
    "\n",
    "        elif phase=='train_dag':\n",
    "\n",
    "            batch_size = inp.shape[0]\n",
    "            pkc_sample = inp[:,0].reshape(batch_size, -1)\n",
    "\n",
    "            pka_sample = self.causal_link_pkc_pka(pkc_sample)\n",
    "            raf_sample = self.causal_link_pkc_pka_raf(torch.cat((pkc_sample, pka_sample), dim=1))\n",
    "            mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((pkc_sample, pka_sample, raf_sample), dim=1))\n",
    "            erk_sample = self.causal_link_mek_pka_erk(torch.cat((mek_sample, pka_sample), dim=1))\n",
    "            jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((pkc_sample, pka_sample), dim=1))\n",
    "            p38_sample = self.causal_link_pkc_pka_p38(torch.cat((pkc_sample, pka_sample), dim=1))\n",
    "            inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "\n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            x = F.relu(self.fourth_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "            \n",
    "            return prediction, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample\n",
    "\n",
    "        elif phase=='sample':\n",
    "            if inde == 0:\n",
    "                pkc_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                pka_sample = self.causal_link_pkc_pka(torch.tensor(pkc_sample, dtype=torch.float))\n",
    "                raf_sample = self.causal_link_pkc_pka_raf(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), pka_sample), dim=1))\n",
    "                mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
    "                erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 1:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                raf_sample = self.causal_link_pkc_pka_raf(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), pka_sample), dim=1))\n",
    "                mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
    "                erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 2:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
    "                erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 3:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                mek_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 4:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                mek_sample = torch.tensor(inp[:,3].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                erk_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "\n",
    "            elif inde == 5:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                mek_sample = torch.tensor(inp[:,3].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                erk_sample = torch.tensor(inp[:,4].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                jnk_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 6:\n",
    "                pkc_sample = torch.tensor(inp[:,0].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                pka_sample = torch.tensor(inp[:,1].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                raf_sample = torch.tensor(inp[:,2].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                mek_sample = torch.tensor(inp[:,3].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                erk_sample = torch.tensor(inp[:,4].reshape(self.sample_size,-1), dtype=torch.float)\n",
    "                jnk_sample = torch.tensor(inp[:,5].reshape(self.sample_size,-1))\n",
    "                p38_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                inp = torch.cat((pkc_sample, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample),dim=1)\n",
    "                return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20805d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to load data from csv file\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe1, dataframe2):\n",
    "        self.data_points = dataframe1\n",
    "        self.targets = dataframe2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_point = torch.tensor(np.array(self.data_points[idx]), dtype=torch.float)\n",
    "        target_point = torch.tensor(np.array(self.targets[idx]), dtype=torch.long)\n",
    "        return input_point, target_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef8209",
   "metadata": {},
   "source": [
    "# ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aaaef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 78.5999984741211\n",
      "test accuracy: 80.4000015258789\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.5\n",
      "test accuracy: 80.5\n",
      "\n",
      "15 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 80.5999984741211\n",
      "\n",
      "20 5\n",
      "validation accuracy: 78.9000015258789\n",
      "test accuracy: 81.0999984741211\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "10 5\n",
      "validation accuracy: 73.69999694824219\n",
      "test accuracy: 73.30000305175781\n",
      "\n",
      "15 5\n",
      "validation accuracy: 76.80000305175781\n",
      "test accuracy: 78.69999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 77.19999694824219\n",
      "test accuracy: 78.9000015258789\n",
      "\n",
      "25 5\n",
      "validation accuracy: 77.9000015258789\n",
      "test accuracy: 80.19999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "35 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "40 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "45 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.19999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 68.80000305175781\n",
      "test accuracy: 68.0\n",
      "\n",
      "10 5\n",
      "validation accuracy: 77.4000015258789\n",
      "test accuracy: 79.80000305175781\n",
      "\n",
      "15 5\n",
      "validation accuracy: 78.4000015258789\n",
      "test accuracy: 81.0\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.0\n",
      "test accuracy: 81.19999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.0\n",
      "test accuracy: 81.5\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 78.5999984741211\n",
      "test accuracy: 80.30000305175781\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "15 5\n",
      "validation accuracy: 78.80000305175781\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 72.0999984741211\n",
      "test accuracy: 73.0\n",
      "\n",
      "10 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    erm_model = Model(7,sample_size=len(data))\n",
    "    optimizer = optim.Adam([{'params': erm_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "\n",
    "    for ep in range(0,epoch): \n",
    "        trainval = DataSet(X_train,y_train)\n",
    "        train_loader = DataLoader(trainval, batch_size=batch_size)\n",
    "        for input_data, target in train_loader:\n",
    "            erm_model.zero_grad()\n",
    "            output = erm_model(input_data)\n",
    "            loss = loss_func(output,target.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if ep%interval == 0:\n",
    "            print(ep, interval)\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=1)\n",
    "            acc_val = 0\n",
    "            acc_test = 0\n",
    "            \n",
    "            for input_data, target in val_loader:\n",
    "                output = erm_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_val += acc\n",
    "\n",
    "            print ('validation accuracy:', float(acc_val/len(val_loader)))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            \n",
    "            for input_data, target in test_loader:\n",
    "                output = erm_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_test += acc\n",
    "                \n",
    "            print('test accuracy:', float(acc_test/len(test_loader)))\n",
    "            print()\n",
    "    print(\"************\")\n",
    "    torch.save(erm_model, \"models/erm_sachs_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab705dfc",
   "metadata": {},
   "source": [
    "# Causal Attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6803a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=3\n",
    "num_feat=7\n",
    "num_alpha=3\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_ca_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_ca_total = []\n",
    "    model = torch.load(\"models/erm_sachs_\"+str(ensemble+1))\n",
    "    for output_index in range(0,1):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_ca_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "    aces_ca_total.append(ace_ca_total)\n",
    "np.save('./aces/sachs_ca_total.npy',aces_ca_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1b4766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.10176131 2.19258358 0.11545693 0.21651297 2.23200216 0.0742025\n",
      " 0.09842272]\n",
      "rmse std:  [0.09715836 0.9016019  0.05735296 0.13859572 0.63006274 0.0440616\n",
      " 0.06976197]\n",
      "rmse all features mean:  0.7187060241403819\n",
      "rmse all features std:  0.27694218134075055\n",
      "frechet mean:  [0.13223125 2.97188839 0.16011002 0.27970836 3.12497292 0.09462399\n",
      " 0.12896724]\n",
      "frechet std:  [0.12704554 1.14069842 0.08258818 0.18027261 0.90048447 0.05460573\n",
      " 0.09408247]\n",
      "frechet all features mean:  0.9846431686074393\n",
      "frechet all features std:  0.36853963237833876\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_ca_total[ensemble][0]),\n",
    "                         rmse(aces_gt[1], aces_ca_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_ca_total[ensemble][2]),\n",
    "                         rmse(aces_gt[3], aces_ca_total[ensemble][3]),\n",
    "                         rmse(aces_gt[4], aces_ca_total[ensemble][4]),\n",
    "                         rmse(aces_gt[5], aces_ca_total[ensemble][5]),\n",
    "                         rmse(aces_gt[6], aces_ca_total[ensemble][6])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(3,1), aces_ca_total[ensemble][0].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(3,1), aces_ca_total[ensemble][1].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(3,1), aces_ca_total[ensemble][2].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(3,1), aces_ca_total[ensemble][3].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(3,1), aces_ca_total[ensemble][4].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[5].reshape(3,1), aces_ca_total[ensemble][5].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[6].reshape(3,1), aces_ca_total[ensemble][6].reshape(3,1))])\n",
    "    \n",
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471a2a7",
   "metadata": {},
   "source": [
    "# CREDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e73c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = {0:(lambda ii:ii), 1:(lambda ii:ii), 2:(lambda ii:ii),\n",
    "        3:(lambda ii:ii), 4:(lambda ii:ii), 5:(lambda ii:0*ii),\n",
    "        6:(lambda ii:0*ii)}\n",
    "\n",
    "def get_grad(x, prior):\n",
    "    a = x.clone().detach().requires_grad_(True)\n",
    "    for f in prior.keys():\n",
    "        z = prior[f]\n",
    "        z = torch.sum(z(a[0][f]), dim=0)\n",
    "        z.backward()\n",
    "    return a.grad\n",
    "\n",
    "def get_grads_to_match(ip, prior):\n",
    "    return get_grad(ip, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d41799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 78.5\n",
      "test accuracy: 80.0\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.5\n",
      "test accuracy: 80.9000015258789\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.5\n",
      "test accuracy: 81.19999694824219\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 60.79999923706055\n",
      "test accuracy: 59.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 69.30000305175781\n",
      "test accuracy: 69.0\n",
      "\n",
      "10 5\n",
      "validation accuracy: 78.69999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 30.700000762939453\n",
      "test accuracy: 31.700000762939453\n",
      "\n",
      "5 5\n",
      "validation accuracy: 68.5999984741211\n",
      "test accuracy: 67.5999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.5\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.19999694824219\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.0\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.0\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n",
      "0 5\n",
      "validation accuracy: 30.700000762939453\n",
      "test accuracy: 31.700000762939453\n",
      "\n",
      "5 5\n",
      "validation accuracy: 77.19999694824219\n",
      "test accuracy: 79.80000305175781\n",
      "\n",
      "10 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.5999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 79.30000305175781\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 79.0999984741211\n",
      "test accuracy: 81.69999694824219\n",
      "\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    credo_model = Model(7,sample_size=len(data))\n",
    "    optimizer = optim.Adam([{'params': credo_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "\n",
    "    for ep in range(0,epoch): \n",
    "        trainval = DataSet(X_train,y_train)\n",
    "        train_loader = DataLoader(trainval, batch_size=batch_size)\n",
    "        for input_data, target in train_loader:\n",
    "            credo_model.zero_grad()\n",
    "            input_data.requires_grad=True\n",
    "            output = credo_model(input_data)\n",
    "            \n",
    "            calc_grads = (autograd.grad(torch.sum(output[0], dim=0), input_data, retain_graph=True, create_graph=True)[0])\n",
    "            grads_to_match = get_grads_to_match(input_data, prior) \n",
    "            hinge_input = torch.abs(grads_to_match - calc_grads)\n",
    "            loss = loss_func(output,target.squeeze()) + 0.01 * torch.norm(torch.clamp(hinge_input, min=0), p=1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if ep%interval == 0:\n",
    "            print(ep, interval)\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=1)\n",
    "            acc_val = 0\n",
    "            acc_test = 0\n",
    "            \n",
    "            for input_data, target in val_loader:\n",
    "                output = credo_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_val += acc\n",
    "\n",
    "            print ('validation accuracy:', float(acc_val/len(val_loader)))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            \n",
    "            for input_data, target in test_loader:\n",
    "                output = credo_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_test += acc\n",
    "                \n",
    "            print('test accuracy:', float(acc_test/len(test_loader)))\n",
    "            print()\n",
    "    print(\"************\")\n",
    "    torch.save(credo_model, \"models/credo_sachs_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa05775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=3\n",
    "num_feat=7\n",
    "num_alpha=3\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_credo_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_credo_total = []\n",
    "    model = torch.load(\"models/credo_sachs_\"+str(ensemble+1))\n",
    "    for output_index in range(0,1):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_credo_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "    aces_credo_total.append(ace_ca_total)\n",
    "np.save('./aces/sachs_credo_total.npy',aces_credo_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89a1e1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.08285197 3.81488683 0.02983257 0.42860937 2.87066199 0.13646983\n",
      " 0.04895687]\n",
      "rmse std:  [0.00000000e+00 4.44089210e-16 0.00000000e+00 5.55111512e-17\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "rmse all features mean:  1.0588956327825614\n",
      "rmse all features std:  7.137148015447435e-17\n",
      "frechet mean:  [0.11701838 5.02455987 0.03905235 0.56964442 4.04197361 0.1671408\n",
      " 0.0688235 ]\n",
      "frechet std:  [0. 0. 0. 0. 0. 0. 0.]\n",
      "frechet all features mean:  1.4326018477303641\n",
      "frechet all features std:  0.0\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_credo_total[ensemble][0]),\n",
    "                         rmse(aces_gt[1], aces_credo_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_credo_total[ensemble][2]),\n",
    "                         rmse(aces_gt[3], aces_credo_total[ensemble][3]),\n",
    "                         rmse(aces_gt[4], aces_credo_total[ensemble][4]),\n",
    "                         rmse(aces_gt[5], aces_credo_total[ensemble][5]),\n",
    "                         rmse(aces_gt[6], aces_credo_total[ensemble][6])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(3,1), aces_credo_total[ensemble][0].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(3,1), aces_credo_total[ensemble][1].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(3,1), aces_credo_total[ensemble][2].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(3,1), aces_credo_total[ensemble][3].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(3,1), aces_credo_total[ensemble][4].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[5].reshape(3,1), aces_credo_total[ensemble][5].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[6].reshape(3,1), aces_credo_total[ensemble][6].reshape(3,1))])\n",
    "    \n",
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958d8e0",
   "metadata": {},
   "source": [
    "# AHCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffb8cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 81.5999984741211\n",
      "test accuracy: 80.3499984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 82.19999694824219\n",
      "test accuracy: 81.25\n",
      "\n",
      "15 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "0 5\n",
      "validation accuracy: 9.300000190734863\n",
      "test accuracy: 7.25\n",
      "\n",
      "5 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "10 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "15 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "20 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "25 5\n",
      "validation accuracy: 80.80000305175781\n",
      "test accuracy: 79.5999984741211\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.4000015258789\n",
      "test accuracy: 81.0999984741211\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5\n",
      "test accuracy: 81.3499984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.4000015258789\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.19999694824219\n",
      "test accuracy: 81.25\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.30000305175781\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.4000015258789\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5\n",
      "test accuracy: 81.30000305175781\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "0 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 73.5\n",
      "test accuracy: 72.8499984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 82.4000015258789\n",
      "test accuracy: 80.94999694824219\n",
      "\n",
      "15 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.3499984741211\n",
      "\n",
      "20 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "25 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.4000015258789\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.3499984741211\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.3499984741211\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "0 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 71.9000015258789\n",
      "test accuracy: 73.55000305175781\n",
      "\n",
      "10 5\n",
      "validation accuracy: 82.30000305175781\n",
      "test accuracy: 80.80000305175781\n",
      "\n",
      "15 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "25 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.5\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "0 5\n",
      "validation accuracy: 60.20000076293945\n",
      "test accuracy: 61.599998474121094\n",
      "\n",
      "5 5\n",
      "validation accuracy: 66.5\n",
      "test accuracy: 68.9000015258789\n",
      "\n",
      "10 5\n",
      "validation accuracy: 82.30000305175781\n",
      "test accuracy: 80.75\n",
      "\n",
      "15 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "20 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "25 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "30 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "35 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "40 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "45 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "50 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "55 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "60 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "65 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "70 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "75 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "80 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "85 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "90 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n",
      "95 5\n",
      "validation accuracy: 82.5999984741211\n",
      "test accuracy: 81.44999694824219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    # Interval / Epochs\n",
    "    \n",
    "    mse_loss_func = nn.MSELoss()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    ahce_model = Model(7,sample_size=len(data))\n",
    "\n",
    "    optimizer = optim.Adam([{'params': ahce_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "\n",
    "    for ep in range(0,100):\n",
    "        trainval = DataSet(X_train,y_train)\n",
    "        train_loader = DataLoader(trainval, batch_size=64)\n",
    "        for input_data, target in train_loader: \n",
    "            for phase in ['train_dag', 'freeze']:\n",
    "                ahce_model.zero_grad()\n",
    "                if phase == 'freeze':\n",
    "                    output = ahce_model(input_data)\n",
    "                    loss = loss_func(output,target.squeeze())\n",
    "                    loss.backward(retain_graph=True)\n",
    "\n",
    "                else:\n",
    "                    output, pka_sample, raf_sample, mek_sample, erk_sample, jnk_sample, p38_sample  = ahce_model(input_data, phase='train_dag')             \n",
    "                    \n",
    "                    loss = 0.01*loss_func(output,target.squeeze()) \n",
    "                    loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "\n",
    "        if ep%interval == 0:\n",
    "            print(ep, interval)\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=1)\n",
    "            acc_val = 0\n",
    "            acc_test = 0\n",
    "\n",
    "            for input_data, target in val_loader:\n",
    "                output = ahce_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_val += acc\n",
    "\n",
    "            print ('validation accuracy:', float(acc_val/len(val_loader)))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "\n",
    "            for input_data, target in test_loader:\n",
    "                output = ahce_model(input_data)\n",
    "                acc = multi_acc(output, target.unsqueeze(1))\n",
    "                acc_test += acc\n",
    "\n",
    "            print('test accuracy:', float(acc_test/len(test_loader)))\n",
    "            print()\n",
    "\n",
    "    torch.save(ahce_model, \"./models/ahce_sachs_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73423e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1986/930309753.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pka_sample = self.causal_link_pkc_pka(torch.tensor(pkc_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_1986/930309753.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  raf_sample = self.causal_link_pkc_pka_raf(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), pka_sample), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  raf_sample = self.causal_link_pkc_pka_raf(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), pka_sample), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mek_sample = self.causal_link_pkc_pka_raf_mek(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float), torch.tensor(raf_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  erk_sample = self.causal_link_mek_pka_erk(torch.cat((torch.tensor(mek_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  jnk_sample = self.causal_link_pkc_pka_jnk(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_1986/930309753.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p38_sample = self.causal_link_pkc_pka_p38(torch.cat((torch.tensor(pkc_sample, dtype=torch.float), torch.tensor(pka_sample, dtype=torch.float)), dim=1))\n"
     ]
    }
   ],
   "source": [
    "n_classes=3\n",
    "num_c=7#no. of features\n",
    "num_alpha=3\n",
    "\n",
    "aces_ahce_total = []\n",
    "for ensemble in range(5):\n",
    "    ace_ahce_total = []\n",
    "    model =  torch.load(\"./models/ahce_sachs_\"+str(ensemble+1))\n",
    "    for output_index in range(1,2):#For every class\n",
    "        #plt.figure()\n",
    "        for t in range(0,num_c):#For every feature\n",
    "            expectation_do_x = []\n",
    "            for x in [0,0.5,1]:\n",
    "                X_values[:,t] = x\n",
    "                sample_data = model(X_values, phase='sample', inde=t, alpha=x).detach().numpy()\n",
    "                cov = np.cov(sample_data, rowvar=False)\n",
    "                means = np.mean(sample_data, axis=0)\n",
    "                cov=np.array(cov)\n",
    "                mean_vector = np.array(means)\n",
    "                inp=copy.deepcopy(mean_vector)\n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "\n",
    "                output=model(input_torchvar)\n",
    "\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_c):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_c)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "\n",
    "            ace_ahce_total.append(np.array(expectation_do_x) - np.mean(np.array(expectation_do_x)))\n",
    "\n",
    "    aces_ahce_total.append(ace_ahce_total)\n",
    "np.save('./aces/sachs_ahce_total.npy',aces_ahce_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4bb63dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.12475822 0.65547383 0.12277304 0.14353915 0.51898169 0.0186962\n",
      " 0.02440762]\n",
      "rmse std:  [0.06767317 0.1756164  0.03766031 0.01556702 0.34269087 0.01361813\n",
      " 0.01961943]\n",
      "rmse all features mean:  0.2298042509487662\n",
      "rmse all features std:  0.0960636188516346\n",
      "frechet mean:  [0.171739   0.91099781 0.1714031  0.17648882 0.70143739 0.02320759\n",
      " 0.02991145]\n",
      "frechet std:  [0.09771076 0.23724553 0.05571379 0.01884757 0.45834208 0.01672321\n",
      " 0.0240606 ]\n",
      "frechet all features mean:  0.3121693101549147\n",
      "frechet all features std:  0.12980622024131902\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in [0,1,2,3,4]:\n",
    "    rmse_results.append([rmse(aces_gt[0], aces_ahce_total[ensemble][0]),\n",
    "                         rmse(aces_gt[1], aces_ahce_total[ensemble][1]),\n",
    "                         rmse(aces_gt[2], aces_ahce_total[ensemble][2]),\n",
    "                         rmse(aces_gt[3], aces_ahce_total[ensemble][3]),\n",
    "                         rmse(aces_gt[4], aces_ahce_total[ensemble][4]),\n",
    "                         rmse(aces_gt[5], aces_ahce_total[ensemble][5]),\n",
    "                         rmse(aces_gt[6], aces_ahce_total[ensemble][6])])\n",
    "    \n",
    "    frechet_results.append([frechet_dist(aces_gt[0].reshape(3,1), aces_ahce_total[ensemble][0].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[1].reshape(3,1), aces_ahce_total[ensemble][1].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[2].reshape(3,1), aces_ahce_total[ensemble][2].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[3].reshape(3,1), aces_ahce_total[ensemble][3].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[4].reshape(3,1), aces_ahce_total[ensemble][4].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[5].reshape(3,1), aces_ahce_total[ensemble][5].reshape(3,1)),\n",
    "                            frechet_dist(aces_gt[6].reshape(3,1), aces_ahce_total[ensemble][6].reshape(3,1))])\n",
    "    \n",
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))\n",
    "\n",
    "print(\"frechet mean: \", np.mean(frechet_results, axis=0))\n",
    "print(\"frechet std: \", np.std(frechet_results, axis=0))\n",
    "print(\"frechet all features mean: \", np.mean(np.mean(frechet_results, axis=0)))\n",
    "print(\"frechet all features std: \", np.mean(np.std(frechet_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08444249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
