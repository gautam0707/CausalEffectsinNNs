{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1bb3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os,csv,math,sys, joblib\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import collections\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.model_selection, sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "#import pydot\n",
    "from similaritymeasures import frechet_dist\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib\n",
    "seed = 99 # To reproduce the results\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c27ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"lungcancer_10000.csv\")\n",
    "dataset.drop(dataset.columns[[0]], axis=1, inplace=True)\n",
    "cols = dataset.columns\n",
    "for i in cols:\n",
    "    dataset[i] = dataset[i].map({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d2fc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACE for asia on dysp is 0.0\n",
      "ACE for tub on dysp is 1.0\n",
      "ACE for smoke on dysp is 1.0\n",
      "ACE for lung on dysp is 0.4911\n",
      "ACE for bronc on dysp is 0.9357\n",
      "ACE for either on dysp is 0.5467\n",
      "ACE for xray on dysp is 0.0\n"
     ]
    }
   ],
   "source": [
    "gt_aces = []\n",
    "\n",
    "#ACE of asia on dysp\n",
    "inp = np.array(dataset['asia']).reshape(-1,1)\n",
    "out = dataset['dysp']\n",
    "clf = LogisticRegression().fit(inp, out)\n",
    "do_value = np.ones([len(inp), 1])\n",
    "base_value = np.zeros([len(inp), 1])\n",
    "gt_aces.append(np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "print('ACE for asia on dysp is', np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "\n",
    "\n",
    "#ACE of Tub on dysp\n",
    "inp = np.array(dataset['tub']).reshape(-1,1)\n",
    "clf = LogisticRegression().fit(inp, out)\n",
    "do_value = np.ones([len(inp), 1])\n",
    "base_value = np.zeros([len(inp), 1])\n",
    "gt_aces.append(np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "print('ACE for tub on dysp is', np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "\n",
    "\n",
    "#ACE of Smoke on dysp\n",
    "inp = np.array(dataset['smoke']).reshape(-1,1)\n",
    "clf = LogisticRegression().fit(inp, out)\n",
    "do_value = np.ones([len(inp), 1])\n",
    "base_value = np.zeros([len(inp), 1])\n",
    "gt_aces.append(np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "print('ACE for smoke on dysp is', np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "\n",
    "\n",
    "#ACE of Lungcancer on dysp\n",
    "inp = np.array(dataset[['lung','smoke']]).reshape(-1,2)\n",
    "clf = LogisticRegression().fit(inp, out)\n",
    "do_value = pd.DataFrame.copy(dataset[['lung', 'smoke']])\n",
    "do_value['lung'] = 1\n",
    "do_value = np.array(do_value).reshape(-1,2)\n",
    "base_value = pd.DataFrame.copy(dataset[['lung', 'smoke']])\n",
    "base_value['lung'] = 0\n",
    "base_value = np.array(base_value).reshape(-1,2)\n",
    "gt_aces.append(np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "print('ACE for lung on dysp is', np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "\n",
    "\n",
    "#ACE of Bronc on dysp\n",
    "inp = np.array(dataset[['bronc','smoke','lung', 'either']]).reshape(-1,4)\n",
    "clf = LogisticRegression().fit(inp, out)\n",
    "do_value = pd.DataFrame.copy(dataset[['bronc', 'smoke', 'lung', 'either']])\n",
    "do_value['bronc'] = 1\n",
    "do_value = np.array(do_value).reshape(-1,4)\n",
    "base_value = pd.DataFrame.copy(dataset[['bronc', 'smoke', 'lung', 'either']])\n",
    "base_value['bronc'] = 0\n",
    "base_value = np.array(base_value).reshape(-1,4)\n",
    "gt_aces.append(np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "print('ACE for bronc on dysp is', np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "\n",
    "\n",
    "#ACE of Either on dysp\n",
    "inp = np.array(dataset[['either', 'lung', 'bronc', 'smoke']]).reshape(-1,4)\n",
    "clf = LogisticRegression().fit(inp, out)\n",
    "do_value = pd.DataFrame.copy(dataset[['either', 'lung', 'bronc', 'smoke']])\n",
    "do_value['either'] = 1\n",
    "do_value = np.array(do_value).reshape(-1,4)\n",
    "base_value = pd.DataFrame.copy(dataset[['either', 'lung', 'bronc', 'smoke']])\n",
    "base_value['either'] = 0\n",
    "base_value = np.array(base_value).reshape(-1,4)\n",
    "gt_aces.append(np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "print('ACE for either on dysp is', np.mean(clf.predict(do_value)) - np.mean(clf.predict(base_value)))\n",
    "\n",
    "\n",
    "#ACE of Xray on dysp\n",
    "gt_aces.append(np.mean([0.0]))\n",
    "print('ACE for xray on dysp is 0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe731ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./aces/aces_gt.npy',gt_aces,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc619671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class samp_network(nn.Module):\n",
    "    def __init__(self, input_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.fc1 = nn.Linear(self.input_size, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9850b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#order of inputs: asia\ttub\tsmoke\tlung\tbronc\teither\txray\tdysp\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, feature_dim, batch_size=64, device='cpu',sample_size=50000):\n",
    "        super(Model, self).__init__()\n",
    "        self.batchsize=batch_size\n",
    "\n",
    "        self.causal_link_asia_tub = samp_network(input_size=1)\n",
    "        self.causal_link_either_xray = samp_network(input_size=1)\n",
    "        self.causal_link_smoke_bronc = samp_network(input_size=1)\n",
    "        self.causal_link_smoke_lung = samp_network(input_size=1)\n",
    "        self.causal_link_tub_lung_either = samp_network(input_size=2)\n",
    "\n",
    "        self.first_layer = nn.Linear(7,4)\n",
    "        self.second_layer = nn.Linear(4,2)\n",
    "        self.third_layer = nn.Linear(2,2)\n",
    "        self.regression_layer = nn.Linear(2, 1)\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def forward(self, inp, phase='freeze', inde=0, alpha=0):\n",
    "        if phase=='freeze':\n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "            return prediction\n",
    "\n",
    "        elif phase=='train_dag':\n",
    "            asia_sample = inp[:,0].reshape(-1,1)\n",
    "            smoke_sample = inp[:,2].reshape(-1,1)\n",
    "            tub_sample = self.causal_link_asia_tub(asia_sample)\n",
    "            lung_sample = self.causal_link_smoke_lung(smoke_sample)\n",
    "            bronc_sample = self.causal_link_smoke_bronc(smoke_sample)\n",
    "            either_sample = self.causal_link_tub_lung_either(torch.cat((tub_sample, lung_sample), dim=1))\n",
    "            xray_sample = self.causal_link_either_xray(either_sample)\n",
    "            inp = torch.cat((asia_sample, tub_sample, smoke_sample, lung_sample, bronc_sample, either_sample, xray_sample),dim=1)\n",
    "            x = F.relu(self.first_layer(inp))\n",
    "            x = F.relu(self.second_layer(x))\n",
    "            x = F.relu(self.third_layer(x))\n",
    "            prediction = self.regression_layer(x)\n",
    "            return prediction, tub_sample, lung_sample, bronc_sample, either_sample, xray_sample \n",
    "\n",
    "        elif phase=='sample':\n",
    "            if inde == 0:\n",
    "                asia_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                smoke_sample = inp[:,2].reshape(self.sample_size,-1)\n",
    "                tub_sample = self.causal_link_asia_tub(torch.tensor(asia_sample, dtype=torch.float))\n",
    "                lung_sample = self.causal_link_smoke_lung(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                bronc_sample = self.causal_link_smoke_bronc(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
    "                xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
    "                inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 1:\n",
    "                asia_sample = inp[:,0].reshape(self.sample_size,-1)\n",
    "                smoke_sample = inp[:,2].reshape(self.sample_size,-1)\n",
    "                tub_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                lung_sample = self.causal_link_smoke_lung(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                bronc_sample = self.causal_link_smoke_bronc(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
    "                xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
    "                inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 2:\n",
    "                asia_sample = inp[:,0].reshape(self.sample_size,-1)\n",
    "                smoke_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                tub_sample = self.causal_link_asia_tub(torch.tensor(asia_sample, dtype=torch.float))\n",
    "                lung_sample = self.causal_link_smoke_lung(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                bronc_sample = self.causal_link_smoke_bronc(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
    "                xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
    "                inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 3:\n",
    "                asia_sample = inp[:,0].reshape(self.sample_size,-1)\n",
    "                smoke_sample = inp[:,2].reshape(self.sample_size,-1)\n",
    "                tub_sample = self.causal_link_asia_tub(torch.tensor(asia_sample, dtype=torch.float))\n",
    "                lung_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                bronc_sample = self.causal_link_smoke_bronc(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
    "                xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
    "                inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 4:\n",
    "                asia_sample = inp[:,0].reshape(self.sample_size,-1)\n",
    "                smoke_sample = inp[:,2].reshape(self.sample_size,-1)\n",
    "                tub_sample = self.causal_link_asia_tub(torch.tensor(asia_sample, dtype=torch.float))\n",
    "                lung_sample = self.causal_link_smoke_lung(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                bronc_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
    "                xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
    "                inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 5:\n",
    "                asia_sample = inp[:,0].reshape(self.sample_size,-1)\n",
    "                smoke_sample = inp[:,2].reshape(self.sample_size,-1)\n",
    "                tub_sample = self.causal_link_asia_tub(torch.tensor(asia_sample, dtype=torch.float))\n",
    "                lung_sample = self.causal_link_smoke_lung(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                bronc_sample = self.causal_link_smoke_bronc(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                either_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
    "                inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
    "                return inp\n",
    "\n",
    "            elif inde == 6:\n",
    "                asia_sample = inp[:,0].reshape(self.sample_size,-1)\n",
    "                smoke_sample = inp[:,2].reshape(self.sample_size,-1)\n",
    "                tub_sample = self.causal_link_asia_tub(torch.tensor(asia_sample, dtype=torch.float))\n",
    "                lung_sample = self.causal_link_smoke_lung(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                bronc_sample = self.causal_link_smoke_bronc(torch.tensor(smoke_sample, dtype=torch.float))\n",
    "                either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
    "                xray_sample = torch.tensor([alpha]*self.sample_size, dtype=torch.float).reshape(self.sample_size,-1)\n",
    "                inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
    "                return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71380676",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size=5 # to get mean and std values\n",
    "batch_size = 16\n",
    "\n",
    "values = list(dataset.columns.values)\n",
    "y = dataset[values[-1:]]\n",
    "y = np.array(y, dtype='float32')\n",
    "X = dataset[values[:-1]]\n",
    "X = np.array(X, dtype='float32')\n",
    "\n",
    "indices = np.random.choice(len(X), len(X), replace=False)\n",
    "X_values = X[indices]\n",
    "y_values = y[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 2000\n",
    "val_size = 1000\n",
    "\n",
    "interval = 10\n",
    "epoch = 50\n",
    "\n",
    "X_test = X_values[-test_size:]\n",
    "X_trainval = X_values[:-test_size]\n",
    "X_val = X_trainval[-val_size:]\n",
    "X_train = X_trainval[:-val_size]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "y_test = y_values[-test_size:]\n",
    "y_trainval = y_values[:-test_size]\n",
    "y_val = y_trainval[-val_size:]\n",
    "y_train = y_trainval[:-val_size]\n",
    "\n",
    "def binary_acc(pred, true):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == true).sum().float()\n",
    "    acc = correct_results_sum/true.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13fe1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to load data from csv file\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe1, dataframe2):\n",
    "        self.data_points = dataframe1\n",
    "        self.targets = dataframe2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_point = torch.tensor(np.array(self.data_points[idx]), dtype=torch.float)\n",
    "        target_point = torch.tensor(np.array(self.targets[idx]), dtype=torch.float)\n",
    "        return input_point, target_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43f93f",
   "metadata": {},
   "source": [
    "# ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "595e3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "\n",
    "    # Interval / Epochs\n",
    "    \n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    erm_model = Model(7,sample_size=len(dataset))\n",
    "    optimizer = optim.Adam([{'params': erm_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "\n",
    "    for ep in range(0,epoch): \n",
    "        trainval = DataSet(X_train,y_train)\n",
    "        train_loader = DataLoader(trainval, batch_size=batch_size)\n",
    "        for input_data, target in train_loader:\n",
    "            erm_model.zero_grad()\n",
    "            output = erm_model(input_data)\n",
    "            loss = loss_func(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if ep%interval == 0:\n",
    "            print(ep, interval)\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=1)\n",
    "            acc_val = 0\n",
    "            acc_test = 0\n",
    "            \n",
    "            for input_data, target in val_loader:\n",
    "                output = erm_model(input_data)\n",
    "                acc = binary_acc(output, target.unsqueeze(1))\n",
    "                acc_val += acc\n",
    "\n",
    "            print ('validation accuracy:', float(acc_val/len(val_loader)))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            \n",
    "            for input_data, target in test_loader:\n",
    "                output = erm_model(input_data)\n",
    "                acc = binary_acc(output, target.unsqueeze(1))\n",
    "                acc_test += acc\n",
    "                \n",
    "            print('test accuracy:', float(acc_test/len(test_loader)))\n",
    "            print()\n",
    "    print(\"************\")\n",
    "    torch.save(erm_model, \"models/erm_lungcancer_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd2c306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 85.0999984741211\n",
      "test accuracy: 85.0999984741211\n",
      "test accuracy: 85.0999984741211\n",
      "test accuracy: 85.0999984741211\n",
      "test accuracy: 85.0999984741211\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    erm_model = torch.load(\"models/erm_lungcancer_\"+str(ensemble+1))\n",
    "    testval = DataSet(X_test,y_test)\n",
    "    test_loader = DataLoader(testval, batch_size=1)\n",
    "    acc_test = 0\n",
    "    for input_data, target in test_loader:\n",
    "        output = erm_model(input_data)\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        acc_test += acc\n",
    "\n",
    "    print('test accuracy:', float(acc_test/len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559790a",
   "metadata": {},
   "source": [
    "# Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b80890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "frechet_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    model = torch.load(\"models/erm_lungcancer_\"+str(ensemble+1))\n",
    "    \n",
    "    data_a = dataset['asia'].values[:1000]\n",
    "    data_t = dataset['tub'].values[:1000]\n",
    "    data_s = dataset['smoke'].values[:1000]\n",
    "    data_l = dataset['lung'].values[:1000]\n",
    "    data_b = dataset['bronc'].values[:1000]\n",
    "    data_e = dataset['either'].values[:1000]\n",
    "    data_x = dataset['xray'].values[:1000]\n",
    "    \n",
    "    do_a = np.linspace(min(dataset['asia']), max(dataset['asia']), 1000)\n",
    "    do_t = np.linspace(min(dataset['tub']), max(dataset['tub']), 1000)\n",
    "    do_s = np.linspace(min(dataset['smoke']), max(dataset['smoke']), 1000)\n",
    "    do_l = np.linspace(min(dataset['lung']), max(dataset['lung']), 1000)\n",
    "    do_b = np.linspace(min(dataset['bronc']), max(dataset['bronc']), 1000)\n",
    "    do_e = np.linspace(min(dataset['either']), max(dataset['either']), 1000)\n",
    "    do_x = np.linspace(min(dataset['xray']), max(dataset['xray']), 1000)\n",
    "    \n",
    "    \n",
    "    test_array_a = np.stack((do_a, data_t, data_s, data_l, data_b, data_e, data_x), axis=1)\n",
    "    test_array_a = torch.from_numpy(test_array_a).type(torch.FloatTensor)\n",
    "\n",
    "    test_array_t = np.stack((data_a, do_t, data_s, data_l, data_b, data_e, data_x), axis=1)\n",
    "    test_array_t = torch.from_numpy(test_array_t).type(torch.FloatTensor)\n",
    "    \n",
    "    test_array_s = np.stack((data_a, data_t, do_s, data_l, data_b, data_e, data_x), axis=1)\n",
    "    test_array_s = torch.from_numpy(test_array_s).type(torch.FloatTensor)\n",
    "    \n",
    "    test_array_l = np.stack((data_a, data_t, data_s, do_l, data_b, data_e, data_x), axis=1)\n",
    "    test_array_l = torch.from_numpy(test_array_l).type(torch.FloatTensor)\n",
    "    \n",
    "    test_array_b = np.stack((data_a, data_t, data_s, data_l, do_b, data_e, data_x), axis=1)\n",
    "    test_array_b = torch.from_numpy(test_array_b).type(torch.FloatTensor)\n",
    "    \n",
    "    test_array_e = np.stack((data_a, data_t, data_s, data_l, data_b, do_e, data_x), axis=1)\n",
    "    test_array_e = torch.from_numpy(test_array_e).type(torch.FloatTensor)\n",
    "    \n",
    "    test_array_x = np.stack((data_a, data_t, data_s, data_l, data_b, data_e, do_x), axis=1)\n",
    "    test_array_x = torch.from_numpy(test_array_x).type(torch.FloatTensor)\n",
    "    \n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    ig_attr_test_a, delta = ig.attribute(test_array_a, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_t, delta = ig.attribute(test_array_t, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_s, delta = ig.attribute(test_array_s, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_l, delta = ig.attribute(test_array_l, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_b, delta = ig.attribute(test_array_b, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_e, delta = ig.attribute(test_array_e, n_steps=50, return_convergence_delta=True)\n",
    "    ig_attr_test_x, delta = ig.attribute(test_array_x, n_steps=50, return_convergence_delta=True)\n",
    "\n",
    "    print(ensemble)\n",
    "    rmse_results.append([rmse(gt_aces[0], np.array(ig_attr_test_a[:,0])),\n",
    "                         rmse(gt_aces[1], np.array(ig_attr_test_t[:,1])),\n",
    "                         rmse(gt_aces[2], np.array(ig_attr_test_s[:,2])),\n",
    "                         rmse(gt_aces[3], np.array(ig_attr_test_l[:,3])),\n",
    "                         rmse(gt_aces[4], np.array(ig_attr_test_b[:,4])),\n",
    "                         rmse(gt_aces[5], np.array(ig_attr_test_e[:,5])),\n",
    "                         rmse(gt_aces[6], np.array(ig_attr_test_x[:,6]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11cce55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.46005648 0.62237716 1.07428594 0.40877386 1.55406308 0.87778694\n",
      " 0.11141186]\n",
      "rmse std:  [0.05390945 0.04392149 0.07803147 0.07840053 0.1462524  0.18627058\n",
      " 0.05262743]\n",
      "rmse all features mean:  0.7298221888456907\n",
      "rmse all features std:  0.09134476362902243\n"
     ]
    }
   ],
   "source": [
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f0898",
   "metadata": {},
   "source": [
    "# Causal Attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b6ae9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=1\n",
    "num_feat=7\n",
    "num_alpha=2\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_ca_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_ca_total = []\n",
    "    model = torch.load(\"models/erm_lungcancer_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_ca_total.append(expectation_do_x[1] - expectation_do_x[0])\n",
    "    aces_ca_total.append(ace_ca_total)\n",
    "np.save('./aces/lungcancer_ca_total.npy',aces_ca_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f3d243d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.07152004 0.79382408 0.99577005 0.44299555 0.73886312 0.40130374\n",
      " 0.03270478]\n",
      "rmse std:  [0.11000619 0.29897767 0.0166511  0.05895573 0.24109224 0.19264384\n",
      " 0.04140146]\n",
      "rmse all features mean:  0.49671162346976144\n",
      "rmse all features std:  0.13710403401181395\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(gt_aces[0], aces_ca_total[ensemble][0]),\n",
    "                         rmse(gt_aces[1], aces_ca_total[ensemble][1]),\n",
    "                         rmse(gt_aces[2], aces_ca_total[ensemble][2]),\n",
    "                         rmse(gt_aces[3], aces_ca_total[ensemble][3]),\n",
    "                         rmse(gt_aces[4], aces_ca_total[ensemble][4]),\n",
    "                         rmse(gt_aces[5], aces_ca_total[ensemble][5]),\n",
    "                         rmse(gt_aces[6], aces_ca_total[ensemble][6])])\n",
    "    \n",
    "rmse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13838f0",
   "metadata": {},
   "source": [
    "# Causal Shapley Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd2b30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability is taken over indices of baseline only\n",
    "def get_probabiity(unique_count, x_hat, indices_baseline, n):\n",
    "    if len(indices_baseline) > 0:\n",
    "        count = 0\n",
    "        for i in unique_count:\n",
    "            check = True\n",
    "            key = np.asarray(i)\n",
    "            for j in indices_baseline:\n",
    "                check = check and key[j] == x_hat[j]\n",
    "            if check:\n",
    "                count += unique_count[i]\n",
    "        return count / n\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def conditional_prob(unique_count, x_hat, indices, indices_baseline, n):\n",
    "    numerator_indices = indices + indices_baseline\n",
    "    numerator = get_probabiity(unique_count, x_hat, numerator_indices, n)\n",
    "    denominator = get_probabiity(unique_count, x_hat, indices, n)\n",
    "    try:\n",
    "        kk = numerator / denominator\n",
    "    except ZeroDivisionError:\n",
    "        denominator = 1e-7\n",
    "        # pass\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def causal_prob(unique_count, x_hat, indices, indices_baseline, causal_struc, n):\n",
    "    p = 1\n",
    "    for i in indices_baseline:\n",
    "        intersect_s, intersect_s_hat = [], []\n",
    "        intersect_s_hat.append(i)\n",
    "        if len(causal_struc[str(i)]) > 0:\n",
    "            for index in causal_struc[str(i)]:\n",
    "                if index in indices or index in indices_baseline:\n",
    "                    intersect_s.append(index)\n",
    "            p *= conditional_prob(unique_count, x_hat, intersect_s, intersect_s_hat, n)\n",
    "        else:\n",
    "            p *= get_probabiity(unique_count, x_hat, intersect_s_hat, n)\n",
    "    return p\n",
    "\n",
    "\n",
    "def get_baseline(X, model):\n",
    "    fx = 0\n",
    "    n_features = X.shape[1]\n",
    "    X = np.reshape(X, (len(X), 1, n_features))\n",
    "    for i in X:\n",
    "        fx += torch.sigmoid(model(torch.tensor(i, dtype=torch.float)))\n",
    "    return fx / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fb75e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns value from using function for different versions\n",
    "def get_value(version, permutation, X, x, unique_count, causal_struct, model, N, is_classification, xi):\n",
    "    # intializing returns\n",
    "    absolute_diff, f1, f2 = 0, 0, 0\n",
    "    xi_index = permutation.index(xi)\n",
    "    indices = permutation[:xi_index + 1]\n",
    "    indices_baseline = permutation[xi_index + 1:]\n",
    "    x_hat = np.zeros(N)\n",
    "    x_hat_2 = np.zeros(N)\n",
    "    len_X = len(X)\n",
    "    for j in indices:\n",
    "        x_hat[j] = x[j]\n",
    "        x_hat_2[j] = x[j]\n",
    "    if version == '2' or version == '3' or version == '4':\n",
    "        proba1, proba2 = 0, 0\n",
    "        baseline_check_1, baseline_check_2 = [], []\n",
    "        f1, f2 = 0, 0\n",
    "        indices_baseline_2 = indices_baseline[:]\n",
    "        for i in unique_count:\n",
    "            X = np.asarray(i)\n",
    "            for j in indices_baseline:\n",
    "                x_hat[j] = X[j]\n",
    "                x_hat_2[j] = X[j]\n",
    "\n",
    "            # No repetition\n",
    "            # Eg if baseline_indices is null, it'll only run once as x_hat will stay the same over each iteration\n",
    "            if x_hat.tolist() not in baseline_check_1:\n",
    "                baseline_check_1.append(x_hat.tolist())\n",
    "                if version == '2':\n",
    "                    prob_x_hat = get_probabiity(unique_count, x_hat, indices_baseline, len_X)\n",
    "                elif version == '3':\n",
    "                    prob_x_hat = conditional_prob(unique_count, x_hat, indices, indices_baseline, len_X)\n",
    "                else:\n",
    "                    prob_x_hat = causal_prob(unique_count, x_hat, indices, indices_baseline, causal_struct, len_X)\n",
    "                proba1 += prob_x_hat\n",
    "                x_hat = np.reshape(x_hat, (1, N))\n",
    "                f1 = f1 + (torch.sigmoid(model(torch.tensor(x_hat, dtype=torch.float))) * prob_x_hat if is_classification else model.predict(\n",
    "                    x_hat) * prob_x_hat)\n",
    "\n",
    "            # xi index will be given to baseline for f2\n",
    "            x_hat_2[xi] = X[xi]\n",
    "            if xi not in indices_baseline_2:\n",
    "                indices_baseline_2.append(xi)\n",
    "\n",
    "            # No repetition\n",
    "            indices_2 = indices[:]\n",
    "            indices_2.remove(xi)\n",
    "            if x_hat_2.tolist() not in baseline_check_2:\n",
    "                baseline_check_2.append(x_hat_2.tolist())\n",
    "                if version == '2':\n",
    "                    prob_x_hat_2 = get_probabiity(unique_count, x_hat_2, indices_baseline_2, len_X)\n",
    "                elif version == '3':\n",
    "                    prob_x_hat_2 = conditional_prob(unique_count, x_hat_2, indices_2, indices_baseline_2, len_X)\n",
    "                else:\n",
    "                    prob_x_hat_2 = causal_prob(unique_count, x_hat_2, indices_2, indices_baseline_2, causal_struct,\n",
    "                                               len_X)\n",
    "                proba2 += prob_x_hat_2\n",
    "                x_hat_2 = np.reshape(x_hat_2, (1, N))\n",
    "                f2 = f2 + (torch.sigmoid(model(torch.tensor(x_hat, dtype=torch.float))) * prob_x_hat_2 if is_classification else model.predict(\n",
    "                    x_hat_2) * prob_x_hat_2)\n",
    "            x_hat = np.squeeze(x_hat)\n",
    "            x_hat_2 = np.squeeze(x_hat_2)\n",
    "        absolute_diff = abs(f1 - f2)\n",
    "    elif version == '1':\n",
    "        f1, f2 = 0, 0\n",
    "        for i in range(len(X)):\n",
    "            for j in indices_baseline:\n",
    "                x_hat[j] = X[i][j]\n",
    "                x_hat_2[j] = X[i][j]\n",
    "            x_hat = np.reshape(x_hat, (1, N))\n",
    "            f1 += torch.sigmoid(model(torch.tensor(x_hat, dtype=torch.float))) if is_classification else model.predict(x_hat)\n",
    "            x_hat_2[xi] = X[i][xi]\n",
    "            x_hat_2 = np.reshape(x_hat_2, (1, N))\n",
    "            f2 += torch.sigmoid(model(torch.tensor(x_hat, dtype=torch.float))) if is_classification else model.predict(x_hat_2)\n",
    "            x_hat = np.squeeze(x_hat)\n",
    "            x_hat_2 = np.squeeze(x_hat_2)\n",
    "        absolute_diff = abs(f1 - f2) / len_X\n",
    "        f1 = f1 / len_X\n",
    "        f2 = f2 / len_X\n",
    "    return absolute_diff, f1, f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b12642dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_shapley(version, xi, N, X, x, m, model, unique_count, causal_struct, is_classification,\n",
    "                        global_shap=False):\n",
    "    R = list(itertools.permutations(range(N)))\n",
    "    random.shuffle(R)\n",
    "    score = 0\n",
    "    count_negative = 0\n",
    "    vf1, vf2 = 0, 0\n",
    "    for i in range(m):\n",
    "        abs_diff, f1, f2 = get_value(version, list(R[i]), X, x, unique_count, causal_struct, model, N,\n",
    "                                     is_classification, xi)\n",
    "        vf1 += f1\n",
    "        vf2 += f2\n",
    "        score += abs_diff\n",
    "        if not global_shap:\n",
    "            if vf2 > vf1:\n",
    "                count_negative -= 1\n",
    "            else:\n",
    "                count_negative += 1\n",
    "    if count_negative < 0 and not global_shap:\n",
    "        score = -1 * score\n",
    "    return score / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0568642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapley(version, file_name, local_shap=0, global_shap=True, is_classification=False):\n",
    "    sigma_phi = 0\n",
    "    causal_struct = None\n",
    "    try:\n",
    "        causal_struct = json.load(open('lungcancer.json', 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    n_features = 7\n",
    "    unique_count = collections.Counter(map(tuple, X_train[:100]))\n",
    "    ##### f(x) with baseline\n",
    "    f_o = get_baseline(X_train[:100], model)\n",
    "    baseline = np.mean(X_train[:100], axis=0)\n",
    "    # To convert baseline (mean) to discrete\n",
    "    if is_classification:\n",
    "        baseline = [1 if i > 0.5 else 0 for i in baseline]\n",
    "    \n",
    "    rmse_shapley_values = []\n",
    "\n",
    "    if global_shap:\n",
    "        for feature in range(n_features):\n",
    "            print(\"feature: \", feature)\n",
    "            global_shap_score = Parallel(n_jobs=-1)(\n",
    "                delayed(approximate_shapley)(version, feature, n_features, X_train[:100], x, math.factorial(n_features), model,\n",
    "                                             unique_count, causal_struct, is_classification, global_shap) for i, x in\n",
    "                enumerate(X_train[:100]))\n",
    "            rmse_shapley_values.append(rmse(gt_aces[feature], np.array([i.detach().numpy()[0][0] for i in global_shap_score])))\n",
    "\n",
    "    return rmse_shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97990f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble:  0\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n",
      "feature:  3\n",
      "feature:  4\n",
      "feature:  5\n",
      "feature:  6\n",
      "ensemble:  1\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n",
      "feature:  3\n",
      "feature:  4\n",
      "feature:  5\n",
      "feature:  6\n",
      "ensemble:  2\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n",
      "feature:  3\n",
      "feature:  4\n",
      "feature:  5\n",
      "feature:  6\n",
      "ensemble:  3\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n",
      "feature:  3\n",
      "feature:  4\n",
      "feature:  5\n",
      "feature:  6\n",
      "ensemble:  4\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n",
      "feature:  3\n",
      "feature:  4\n",
      "feature:  5\n",
      "feature:  6\n",
      "rmse mean:  [0.         0.9918499  0.99638546 0.48273683 0.9328338  0.5373782\n",
      " 0.03397153]\n",
      "rmse std:  [0.         0.00026805 0.00010543 0.00024894 0.00024982 0.00038659\n",
      " 0.00109127]\n",
      "rmse all features mean:  0.5678794\n",
      "rmse all features std:  0.00033572858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5118/3595498321.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save('./models/cshapresults.npy', np.array([rmse_results, frechet_results]))\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "for ensemble in range(ensemble_size):\n",
    "    print(\"ensemble: \",ensemble)\n",
    "    model = torch.load(\"models/erm_lungcancer_\"+str(ensemble+1))\n",
    "    r = shapley(version='4', file_name='lungcancer', local_shap=12, is_classification=True, global_shap=True)\n",
    "    rmse_results.append(r)\n",
    "    \n",
    "np.save('./models/cshapresults.npy', np.array([rmse_results]))\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b64cd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = {0:(lambda ii:0*ii), 1:(lambda ii:1*ii), 2:(lambda ii:1*ii),\n",
    "        3:(lambda ii:0.5*ii), 4:(lambda ii:1*ii), 5:(lambda ii:0.5*ii),\n",
    "        6:(lambda ii:0*ii)}\n",
    "\n",
    "def get_grad(x, prior):\n",
    "    a = x.clone().detach().requires_grad_(True)\n",
    "    for f in prior.keys():\n",
    "        z = prior[f]\n",
    "        z = torch.sum(z(a[0][f]), dim=0)\n",
    "        z.backward()\n",
    "    return a.grad\n",
    "\n",
    "def get_grads_to_match(ip, prior):\n",
    "    return get_grad(ip, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b013ee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "validation accuracy: 40.5\n",
      "test accuracy: 44.349998474121094\n",
      "\n",
      "10 10\n",
      "validation accuracy: 83.0\n",
      "test accuracy: 83.94999694824219\n",
      "\n",
      "20 10\n",
      "validation accuracy: 83.0\n",
      "test accuracy: 83.94999694824219\n",
      "\n",
      "30 10\n",
      "validation accuracy: 83.0\n",
      "test accuracy: 83.94999694824219\n",
      "\n",
      "40 10\n",
      "validation accuracy: 83.0\n",
      "test accuracy: 83.94999694824219\n",
      "\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "\n",
    "    # Interval / Epochs\n",
    "    \n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    credo_model = Model(7,sample_size=len(dataset))\n",
    "    optimizer = optim.Adam([{'params': credo_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "\n",
    "    for ep in range(0,epoch): \n",
    "        trainval = DataSet(X_train,y_train)\n",
    "        train_loader = DataLoader(trainval, batch_size=64)\n",
    "        for input_data, target in train_loader:\n",
    "            credo_model.zero_grad()\n",
    "            input_data.requires_grad=True\n",
    "            output = credo_model(input_data)\n",
    "            \n",
    "            calc_grads = (autograd.grad(torch.sum(output, dim=0), input_data, retain_graph=True, create_graph=True)[0])\n",
    "            grads_to_match = get_grads_to_match(input_data, prior) \n",
    "            hinge_input = torch.abs(grads_to_match - calc_grads)\n",
    "            loss = loss_func(output,target) + 0.001 * torch.norm(torch.clamp(hinge_input, min=0), p=1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if ep%interval == 0:\n",
    "            print(ep, interval)\n",
    "            val = DataSet(X_val,y_val)\n",
    "            val_loader = DataLoader(val, batch_size=1)\n",
    "            acc_val = 0\n",
    "            acc_test = 0\n",
    "            \n",
    "            for input_data, target in val_loader:\n",
    "                output = credo_model(input_data)\n",
    "                acc = binary_acc(output, target.unsqueeze(1))\n",
    "                acc_val += acc\n",
    "\n",
    "            print ('validation accuracy:', float(acc_val/len(val_loader)))\n",
    "            testval = DataSet(X_test,y_test)\n",
    "            test_loader = DataLoader(testval, batch_size=1)\n",
    "            \n",
    "            for input_data, target in test_loader:\n",
    "                output = credo_model(input_data)\n",
    "                acc = binary_acc(output, target.unsqueeze(1))\n",
    "                acc_test += acc\n",
    "                \n",
    "            print('test accuracy:', float(acc_test/len(test_loader)))\n",
    "            print()\n",
    "    print(\"************\")\n",
    "    torch.save(credo_model, \"models/credo_lungcancer_\"+str(ensemble+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029b176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 83.94999694824219\n",
      "test accuracy: 83.94999694824219\n",
      "test accuracy: 83.94999694824219\n",
      "test accuracy: 84.69999694824219\n",
      "test accuracy: 83.94999694824219\n"
     ]
    }
   ],
   "source": [
    "for ensemble in range(ensemble_size):\n",
    "    credo_model = torch.load(\"models/credo_lungcancer_\"+str(ensemble+1))\n",
    "    testval = DataSet(X_test,y_test)\n",
    "    test_loader = DataLoader(testval, batch_size=1)\n",
    "    acc_test = 0\n",
    "    for input_data, target in test_loader:\n",
    "        output = credo_model(input_data)\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        acc_test += acc\n",
    "\n",
    "    print('test accuracy:', float(acc_test/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "459217c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=1\n",
    "num_feat=7\n",
    "num_alpha=2\n",
    "\n",
    "cov = np.cov(X_values, rowvar=False)\n",
    "mean_vector = np.mean(X_values, axis=0)\n",
    "\n",
    "aces_credo_total = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    ace_credo_total = []\n",
    "    model = torch.load(\"models/credo_lungcancer_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):\n",
    "        for t in range(0,num_feat):\n",
    "            expectation_do_x = []\n",
    "            inp=copy.deepcopy(mean_vector)\n",
    "            for x in np.linspace(0, 1, num_alpha):                \n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "                output=model(input_torchvar)\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_feat):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_feat)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "            ace_credo_total.append(expectation_do_x[1] - expectation_do_x[0])\n",
    "    aces_credo_total.append(ace_credo_total)\n",
    "np.save('./aces/lungcancer_credo_total.npy',aces_credo_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5d9186de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [5.44726849e-04 1.00003798e+00 9.99995661e-01 4.44970928e-01\n",
      " 3.78005159e-01 4.82776379e-01 4.29213047e-04]\n",
      "rmse std:  [4.29021338e-04 2.08854316e-04 6.65479930e-04 9.22940608e-02\n",
      " 2.40048627e-01 8.99921303e-02 3.26329729e-04]\n",
      "rmse all features mean:  0.47239429233823504\n",
      "rmse all features std:  0.06056635769639148\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "\n",
    "for ensemble in range(ensemble_size):\n",
    "    rmse_results.append([rmse(gt_aces[0], aces_credo_total[ensemble][0]),\n",
    "                         rmse(gt_aces[1], aces_credo_total[ensemble][1]),\n",
    "                         rmse(gt_aces[2], aces_credo_total[ensemble][2]),\n",
    "                         rmse(gt_aces[3], aces_credo_total[ensemble][3]),\n",
    "                         rmse(gt_aces[4], aces_credo_total[ensemble][4]),\n",
    "                         rmse(gt_aces[5], aces_credo_total[ensemble][5]),\n",
    "                         rmse(gt_aces[6], aces_credo_total[ensemble][6])])\n",
    "\n",
    "mse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "80c0ae3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "validation accuracy: 57.5\n",
      "test accuracy: 55.54999923706055\n",
      "\n",
      "0 5\n",
      "validation accuracy: 63.20000076293945\n",
      "test accuracy: 61.650001525878906\n",
      "\n",
      "5 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "5 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "0 5\n",
      "validation accuracy: 57.5\n",
      "test accuracy: 55.54999923706055\n",
      "\n",
      "0 5\n",
      "validation accuracy: 57.5\n",
      "test accuracy: 55.54999923706055\n",
      "\n",
      "5 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "5 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "0 5\n",
      "validation accuracy: 42.5\n",
      "test accuracy: 44.45000076293945\n",
      "\n",
      "0 5\n",
      "validation accuracy: 57.5\n",
      "test accuracy: 55.54999923706055\n",
      "\n",
      "5 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "5 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "0 5\n",
      "validation accuracy: 77.19999694824219\n",
      "test accuracy: 76.1500015258789\n",
      "\n",
      "0 5\n",
      "validation accuracy: 76.80000305175781\n",
      "test accuracy: 76.3499984741211\n",
      "\n",
      "5 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "5 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "10 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "15 5\n",
      "validation accuracy: 85.5\n",
      "test accuracy: 85.0999984741211\n",
      "\n",
      "0 5\n",
      "validation accuracy: 42.5\n",
      "test accuracy: 44.45000076293945\n",
      "\n",
      "0 5\n",
      "validation accuracy: 57.5\n",
      "test accuracy: 55.54999923706055\n",
      "\n",
      "5 5\n",
      "validation accuracy: 57.5\n",
      "test accuracy: 55.54999923706055\n",
      "\n",
      "5 5\n",
      "validation accuracy: 57.5\n",
      "test accuracy: 55.54999923706055\n",
      "\n",
      "10 5\n",
      "validation accuracy: 0.0\n",
      "test accuracy: 0.0\n",
      "\n",
      "10 5\n",
      "validation accuracy: 0.0\n",
      "test accuracy: 0.0\n",
      "\n",
      "15 5\n",
      "validation accuracy: 0.0\n",
      "test accuracy: 0.0\n",
      "\n",
      "15 5\n",
      "validation accuracy: 0.0\n",
      "test accuracy: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 91 # To reproduce the results\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "interval = 5\n",
    "epoch = 30\n",
    "for ensemble in range(5):\n",
    "    # Interval / Epochs\n",
    "    trainval = DataSet(X_train,y_train)\n",
    "    train_loader = DataLoader(trainval, batch_size=16)\n",
    "    \n",
    "    mse_loss_func = nn.MSELoss()\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    ahce_model = Model(7,sample_size=len(dataset))\n",
    "\n",
    "    optimizer = optim.Adam([{'params': ahce_model.parameters()}], lr = 0.001, weight_decay=1e-4)\n",
    "    losses = []\n",
    "    for i in range(1):\n",
    "        for ep in range(0,20):\n",
    "            for phase in ['freeze', 'train_dag']: \n",
    "                for input_data, target in train_loader:\n",
    "                    ahce_model.zero_grad()\n",
    "                    input_data.requires_grad=False\n",
    "                    if phase == 'freeze':\n",
    "                        output = ahce_model(input_data)\n",
    "                        loss = loss_func(output,target)\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    else:\n",
    "                        output, tub_sample, lung_sample, bronc_sample, either_sample, xray_sample  = ahce_model(input_data, phase='train_dag')\n",
    "                           \n",
    "                        lam_bda = 0\n",
    "                        loss = lam_bda*mse_loss_func(tub_sample, input_data[:,1])\n",
    "                        loss.backward(retain_graph=True)  \n",
    "\n",
    "                        loss = lam_bda*mse_loss_func(lung_sample, input_data[:,3])\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        \n",
    "                        loss = lam_bda*mse_loss_func(bronc_sample, input_data[:,4])\n",
    "                        loss.backward(retain_graph=True)\n",
    "\n",
    "                        loss = lam_bda*mse_loss_func(either_sample, input_data[:,5])\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        \n",
    "                        loss = lam_bda*torch.sqrt(mse_loss_func(xray_sample, input_data[:,6]))\n",
    "                        loss.backward(retain_graph=True)                   \n",
    "    \n",
    "                        loss = loss_func(output,target)\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "\n",
    "                if ep%interval == 0:\n",
    "                    print(ep, interval)\n",
    "                    val = DataSet(X_val,y_val)\n",
    "                    val_loader = DataLoader(val, batch_size=1)\n",
    "                    acc_val = 0\n",
    "                    acc_test = 0\n",
    "\n",
    "                    for input_data, target in val_loader:\n",
    "                        output = ahce_model(input_data)\n",
    "                        acc = binary_acc(output, target.unsqueeze(1))\n",
    "                        acc_val += acc\n",
    "\n",
    "                    print ('validation accuracy:', float(acc_val/len(val_loader)))\n",
    "                    testval = DataSet(X_test,y_test)\n",
    "                    test_loader = DataLoader(testval, batch_size=1)\n",
    "\n",
    "                    for input_data, target in test_loader:\n",
    "                        output = ahce_model(input_data)\n",
    "                        acc = binary_acc(output, target.unsqueeze(1))\n",
    "                        acc_test += acc\n",
    "\n",
    "                    print('test accuracy:', float(acc_test/len(test_loader)))\n",
    "                    print()\n",
    "\n",
    "    torch.save(ahce_model, \"./models/ahce_lungcancer_\"+str(ensemble+1))\n",
    "#     torch.save(ahce_model, \"./models/ahce_lungcancer_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b89251fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble:  0\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n",
      "feature:  3\n",
      "feature:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5118/265272099.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tub_sample = self.causal_link_asia_tub(torch.tensor(asia_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_5118/265272099.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_5118/265272099.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_5118/265272099.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
      "/tmp/ipykernel_5118/265272099.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_5118/265272099.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_5118/265272099.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
      "/tmp/ipykernel_5118/265272099.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lung_sample = self.causal_link_smoke_lung(torch.tensor(smoke_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_5118/265272099.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bronc_sample = self.causal_link_smoke_bronc(torch.tensor(smoke_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_5118/265272099.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_5118/265272099.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_5118/265272099.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
      "/tmp/ipykernel_5118/265272099.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_5118/265272099.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_5118/265272099.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
      "/tmp/ipykernel_5118/265272099.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_5118/265272099.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_5118/265272099.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  5\n",
      "feature:  6\n",
      "ensemble:  1\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5118/265272099.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xray_sample = self.causal_link_either_xray(torch.tensor(either_sample, dtype=torch.float))\n",
      "/tmp/ipykernel_5118/265272099.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n",
      "/tmp/ipykernel_5118/265272099.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  either_sample = self.causal_link_tub_lung_either(torch.cat((torch.tensor(tub_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float)), dim=1))\n",
      "/tmp/ipykernel_5118/265272099.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp = torch.cat((torch.tensor(asia_sample, dtype=torch.float), torch.tensor(tub_sample, dtype=torch.float), torch.tensor(smoke_sample, dtype=torch.float), torch.tensor(lung_sample, dtype=torch.float), torch.tensor(bronc_sample, dtype=torch.float), torch.tensor(either_sample, dtype=torch.float), torch.tensor(xray_sample, dtype=torch.float)),dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  3\n",
      "feature:  4\n",
      "feature:  5\n",
      "feature:  6\n",
      "ensemble:  2\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n",
      "feature:  3\n",
      "feature:  4\n",
      "feature:  5\n",
      "feature:  6\n",
      "ensemble:  3\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n",
      "feature:  3\n",
      "feature:  4\n",
      "feature:  5\n",
      "feature:  6\n",
      "ensemble:  4\n",
      "feature:  0\n",
      "feature:  1\n",
      "feature:  2\n",
      "feature:  3\n",
      "feature:  4\n",
      "feature:  5\n",
      "feature:  6\n"
     ]
    }
   ],
   "source": [
    "n_classes=1\n",
    "num_c=7#no. of features\n",
    "num_alpha=2\n",
    "\n",
    "aces_ahce_total = []\n",
    "for ensemble in range(5):\n",
    "    print(\"ensemble: \", ensemble)\n",
    "    ace_ahce_total = []\n",
    "    model =  torch.load(\"./models/ahce_lungcancer_\"+str(ensemble+1))\n",
    "    for output_index in range(0,n_classes):#For every class\n",
    "        #plt.figure()\n",
    "        for t in range(0,num_c):#For every feature\n",
    "            print(\"feature: \", t)\n",
    "            expectation_do_x = []\n",
    "            for x in np.linspace(0, 1, num_alpha):\n",
    "                X_values[:,t] = x\n",
    "                sample_data = model(X_values, phase='sample', inde=t, alpha=x).detach().numpy()\n",
    "                cov = np.cov(sample_data, rowvar=False)\n",
    "                means = np.mean(sample_data, axis=0)\n",
    "                cov=np.array(cov)\n",
    "                mean_vector = np.array(means)\n",
    "                inp=copy.deepcopy(mean_vector)\n",
    "                inp[t] = x\n",
    "                input_torchvar = autograd.Variable(torch.FloatTensor(inp), requires_grad=True)\n",
    "\n",
    "                output=model(input_torchvar)\n",
    "\n",
    "                o1=output.data.cpu()\n",
    "                val=o1.numpy()[output_index]#first term in interventional expectation                                       \n",
    "\n",
    "                grad_mask_gradient = torch.zeros(n_classes)\n",
    "                grad_mask_gradient[output_index] = 1.0\n",
    "                #calculating the hessian\n",
    "                first_grads = torch.autograd.grad(output.cpu(), input_torchvar.cpu(), grad_outputs=grad_mask_gradient, retain_graph=True, create_graph=True)\n",
    "\n",
    "                for dimension in range(0,num_c):#Tr(Hessian*Covariance)\n",
    "                    if dimension == t:\n",
    "                        continue\n",
    "                    temp_cov = copy.deepcopy(cov)\n",
    "                    temp_cov[dimension][t] = 0.0#row,col in covariance corresponding to the intervened one made 0\n",
    "                    grad_mask_hessian = torch.zeros(num_c)\n",
    "                    grad_mask_hessian[dimension] = 1.0\n",
    "\n",
    "                    #calculating the hessian\n",
    "                    hessian = torch.autograd.grad(first_grads, input_torchvar, grad_outputs=grad_mask_hessian, retain_graph=True, create_graph=False)\n",
    "                    val += np.sum(0.5*hessian[0].data.numpy()*temp_cov[dimension])#adding second term in interventional expectation\n",
    "                expectation_do_x.append(val)#append interventional expectation for given interventional value\n",
    "\n",
    "            ace_ahce_total.append(expectation_do_x[1] - expectation_do_x[0])\n",
    "\n",
    "    aces_ahce_total.append(ace_ahce_total)\n",
    "np.save('./aces/lungcancer_ahce_total.npy',aces_ahce_total,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "02dc9142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.05796246 0.5865253  0.56491578 0.77649596 1.1145698  0.65574369\n",
      " 0.08098546]\n",
      "rmse std:  [0.06275573 0.29022294 0.33369286 0.75975439 0.51841772 0.2375601\n",
      " 0.12799542]\n",
      "rmse all features mean:  0.5481712078000819\n",
      "rmse all features std:  0.33291416532225815\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "\n",
    "for ensemble in [0,1,2,3]:\n",
    "    rmse_results.append([rmse(gt_aces[0], aces_ahce_total[ensemble][0]),\n",
    "                         rmse(gt_aces[1], aces_ahce_total[ensemble][1]),\n",
    "                         rmse(gt_aces[2], aces_ahce_total[ensemble][2]),\n",
    "                         rmse(gt_aces[3], aces_ahce_total[ensemble][3]),\n",
    "                         rmse(gt_aces[4], aces_ahce_total[ensemble][4]),\n",
    "                         rmse(gt_aces[5], aces_ahce_total[ensemble][5]),\n",
    "                         rmse(gt_aces[6], aces_ahce_total[ensemble][6])])\n",
    "\n",
    "mse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7b666e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse mean:  [0.05796246 0.5865253  0.56491578 0.77649596 1.1145698  0.65574369\n",
      " 0.08098546]\n",
      "rmse std:  [0.06275573 0.29022294 0.33369286 0.75975439 0.51841772 0.2375601\n",
      " 0.12799542]\n",
      "rmse all features mean:  0.5481712078000819\n",
      "rmse all features std:  0.33291416532225815\n"
     ]
    }
   ],
   "source": [
    "rmse_results = []\n",
    "\n",
    "for ensemble in [0,1,2,3]:\n",
    "    rmse_results.append([rmse(gt_aces[0], aces_ahce_total[ensemble][0]),\n",
    "                         rmse(gt_aces[1], aces_ahce_total[ensemble][1]),\n",
    "                         rmse(gt_aces[2], aces_ahce_total[ensemble][2]),\n",
    "                         rmse(gt_aces[3], aces_ahce_total[ensemble][3]),\n",
    "                         rmse(gt_aces[4], aces_ahce_total[ensemble][4]),\n",
    "                         rmse(gt_aces[5], aces_ahce_total[ensemble][5]),\n",
    "                         rmse(gt_aces[6], aces_ahce_total[ensemble][6])])\n",
    "\n",
    "mse_results = np.array(rmse_results)\n",
    "print(\"rmse mean: \", np.mean(rmse_results, axis=0))\n",
    "print(\"rmse std: \", np.std(rmse_results, axis=0))\n",
    "print(\"rmse all features mean: \", np.mean(np.mean(rmse_results, axis=0)))\n",
    "print(\"rmse all features std: \", np.mean(np.std(rmse_results, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c399f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
